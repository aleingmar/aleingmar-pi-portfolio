[{"content":"This project was developed for the CI/CD course as part of the official university master\u0026rsquo;s degree in Development and Operations (DevOps). The main objective of this project is to carry out the development of unit and API test automation of a given Calculator program. GitHub repository: https://github.com/aleingmar/automatic-tests-python\nProject report: Project documentation: View documentation in pdf\n","date":"2025-05-17T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/automatic-tests-python/pytest_hu_a239cf477c0d0da6.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/automatic-tests-python/","title":"Test Automation with Python"},{"content":"This project was developed for the Containers subject as part of the official university master\u0026rsquo;s degree in Development and Operations (DevOps). The main objective of this project is to deploy a multi-tier application built on top of several containers orchestrated and deployed using Docker compose technology. The deployment is based on and is a continuation of the project \u0026ldquo;Dockerisation of multi-tier application\u0026rdquo; already uploaded to this portfolio. GitHub repository: https://github.com/aleingmar/multi-layer-app-dockercompose\nProject report: Project documentation: View documentation in pdf\n","date":"2025-05-10T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/multi-layer-dockercompose/dockerCompose_hu_674db0615fbd4c26.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/multi-layer-dockercompose/","title":"Deployment of multi-container service"},{"content":"This project was developed for the Containers subject as part of the official university master\u0026rsquo;s degree in Development and Operations (DevOps). The main objective of this project is to dockerise a simple multilayer application, designed with a pedagogical perspective to illustrate the layered architecture and its deployment using containers. This application has been designed with three different layers: web presentation, business logic and data persistence. Although a MEAN (Mongo - Express - Angular - Node) stack was initially considered, the presentation layer based on Angular and Nginx has not been fully implemented. Therefore, the final structure of the project is configured as follows:\nFirst layer, presentation layer: Nginx + Website\nSecond layer, business logic layer: App.js application developed using Express on Node.js.\nPersistence layer: Implemented using MongoDB.\nThe functionality of the application is deliberately simple, with the aim of focusing on structure and deployment. It is a kind of ‘Hello World’, where the client initially connects to the presentation layer (Nginx), which delivers an index.html file. This file contains a call that triggers a second HTTP request to the same Nginx server, but which is processed differently depending on the PATH of the request.\nThis request is redirected to the backend layer, where the Express application is deployed.\nFrom there, the application tries to establish a connection to the MongoDB database.\nDepending on the result of that connection, the backend responds to the client with a message in JSON format, indicating if the connection was successful (‘Hello world, connection successfully established’) or if an error occurred.\nGitHub repository: https://github.com/aleingmar/multi-layer-app-dockerisation\nExperimentation video and project memory: Documentation of the project: View documentation in pdf\n","date":"2025-04-19T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/multilayer-dockerisation/dockerizacion_hu_519fa5c8de698195.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/multilayer-dockerisation/","title":"Dockerization of a Multi-layer application"},{"content":"This project was developed for the Cloud Systems Administration subject as part of the official university master\u0026rsquo;s degree in Development and Operations (DevOps).\nThe main objective of the project was to develop scripts in Powershell to automate common tasks in Windows environments, streamlining processes that would normally require manual intervention.\nPowerShell conceptual snapshots PowerShell is Microsoft\u0026rsquo;s command interpreter and scripting language, designed specifically for Windows system administration. It is based on an object-oriented language, which means that the output of commands is not simply text, but structured objects with properties and methods that can be easily manipulated.\nFor example, in Bash, inter-process communication in pipes (|) is in plain text, which forces the use of additional tools such as grep, awk or sed to process the output. In PowerShell, on the other hand, pipelines exchange objects, allowing you to work directly with their attributes without the need to pause and restart the pipeline.\nExample Comparison of PowerShell vs. Bash PowerShell (working with objects):\nGet-Process | Where-Object { $_.CPU -gt 10 } | Select-Object Name, CPU\nHere, Get-Process returns a list of processes as objects, and we filter out those whose CPU usage ($_ .CPU) is greater than 10. Then, we select only the Name and CPU properties.\nBash (working with text):\nps aux | awk '$3 \u0026gt; 10 {print $11, $3}'\nps aux returns the list of processes as plain text, so it is necessary to use awk to extract and compare the third column (CPU usage).\nTasks automated by means of scripts: Specifically the four tasks to be automated are as follows:\nListing files by size Write a script that displays a list of files in the current directory that are larger than 1024 bytes.\nRename JPG files by date Write a script that renames all files with a JPG extension in the current directory, adding a date prefix in year, month, day format. For example, a file named \u0026ldquo;image1.jpg\u0026rdquo; would be renamed to \u0026ldquo;20240413-image1.jpg\u0026rdquo;, if the script is run on 13 April 2024.\nHard disk space monitoring Program a PowerShell script that displays hard disks with a percentage of free space below a given parameter. The script should print the drive letter and the values in GB of free space and size without decimals. The expression Get-WmiObject Win32_LogicalDisk retrieves the information of the system disks.\nCreating an interactive menu Program a script that displays a menu with the following options, so that the option associated with the number entered by the user is executed:\nList the services started. Display the system date. Execute the Notepad. Run the Calculator. Exit. GitHub repository: https://github.com/aleingmar/Powershell-Scripting\nExperimentation video and project report: Project documentation: View documentation in pdf\n","date":"2025-02-17T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/powershell-scripting/powershell-logo_hu_a668546c78800463.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/powershell-scripting/","title":"PowerShell Scripting"},{"content":"This project was developed for the Deployment Automation Tools course as part of the official university master\u0026rsquo;s degree in Development and Operations (DevOps).\nThe main objective of the project was to automate the local deployment of a complete WordPress environment using Ansible and Vagrant. An optimised secure architecture was implemented using Nginx as a reverse proxy that blocks traffic destined for certain sensitive Wordpress administration paths.\nVagrant creates and raises the virtual machine, on which Ansible is installed. Ansible then automatically self-provisions and configures all the necessary services, including Apache, MySQL, WordPress and Nginx, leaving the system completely ready for use.\nGeneral structure of the Anisble provisioning project The following is the organisation of the Ansible files and roles, to make it easier to understand the general operation of the project:\nMain playbook: provision/playbook.yml. This file acts as the starting point for Ansible. From here, the roles needed to configure all the components of the environment are included. In this case, the code is divided into four roles: apache, mysql, wordpress and nginx, which are executed in this order. The installation of PHP and its modules has been decided to be included directly in this playbook, instead of creating a separate role, as it is only a few lines of code. The order of provisioning is as follows:\nPHP modules Apache MySQL WordPress Nginx Variable management with Ansible Instead of using Hiera as with Puppet, Ansible uses YAML files inside the group_vars/all.yml directory, allowing variables to be separated from the main code. This ensures a more secure approach, avoiding exposing sensitive credentials when uploading the project to a repository. Although this project is academic and does not include encrypted variables, Ansible Vault allows you to encrypt variables if necessary.\nVariables are declared in: group_vars/all.yml. Jinja2 (.j2) templates are used to inject dynamic values into the configuration files. Roles in Ansible To better organise the manifests and auxiliary files that Ansible needs for infrastructure configuration automation, I split the content into four main roles in Ansible, each responsible for a part of the system. This allows for modularity, code reuse and better organisation of the playbook.\nApache Role With this role, Ansible installs and configures the Apache web server, which acts as a backend to serve WordPress. Apache is only accessible from the virtual machine itself, as Nginx will act as a reverse proxy.\nThe main tasks it performs are:\nInstall Apache and make sure the service is active. Remove the default Apache page. Configure Apache to listen on 127.0.0.1:8080. Setting the listening port to 127.0.0.1:8080 means that Apache will only accept connections from local processes on the same machine where it is running. The address 127.0.0.1 is the loopback (localhost) address, which prevents access from other machines on the network. This is useful when Apache is behind a reverse proxy, such as Nginx, which handles external connections and forwards requests to Apache on port 8080.’ Copy the custom configuration from a Jinja2 template (wp-apache-config.conf.j2). Enable the new site and restart Apache automatically. With this configuration, Apache is kept isolated from shortcuts, ensuring that it can only be queried through Nginx.\nMySQL Role In this role Ansible provisions the virtual machine with a MySQL database to ensure proper storage and access to WordPress data.\nThe main tasks it performs are:\nInstall the MySQL server. Create the necessary database for WordPress. Configure the user and assign the appropriate permissions. Execute an initialisation script (init-wordpress.sql.j2) to prepare the database with the initial structure and data. This role ensures that the database is ready and properly configured before WordPress attempts to connect later by running its role.\nWordPress Role This role automates the installation and configuration of WordPress, ensuring a functional and ready-to-use deployment.\nKey tasks it performs include:\nDownloading and extracting WordPress into /var/www/html/wordpress. Create and configure the wp-config.php file using a template (wp-config.php.j2). Ensure correct permissions for WordPress (chown -R www-data:www-data). Install wp-cli and use it to configure WordPress automatically. Initialise the database with minimal content using init-wordpress-content.sql.j2. Configure Apache to serve WordPress content. With this role, WordPress is installed, automatically configured and ready for use, without any manual intervention.\nNginx Role This role implements Nginx as a reverse proxy, forming the first layer of defence of the system. Its main function is to handle incoming requests and block unwanted access.\nThe main actions performed are:\nInstall Nginx in the virtual machine. Configure Nginx as a reverse proxy, redirecting requests to Apache on port 8080. Block access to sensitive paths such as /wp-admin and /wp-login.php to increase security. Optimise delivery of static files (CSS, JS, images) directly from Nginx, improving performance. Disable the default Nginx page and enable a WordPress-specific configuration. Restart Nginx automatically after applying the configuration. Why is Nginx important in this project?\nProtects Apache by acting as a single external access point, preventing direct attacks. Improves security by blocking access to critical management paths. With this configuration, Nginx filters traffic and only allows secure requests to WordPress, strengthening the system infrastructure. System architecture Request processing and data flow When a user accesses WordPress, the request follows the following flow:\nThe user accesses WordPress from a browser. Nginx receives the request on port 80 and decides whether to block the request or forward it to Apache. If the request is valid, Nginx forwards it to Apache on 127.0.0.1:8080. Apache processes the request, executing the WordPress PHP scripts. If the page requires database data, Apache queries MySQL. Apache returns the generated response to Nginx. Nginx sends the response to the user. This ensures that Apache is only accessible from the machine itself, while Nginx acts as the first line of defence.\nCommunication between Nginx and Apache. To better understand how the two servers connect, it is important to know how their ports and IPs work:\nNginx listens on 0.0.0.0.0:80, which means it accepts connections to port 80 and to any IP that identifies the machine running it. Apache listens on 127.0.0.1:8080, which means that this process can only communicate with other processes from the same machine that send traffic to that ip and port 8080. 127.0.0.1 is the loopback address, used for internal communication within the same machine. External traffic never reaches Apache directly, as Nginx acts as an intermediary. Key benefit: If someone tries to access Apache directly from another machine, the connection will be rejected because Apache is not exposed to the network. Conclusion All in all, simply by going to the console in the directory where the Vagrantfile is located and running a simple vagrant up, a functional, customised and secure WordPress environment is automatically deployed, accessible from a web client at http://192.168.55.10.\nGitHub repository: https://github.com/aleingmar/wordpress_ansible\nExperimentation video and project report: Project documentation: View documentation in pdf.\n","date":"2025-01-24T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/wp-ansible/wordpress-ansible_hu_56ee52534912ab5a.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/wp-ansible/","title":"Automated deployment of Wordpress environment with Ansible"},{"content":"This project was developed for the DevOps Tools course, as part of the oficial university\u0026rsquo;s master\u0026rsquo;s degree in Development and Operations (DevOps).\nThe aim of the project was to automatically deploy a fully functional multi-tier MEAN system in the AWS cloud. This system consists of a load balancer, several instances for the web application and a dedicated instance for the MongoDB database. I use Terraform, Packer and Ansible for infrastructure automation and provisioning.\nOn a personal level, I consider it important to highlight that the documentation report of this project is particularly complete, as it includes all the details of the development process. Among them, I had to deal with three main problems during this process. Without going into too much detail, these were:\nDeployment on Azure, version incompatibility and choice of AWS. Execution of an interactive command blocking the automatic provisioning process. Static resource inconsistency and balancer logoff. In my opinion, these problems are very interesting to analyse, as they are common situations in this type of work. Although they may seem minor, they have been fundamental in the development of the project.\nIt is also worth mentioning that for this project I have used similar technologies to those of the project ‘Creation and automated deployment of image in multicloud environment’, which is also available in my portfolio. For this reason, in this publication I have decided to highlight three aspects that differentiate both works:\nThe use of the MEAN stack. The modularisation of Terraform. The deployment process and architecture, although the latter is presented in a summarised form, as it is explained extensively and in detail in the report. Technology Stack Terraform: with terraform I centralise the whole deployment process, raise and manage the infrastructure elements that make up the system. Some of these elements are for example the networks that connect the different instances, the instances themselves, the load balancer\u0026hellip; In short, the infrastructure that supports the service.\nPacker: with packer I create the image that serves as a base for the instances that I build with Terraform. In this project, Packer generates a custom image for the first layer of the system, provisioning it with the necessary services such as Node.js, Nginx, Angular\u0026hellip;\nAnsible: with Ansible I do the provisioning of the instance that is raised and used by Packer for the creation of the image. In this project, Ansible automatically provisions the instance with Angular, Express, MongoDB, Nginx, Node\u0026hellip;\nMEAN Stack: The system being built is a service made up of the MEAN technology stack, widely used in the industry for its versatility and performance:\nMongoDB: Non-relational document-oriented database, ideal for handling large volumes of structured and unstructured data. Express: Backend framework for Node.js that facilitates the development of robust and scalable web applications. Angular: Frontend framework that allows the development of modern and reactive interfaces, improving the user experience. Node.js: Execution environment for Js on the server side. Modularisation of the Terraform template Importance of modularisation Modularisation in Terraform is vital in projects that use Terraform. It basically consists of dividing the main.tf into different ‘modules’ according to certain categories. Not only does this improve code readability and maintainability, but it also allows responsibilities to be divided and configurations to be reused between projects. Although managing variables between modules can be complex, this practice is essential in large, dynamic infrastructures.\nProject modules Security module: This module manages the security groups that define the traffic rules to and from the instances, enabling traffic from protocols such as SSH, HTTP\u0026hellip;. It also configures the SSH keys needed to access the instances remotely.\nNetwork module: Defines the networks and private subnets necessary for system connectivity and also configures routing tables and gateways to guarantee access between infrastructure layers.\nInstance module: Deploys the first and second layer instances, assigning public and private IP addresses and also provisions these instances for proper operation in the system.\nLoad balancer module: Configures and defines everything related to the load balancer that distributes the traffic between the instances of the first layer. In addition to the load balancer itself, for this to work it needs more elements such as destination groups, distribution strategies such as round-robin, definition of session persistence\u0026hellip;\nImage module: This module integrates Terraform with Packer for the creation of base images. Terraform executes packer build, retrieves the generated image and uses it to instantiate resources of the first layer.\nDeployment process and architecture The deployment starts with the integration of Terraform and Packer. Terraform invokes Packer, which is responsible for raising a temporary instance in AWS to generate a base image. During this process, this instance is provisioned with Ansible, which installs and configures services such as Angular, Express and MongoDB, as well as copying essential files from the local environment. Once provisioning is complete, Packer creates the base image and destroys the temporary instance, leaving an image ready for reuse.\nWith the image generated, Terraform proceeds to deploy the complete infrastructure. First, the networks and subnets are configured, ensuring internal connectivity between the layers of the system. Next, the first layer instances are deployed using the base image. These instances host the frontend and backend of the application, with Node.js and Nginx serving as the operational core.\nAt the same time, Terraform builds the second tier instance, dedicated to data persistence with MongoDB. This instance is connected via a private network to the first layer instances, guaranteeing secure and stable communication. In addition to this, terraform also raises a load balancer, configured to distribute the traffic between the first layer instances, which ensures high availability and scalability.\nThe last step is the final provisioning. Terraform uses Bash scripting to finish configuring the deployed instances. For example, in the first layer instances, Angular configurations are adjusted to include the IP addresses of the backend, which allows static files to be generated and served by Nginx with the necessary routes to connect the client to the backend.\nThis entire process ensures a fully automated deployment, resulting in a functional, production-ready system, with components integrated and configured for optimal performance.\nGitHub repository: https://github.com/aleingmar/Multi-layer_MEAN_Deployment\nVideo of the experimentation and project report: Project documentation: View documentation in pdf\n","date":"2025-01-14T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/stack-mean-terraform/stack_hu_db0e2e670f3f68be.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/stack-mean-terraform/","title":"Automatic deployment of multilayer MEAN service on AWS"},{"content":"This project was developed for the DevOps Tools course, as part of the official university master\u0026rsquo;s degree in Development and Operations (DevOps).\nThe aim of the project is to create and automatically deploy an image of a complete web system in a multicloud environment of Azure and AWS. This web system is composed of a small application written with Nodejs and a Nginx web server. To achieve this, I use Terraform, Ansible and Packer technologies mainly.\nTechnologies used: Terraform: With Terraform I centralize all the execution of the process and deploy the necessary infrastructure to raise an instance in the cloud created from the image of the system and accessible through the internet. Packer: With Packer I build the complete system image. Packer uses the cloud as a provider for the creation of the image. It builds an instance and all the necessary infrastructure for the creation of the image and destroys it when it is finished. Ansible: with ansible the provisioning of the instance that packer raises and from which the image is created is carried out. In the case of Azure I do this provisioning with Ansible, in the case of AWS I do the same but directly with Bash scripting. To control multicloud deployment, a parameter has been implemented that must be passed to the terraform apply ‘deployment_target=’, indicating whether you want to deploy in both clouds simultaneously or in a single cloud. If this is the case, you must indicate in which one you want to deploy.\nCreation and deployment process: The sequence of steps in the process would be as follows:\nInitialise by manually executing a terraform init \u0026amp;\u0026amp; terraform apply in the shell. After that, terraform executes the packer build command, which takes care of setting up all the necessary infrastructure and the machine used for the creation of the image. In the case of Azure, an Ansible is installed on this machine and it auto-provisions itself by running a playbook and a series of tasks defined in it. In the case of AWS, the same steps are executed, but instead of an Ansible directly by manual scripting in Bash. The provisioning is based among other things on the installation and management of the services: Nodejs, Nginx, pm2 and App.js on the instance that creates the image. Nodejs: Provides an environment with everything necessary for the application to run and function correctly. Nginx: Web server that will be in charge of redirecting all the traffic to the application and forwarding its responses. It is very important to configure it so that when the image is deployed the server is active and correctly configured to serve the app. Passes traffic from port 80 to port 3000 (where the app.js listens). PM2: Nodejs process manager which is used to ensure that the app.js is active when the image is deployed without having to do anything else (this step is particularly tricky). app.js: core, functional application of the image, it is important to transfer the source code of the app so that it is accessible by the instance that creates the image. After this, Packer creates the image and destroys all the infrastructure it has needed to build on the corresponding cloud provider. Terraform, after waiting for the image creation to finish successfully, builds all the necessary infrastructure (key pair, security group, disk\u0026hellip;) to build an instance from this image. Once the deployment is finished, this instance is accessible through the internet via the public ip. In short, just by executing a terraform init \u0026amp;\u0026amp; terraform apply you deploy a functional web environment accessible from the internet in the public cloud of Azure and AWS. And you also create a reusable image so you can deploy more instances identical to these in the future in a much faster and safer way against possible human errors.\nGitHub repository: https://github.com/aleingmar/CreateImages_Nginx-Nodejs_Packer\nRepository contents and project files: The GiHub repository consists of two main directories with two different versions: /version-2 and /version-3.1. The fully functional directory containing the latest version of the project is the second one (/version-3.1). This is the directory where you have to be located to deploy the terraform init \u0026amp;\u0026amp; terraform apply (cd version-3.1/te*).\nBriefly explaining the contents of the directory:\n/packer/: directory where all the content necessary for Packer to run and build the image is located. /packer/main.pkr.hcl: Packer\u0026rsquo;s main file where all the resources needed to build the image are defined as well as all the variables to be used. /packer/variables.pkrvars.hcl: file where I assign values to all the variables defined in the main.pkr.hcl except for the credentials of the two clouds that for security reasons, I define and assign values as environment variables of my host operating system that I use to launch the terraform. I pass these values as parameters in the terraform apply and packer build command. /packer/providers/: directory where we can find the auxiliary files used to create the image, such as the apache configuration file (nginx_default.conf), the playbook that defines the provisioning with ansible (provision.yml) and the nodejs application code (app.js). /terraform/: directory where all the content necessary for terraform to run and deploy all the necessary infrastructure for the project is located. /terraform/main.tf: main file of terraform, where all the process flow that the deployment must follow and all the infrastructure to be deployed is defined. /terraform/variables.tf: file where all the variables used by terraform are defined. /terraform/terraform.tfvars: file where all the variables are given values except for the credentials of the two clouds that, for security reasons, I define and assign values in environment variables of my host operating system from where I launch terraform. I pass these values as parameters in the terraform apply and packer build command. Contents of Packer main: The content of this file can be differentiated in several parts in which the following components necessary for the creation of the image are defined:\nPLUGINS: Defines the plugins needed for the template.\nDefinition of variables: (no value is assigned here, only maybe the default value).\nBUILDER: Define how the AMI is built in AWS \u0026ndash;\u0026gt; source{}\u0026ndash;\u0026gt; define the base system on which I want to create the image (ubuntu ISO) and the provider for which we create the image (technology with which the image will be deployed) \u0026ndash;\u0026gt; AMAZON. AZURE\nPROVISIONERS: Configure the operating system and the application, how the software will be installed and configured \u0026ndash;\u0026gt; build{}.\nExperimentation video and report of the project: Project documentation: View the pdf\n","date":"2024-12-14T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/imagen-multicloud-packer/imagen-multicloud-packer2_hu_f243c8aeb82a7214.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/imagen-multicloud-packer/","title":"Creation and automated deployment of image in multicloud environment."},{"content":"This project was developed for the Deployment Automation course, as part of the official university master\u0026rsquo;s degree in Development and Operations (DevOps).\nThe main objective of this project is to automatically deploy a test web environment with a custom WordPress service, using Vagrant as the Infrastructure as Code (IaC) tool and Puppet for automated provisioning. By simply running the vagrant up command in the terminal in the directory where the Vagrantfile is located, the entire environment is deployed without any additional configuration. Before you can deploy a WordPress web service, you need to perform several provisioning and configuration tasks, including the following:\nInstall, configure and set up an Apache web server to redirect and serve all content. Install all the specific PHP packages and modules required by WordPress. Install, configure and set up a MySQL database that will be used by WordPress for the persistence of its data. To verify correct operation, simply access localhost:8080 from the browser.\nGitHub repository: https://github.com/aleingmar/WordPress_deployment-puppet-vagrant\nThe project includes two different versions of the environment, organised in separate directories:\n/puppet-two-nodes. In this version, three Puppet nodes are deployed: one Puppet Master and two Puppet Clients. Each client (node) hosts a WordPress environment, provisioned with the directives sent from the Puppet Master. Every min the puppet clients automatically request the new puppet configuration if any via a cron job.\n/puppet-one-node In this version, only one virtual machine (VM) with a self-provisioning Puppet client is raised, without the need for a Puppet Master. Self-provisioning version: puppet-one-node In this version of the environment, only one mv is raised, a self-provisioning puppet agent/client without the need for a Puppet master node. The whole deployment process is done fully automatically.\npuppet-one-node: Vagrantfile The Vagrantfile defines the basic virtual machine (VM) configuration for creating an Infrastructure-as-Code (IaC) environment. It specifies the Ubuntu base box to be used, the networking options (including port forwarding and assigning a private IP), and allocates 1024 MB of RAM to the VM. In addition, Puppet is installed in agent mode, eliminating the need for a master Puppet server, and is configured to use the main manifest default.pp, modules from the modules directory and the Hiera configuration file hiera.yaml to centrally manage data.\nClient-server architecture version: puppet-two-nodes In this version of the environment 3 mvs are raised, one puppet master and two puppet clients. The mvs are automatically raised and their corresponding puppet versions are installed (to the master node the master is installed and so on). Once the client node is up (which is up after the master), as soon as it starts it sends its certificate to the master (which knows it because it is in its puppet.config file). In a MANUAL way, you have to perform the different administration tasks:\nSERVER side: Manually, the only thing the sysadmin in charge of this environment has to do is to do an: sudo /opt/puppetlabs/bin/puppetserver ca sign --all to sign all the unsigned certificates that have arrived (in this case one for each client node) and send them signed to the corresponding clients. On a voluntary basis, but recommended for security purposes, especially in real production environments, you should run before signing them a: sudo /opt/puppetlabs/bin/puppetserver ca list --all to list all the certificates and verify that you don\u0026rsquo;t sign a certificate that you shouldn\u0026rsquo;t. CLIENT side: Once this is done on the server, the corresponding client should receive its signed certificate, with which it will be able to communicate with the master node and ask for puppet configuration/provisioning by executing this command: sudo /opt/puppetlabs/bin/puppet agent --test. General structure of the puppet provisioning project The organisation of the files and modules is detailed below, which makes it easier to understand the general functioning of the project:\nMain file: manifests/default.pp. This file acts as the starting point for Puppet. The modules needed to configure all the components of the environment are imported from here. In this case, the code is split into three modules: apache, mysql and wordpress, which are executed and imported in this order. The PHP installation I have decided to code it directly in this module, without adding another module just for this as it is only 3 or 4 lines of code. The installation of these components is done in this order: apache, php, mysql and wordpress.\nVariables management with Hiera**: hiera.yaml, data/common.yaml. To manage variables, Hiera is used, which allows the keys to be separated from the source code. This ensures a more secure approach, as it avoids exposing sensitive credentials when uploading the project to a cloud repository. Although this project is academic and does not include encrypted variables, Hiera also offers the possibility to encrypt keys.\nVariables are declared along with their values in data/common.yaml.\nThe hiera.yaml file configures how Hiera works.\nTo integrate Hiera with Vagrant, the line puppet.hiera_config_path = ‘hiera.yaml’ is added to the Vagrantfile.\nModules used: The project is divided into three main modules, which guarantees modularity and organisation in the code:\nApache module: This module provisions and configures the Apache web server in the VM. This module provisions and configures the Apache web server in the VM, leaving it ready and active so that the wordpress module can manage and serve content from it.\nmysql module In this module a MySQL server is installed and configured in the VM, ensuring the correct functioning of the database manager. In addition, the necessary database for WordPress is created using the init-wordpress.sql.erb file.\nwordpress module This module installs and configures WordPress, making it fully functional. The main actions performed are:\nInstallation of the wordpress packages and dependencies and activation of the service. Configuration of the wp-config.conf.erb file, which configures the service, among other things, connects WordPress with the database and defines previously generated access keys. Installation and use of the wp-cli tool to automate the configuration of the website. Initialisation of the database using the init-wordpress-content.sql.erb file with the minimum content necessary to launch a web page. Configuration of Apache to serve the page content, using the wp-apache-config.conf.erb file. The service is accessible from the host at localhost:8080 thanks to port 8080 redirection from the host to port 80 on the virtual machine, where Apache listens for incoming HTTP requests.\nDeployment of the puppet-one-node version enviroment: ","date":"2024-11-25T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/wordpress-puppet/wordpress-puppet_hu_89d567fef6fadc74.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/wordpress-puppet/","title":"Automated Wordpress deployment using Vagrant and Puppet"},{"content":"This project was developed for the Deployment Automation course, as part of the official university master\u0026rsquo;s degree in Development and Operations (DevOps).\nThe aim of the project is to deploy and configure in an automated way a web environment on a virtual machine hosting an Apache server, which serves a basic web page. The virtual machine is created using IaC (Infrastructure as Code) with Vagrant, and Puppet is used for its provisioning, which manages the installation of Apache and the automatic loading of a simple HTML file, thus creating a functional web service.\nIn short, simply running a vagrant up starts the whole deployment and provisioning process and automatically (without doing anything else) a virtual machine is raised in which puppet is installed, an Apache web server is configured and installed to activate and listen to port 80 (http) of the VM and to return a simple web page that is inserted inside it.\nGitHub repository: https://github.com/aleingmar/deployment_apache-puppet-vagrant\nThe GiHub repository consists of two directories with two different versions: /easy_mode and /hard_mode.\nThe first folder (/easy_mode) contains the deployment project with a simplified structure. This version does not follow the architecture and code organisation of complex deployment projects, and the Apache configuration is more basic.\nThe second folder (/hard_mode) uses a more Puppet-friendly code pattern, for example using modules and other Puppet-typical elements. In addition, the Apache configuration is more advanced and detailed.\nBoth versions manage to deploy correctly.\nExplaining for example the simple version (/easy_mode) a directory where a ‘Vagrantfile’ file is located and a ‘manifests’ folder inside which the ‘apache.pp’ file is located.\nThe Vagrantfile defines the virtual machine infrastructure that needs to be deployed to support the web service. This is taken care of by Vagrant and underneath it, it uses VirtualBox as virtualisation provider.\nIn the Vagrantfile, before telling Vagrant to provision the infrastructure using Puppet, a script is run to install Puppet inside the virtual machines. Puppet in this case works in stand-alone mode (without following the client-server model).\nThe file ‘apache.pp’ defines the desired configuration for this infrastructure and serves as a declarative guide for Puppet to develop its work. Since Puppet uses a declarative language, you don\u0026rsquo;t tell it how you want things to be done, only what you want to achieve, and Puppet does the rest.\nThe service is accessible from the host on localhost:8080 by redirecting port 8080 on the host to port 80 on the virtual machine, where Apache listens for incoming HTTP requests.\n","date":"2024-10-11T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/apache-web-puppet/apache-puppet_hu_548344b7aac25795.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/apache-web-puppet/","title":"Automated Apache deployment using Vagrant and Puppet"},{"content":"This project aims to create a web portfolio to show all the projects I have developed during my academic and personal career, deploy it on my own RPI5 server and make it securely accessible from the internet.\nWeb Development For the construction of my portfolio I chose Hugo, a static website generation platform that allows you to create modern frontend pages using Markdown files. The decision to use Hugo was based on the fact that I had already written part of my portfolio in Obsidian, a tool (also written in Markdown) that I regularly use to take notes and organise my notes. Hugo\u0026rsquo;s ability to take advantage of Markdown format files allowed me to migrate this content easily and focus more on the quality of the content than on the technical development.\nThe theme I selected for my portfolio is hugo-theme-stack, because of its clean and modern format, which fits perfectly with the structure and design I was looking for. This template, with its focus on performance and simplicity, allowed me to optimise the development of my portfolio without having to invest too much time in interface design. Even so, I have programmed new functionalities that differ from the open source base project for my personal use.\nDevOps and Deployment In terms of deployment and operational management, my portfolio is hosted on my Raspberry Pi 5 server with 8 GB of RAM, which provides an efficient and energy-efficient solution. To ensure an orderly and isolated environment, I use Docker, where the portfolio runs inside a container. This allows me to package the application independently from the rest of the system, facilitating management and avoiding conflicts with other services running on the same server.\nThe web server that handles the requests is Caddy, a lightweight solution that allows me to secure the connection with HTTPS automatically and redirect the traffic to the different services I have deployed. In addition to my own portfolio, I also host the portfolio of a colleague in another container. To monitor the web portfolio I use Google Analytics 4 (GA4) which allows me to see statistics about the accesses to the website.\nFor development, I usually work locally using Visual Studio Code, although sometimes I use GitHub Codespaces when I prefer to work in a remote environment. The deployment process is simple: I connect to the server via SSH, pull the latest changes from GitHub and restart the Docker container running Hugo. This workflow is fully automated through a bash script and a Docker Compose file, which simplifies the process of lifting the web application with each update.\nWeb portfolio change deployment process with bash scripting:\n","date":"2024-09-01T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/hugo-portfolio/hugo-portfolio_hu_98ad7a3badfda038.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/hugo-portfolio/","title":"Deployment and creation of a web portfolio with Hugo."},{"content":"This project consists of hosting and managing autonomously certain services on my own server, a RasberryPi 5 with 8gb of Ram.\nFor the administration and configuration of both the server and its hosted services, I access remotely via SSH protocol.\nThe server is associated to a main domain managed by DuckDNS, which allows me to access the services remotely through the browser. To prevent the dynamic IP of my home network from changing and losing access to the server, I have a service that automatically updates this IP every 5 minutes, ensuring that it is always correctly synchronised.\nTo organise access via subdomains and ensure connection to my services via HTTPS, I use Caddy as a web server, which acts as an intermediary and handles the TLS/SSL certificates, guaranteeing secure and uncomplicated access.\nIn addition, I have implemented an advanced control panel called Homarr, which provides me with a centralised interface from which I can easily log in and access the different services deployed on the server.\nAll the services hosted on the server including the ones mentioned above are hosted using Docker containers and are organized in specific subdomains. At the time of writing, the unmentioned services hosted on the server are:\nVaultwarden: A password manager. Portainer: A container manager with a web interface. Pi-hole: An ad-blocking DNS service. WireGuard: A VPN service. WireGuard is integrated with Pi-hole. This setup allows me not only to redirect my traffic through my server to secure my connection, but also to enjoy ad-free browsing, no matter where I am.\n","date":"2024-08-01T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/my-server/rasberry_hu_5dce55d4f3ab2b45.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/my-server/","title":"My own RPI5 server"},{"content":"While I was doing my Final Degree Project and my internship, I had the opportunity to be part of the ES3 (Engineering and Science for Software Systems) research group at the University of Seville. This group specialises in the research and development of advanced software systems https://www.linkedin.com/company/grupoes3/.\nMy work focused on developing part of the screenRPA platform, a Robotic Process Automation (RPA) platform. My main objective was to improve the ability to extract value and knowledge from RPA processes. To do this, I worked in three different areas:\nWeb platform improvement and results visualisation: I worked on the development of the web platform to visualise and extract results along the entire RPA process pipeline, thus facilitating the interaction and analysis of each of the phases by the analysts. All this following the software design pattern Controller View Model. Application of advanced decision mining techniques: I developed and applied a new technique based on Machine Learning, specifically in the decision discovery phase. During this phase we seek to understand why a user performs one action instead of another. Decision mining is a speciality within the field of process mining. The application of this technique allowed the discovery of non-deterministic patterns and rules in user decisions, providing a greater understanding of automated processes. Automatic generation of business reports: I implemented a system on the platform for the automatic creation of business reports understandable by analysts and clients. These reports broke down all the knowledge extracted by the platform after execution, facilitating strategic decision making. This project allowed me to apply advanced knowledge in automation, Machine Learning and process mining, contributing to the evolution of an innovative platform within the field of business automation.\nFinal thesis report related to this project: View pdf\nPart of the screenRPA platform developed in my work: ","date":"2024-06-06T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/screenrpa-rpa-us/es3_hu_a08a7b01fc6df687.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/screenrpa-rpa-us/","title":"ScreenRPA"},{"content":"This project was developed during the Management of Information Services and Technologies (GSTI) course during my fourth year of my degree. The main objective of the project was to develop an Android mobile application to cover the telecare needs of an integrated care process (PAI) modelled by the Andalusian Health Service (SAS). Specifically, this application focuses on the dermatological telecare process for patients with skin cancer.\nThe purpose of the application is to facilitate the interaction between the patient and the dermatologist, speeding up communication and clinical decision-making, as part of an e-Health system. This application provides an environment that allows the patient to share images of their evolution with the dermatologist, who can respond and follow up almost immediately, reducing the usual waiting times. These images are stored in the Goggle Cloud, which ensures the scalability of the system and protection against loss.\nIn addition, the application includes other key functionalities such as the creation of appointments and teleconsultations through the app, access to the location of nearby medical centres on Google Maps, management of both doctor and patient profiles\u0026hellip;\nProject report: View pdf\n","date":"2024-02-15T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/android-gsti/avi-health_hu_b77bdd28d85e36fb.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/android-gsti/","title":"AVI Health"},{"content":"This project was developed for a Systems Architectures and Distributed Systems (ASSB) assignment during my fourth year of undergraduate studies. The main objective was to deploy a functional web service in a cloud environment using AWS services, staying within the limits of the free plan.\nThe deployed web service consists of a load balancer that distributes requests between two EC2 instances, each with its own Apache server and web page.\nThe main tasks performed included:\nControlling and creating cost alerts: Setting up alerts in AWS to ensure that all operations conformed to the free plan, avoiding unwanted additional charges. Deployment of EC2 instances: Raise two instances of EC2 and connect to them via SSH. On each of the instances, install the Apache web server. Configuring Apache web servers: Configuring the Apache services on both instances to serve a static web page created by myself. Deployment of a load balancer: Deployment of a load balancer on AWS to evenly distribute incoming requests between the two Apache servers, optimising the load and ensuring higher availability. View memory in pdf\n","date":"2024-01-01T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/aws-ec2-assb/aws-ec2_hu_e5e872738d324ce.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/aws-ec2-assb/","title":"Web service deployment with EC2 AWS"},{"content":"This project was developed for the course Architecture of Systems and Software Basis (ASSB), during my fourth year of studies. The main objective was to become familiar with parallel programming on distributed memory computers using the MPI library in C (Message-Passing Interface). A widely used technique for distributed computing on multiple machines, commonly used in high performance clusters. In the case of this project everything was executed in my own computer, understanding my computer as a kind of cluster and its different cores as machines that form it.\nThe main tasks of this project are:\nDevelopment of the ‘Hello World’ program with MPI: As a first step, I programmed a basic program so that each process would print a message with its rank and the name of the processor it was running on. In addition, I experimented with the possibility of launching more processes than the number of physical cores available, observing how MPI handles this scenario even though it loses performance.\nScalar product of vectors in parallel: I implemented a program with MPI that computes the scalar product of two large vectors, distributing the work among several processes. Each process calculated a part of the scalar product, and then the results were brought together in the process of rank 0, which displayed the total result. A runtime measurement was also added, allowing analysis of how performance varied as the number of processes changed.\nSolving integrals using the trapezoid method: I then implemented a program to calculate integrals using the trapezoid method, parallelising the program so that each process calculated the sum of the trapezoids in a specific sub-interval. As before, the process at rank 0 was responsible for summing the results of all the processes and displaying the final value of the integral.\nPerformance and scalability analysis: To evaluate the performance of the parallel program, I measured execution times and speedup when using different numbers of processes, from 1 to more than twice the number of available physical cores. The results were visualised in graphs showing how speedup improved as the number of processes increased, but also how efficiency decreased at certain points due to overhead in inter-process communication.\nProject documentation: View documentation in pdf\n","date":"2023-11-01T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/mpi-assb/mpi-logo_hu_97079222e835d566.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/mpi-assb/","title":"Parallel computing in distributed systems with MPI."},{"content":"This project was developed during the Systems and Software Architecture and Basis (ASSB) course during my fourth year of studies. The main objective was to implement an algorithm for calculating the Pi number using the MonteCarlo method by programming in C both sequentially and using parallel programming techniques to take full advantage of the resources of multicore processors using the OpenMP library, which allows code to be executed in multiple threads efficiently.\nThe main tasks performed were the following:\nDevelopment of the sequential and parallel version of the algorithm: I implemented a program that simulates the throwing of random ‘darts’ inside a square inscribed in a circle. The ratio between the hits inside the circle and the total throws is used to calculate the value of Pi. In the parallel version, I used OpenMP to divide the work among several threads, taking full advantage of the resources of multi-core processors.\nTime measurements and performance analysis: After developing the two versions of the program, I performed time measurements to evaluate the performance of the parallel version compared to the sequential version. I used different thread configurations, from a single thread to more than twice the physical cores of the processor, in order to analyse the speedup and scalability of the algorithm. The speedup was calculated as the ratio between the single-threaded execution time and the multi-threaded execution time.\nOptimisation and management of shared resources: During development, it was necessary to solve common problems in parallel programming, such as race conditions. In this case, I used OpenMP directives to define private variables on a per-thread basis, preventing multiple threads from simultaneously accessing the same global variables and affecting the final result.\nGenerating performance graphs: After collecting runtime and speedup data, I generated graphs to visualise the performance of the program as the number of threads increased. These graphs demonstrated how the application scaled with increased thread count, highlighting the advantages and limitations of parallelisation in a shared-memory environment.\nProject documentation: View documentation in pdf\n","date":"2023-10-01T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/openmp-assb/openmp-logo_hu_ed71b4fc7ce3335f.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/openmp-assb/","title":"Parallel computing on multicore processors with OpenMP."},{"content":"This project was developed during my third year of career for the Datathon organized by Dedalus and AWS in Andalusia, in collaboration with two fellow students. The main objective of this datathon was to work with a dataset provided on patients admitted to Intensive Care Units (ICU), to extract clinical value through cohort analysis and the creation of predictive models. The aim of this competition was to generate relevant information to improve hospital efficiency and optimize decision making in clinical contexts.\nCohort analysis Cohort analysis was one of the fundamental pillars of the project, as it allowed us to identify key patterns among hospitalized patients.\nWe performed a specific analysis on patients with cardiovascular problems. This analysis included several important aspects:\nThe percentage of cases according to gender, which helped us to identify possible differences in the prevalence of cardiovascular problems between men and women. The distribution of cases in relation to age, which allowed us to obtain a clear picture of the age groups most affected. The number of cases according to body mass index (BMI), which provided a perspective on how BMI influences the incidence of these pathologies. The percentage of mortality according to BMI, revealing possible correlations between body weight and fatal outcomes in patients with cardiovascular problems. We performed a more general cohort analysis for all patients admitted to the ICU. This study included:\nThe number of ICU admissions and discharges in different time slots, which provided information on the times of day when more admissions occurred. The mortality rate of patients according to diagnosis, shedding light on which were the most critical pathologies in terms of survival. The length of stay in ICU according to each diagnosis, which facilitated the identification of treatments and pathologies that required longer hospitalization time. Hospital Comparison Another highlight of the project was the comparative analysis between different hospitals. In this analysis, we evaluated the performance of hospitals in terms of their capacity to treat critical pathologies, using mortality as a key indicator. This analysis allowed us to compare the efficiency of hospitals in the treatment of certain diagnoses and pathologies, in addition to exploring the average length of stay in the ICU for each hospital. With this information, we were able to identify possible areas for improvement in hospital management and clinical care in different health centers.\nPredictive Models As part of the project, we also developed predictive models in order to anticipate behaviors and improve planning in ICUs. One of the main models was ICU congestion monitoring, which had the ability to predict when critical occupancy levels would be reached and thus anticipate potential bottlenecks in hospital care.\nIn addition, we designed models that could predict the length of stay of patients in the ICU, classifying them into short, medium or long stays. These predictive models not only allowed us to optimize hospital resource allocation, but also contributed to better discharge planning and reduced waiting times.\nPresentation: View presentation in pdf\n","date":"2023-02-01T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/datathon/dedalus-logo_hu_20af7d17515ab7de.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/datathon/","title":"Datathon Andalucía Dedalus"},{"content":"This project was developed for the Coding and Health Information Management (CGIS) course during my third year of studies. The main objective of the project is to design, model and develop a web application for managing the working hours of healthcare staff in a hospital.\nIn general terms, for the normal roles of healthcare professionals, the application allows them to consult their access to the hospital and create incidents if they see any anomaly with the register. For the administration and responsibility roles, the application allows them to monitor and keep track of workers\u0026rsquo; compliance with their working hours and enables them to process any incidents that professionals raise when they see a problem with their access records.\nIn addition to this, the application allows functionalities common to all roles such as the creation of user accounts, the management of user profiles\u0026hellip;\nDocumentation Table of contents Problem_domain Objectives System_Users Information_requirements: Functional_requirements: Business_rules: Non_functional_requirements: Conceptual_UML_model Problem domain: Currently the system governing the working hours of healthcare professionals is very complex. Events such as on-call duty, shift changes or rotations between centres are a daily occurrence in the sector. This makes the organisation of this system very difficult to manage and many of these events often go unrecorded.\nObjectives: The objective of our system will be to provide professionals in the sector with a tool that will allow both management and healthcare professionals a form of management that effectively and efficiently solves the aforementioned problems. Facilitating the processes to a great extent and allowing the professionals themselves to play a leading role in these processes. The main objectives of our system will be:\nOBJ-1. Management of access to the hospital centre:\nOne of the objectives of our system will be to carry out a control/monitoring of the accesses to the hospital centre by the healthcare staff. This will allow us to accurately record the number of hours worked by the professionals, avoid staff saturation, notify incidents in accesses, detect possible fraud\u0026hellip;\nOBJ-2.Registration and management of incidences on the accesses of health personnel:\nOne of the objectives of our system will be to carry out a control of the accesses to the hospital centre by the health personnel by means of the use of incidences. Allowing to register the date of presentation of the incident, the date of the response, the status, the reason for presentation and response\u0026hellip;\nOBJ-3. Registration and management of health personnel information:\nOne of the objectives of our system will be to record and manage the information of healthcare personnel. Allowing the registration of personal information about professionals, about their speciality, position\u0026hellip;.\nUsers of the system: The types of users who will be able to access the system and make specific use of it will be:\nHealth professionals (doctors and nurses).\nManagement\nHeads of duty.\nAdministrator\nInformation requirements: IR-001. User information: The system shall store personal data on all users. Primary and secondary email address, password and name.\nIR-002. Information on healthcare personnel: The system shall store data on healthcare personnel. Type of profession (doctor and nurse), medical speciality and position within the hospital system (management, duty manager, regular healthcare).\nIR-003. Información sobre las especialidades médicas: El sistema deberá almacenar datos sobre las especialidades médicas del personal sanitario. Nombre de especialidad (cardiología, radiología y pediatría).\nIR-003. Information on medical specialties: The system shall store data on the medical specialties of health personnel. Name of speciality (cardiology, radiology and paediatrics).\nIR-004. Information on access to the health care centre: The system shall record data on access to the health care centre by health care personnel. Date/time of entry, date/time of exit and number of hours worked in each day.\nIR-006. Information on incidents: The system shall record data on incidents (regarding problems with access) reported by health personnel. Date/time of the incident, reason for the incident, access to which it refers, status of the incident (accepted, rejected, pending-response), reason for the response and the healthcare staff member making the response.\nFunctional requirements: FR-001. User registration: (all roles).\nWe want the system to allow health professionals to register as users with a password and access the system.\nACCESSES\nFR-002. Create access: (address, administrator)\nI want to be able to create access to health professionals that are compatible with my responsibility.\nFR-003. View my accesses: (all roles)\nI want to be able to consult my access history to the medical centre.\nFR-004. Consult my accesses: (on-call manager, management, administrator)\nI want to be able to consult the accesses of the medical staff that are compatible with my responsibility.\nFR-005. Consult in detail an access: (all roles) ** I want to be able to consult in detail an access of the restrooms that are compatible with my responsibility.\nI want to be able to consult in detail an access of my list of accesses.\nFR-006. Modify access: (address, administrator)\nI want to be able to modify the accesses of the health professional that are compatible with my responsibility.\nFR-007. Delete access: (address, administrator)\nI want to be able to delete access to the health professional that are compatible with my responsibility.\nFR-008. Filter access by dates: (all roles)\nI want to be able to filter my access history to the medical centre by date.\nINCIDENTS\nFR-009. Create incident log: (healthcare professional, duty manager).\nI want to be able to notify of any incident on the records of my accesses.\nFR-0010. Consult incidents: (management, administrator).\nI want to be able to consult a list of the incidents of the medical staff.\nFR-0011. Query my incidents: (healthcare professional, head of duty, management, administrator)\nI want to be able to consult a list of my incidents.\nFR-0012. Consult in detail incidents: (all roles).\nI want to be able to consult in detail the incidents.\nFR-0013. Modify incidents: (healthcare professional, duty manager)\nI want to be able to modify my incidents.\nFR-0014. Delete incidents: (health professional).\nI want to be able to delete my incidences on the records of my accesses.\nFR-015. Solve incidents: (management, administrator)\nI want to be able to approve or deny incidents by letting you know the reason for the resolution.\nHEALTH PROFESSIONALs\nFR-0016. Create health professional: (address, administrator)\nI want to be able to create a new health professional.\nFR-0017. Consult health professional: (head of duty, management, administrator )\nI want to be able to consult a list of health professional that are compatible with my responsibility.\nFR-0018. Consult in detail health professional: (management, administrator ) ** I want to be able to see in detail the details of the health professional.\nI want to be able to see in detail the details of a health professional.\nFR-0019. Consult in detail health professional: (head of duty, management, administrator)\nI want to be able to consult in detail a health professional in the list of health professional that are compatible with my responsibility.\nFR-0020. Modify health professional: (management, administrator )\nI want to be able to modify the details of a health professional.\nFR-0021. Delete health professional: (address, administrator )\nI want to be able to delete the data of a health professional.\nFR-0022. Filter health professional by name: (head of duty, address, administrator)\nI want to be able to filter by name the list of health professional that are compatible with my responsibility.\nFR-0023. Filter health professional by name: (address, administrator )\nI want to be able to filter the list of health professional by profession.\nBusiness rules: BR-001. Not specified\nNon-functional requirements: NFR-001. Security: The system must be protected against unauthorised access.\nNFR-002. Performance: The system must be able to handle the required number of users without any degradation in performance.\nNFR-003. Scalability: The system must be able to scale up or down as required.\nNFR-004. Availability: The system must be available when needed.\nNFR-005. Maintenance: The system must be easy to maintain and upgrade.\nNFR-006. Portability: The system must be able to run on different platforms with minimal changes.\nNFR-007. Reliability: The system must be reliable and meet user requirements.\nNFR-008. Usability: The system must be easy to use and understand.\nNFR-009. Compatibility: The system must be compatible with other systems.\nNFR-010. Compliance: The system must be compliant with all applicable laws and regulations.\nThe system must have an availability of 99.96%.\nUML Conceptual Model ","date":"2022-01-01T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/cgis-laravel-jornadas/logo-laravel_hu_6bd97112e97e83b0.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/cgis-laravel-jornadas/","title":"Web application for the management of working days"},{"content":"This project was developed during the Security, Confidentiality and Identity Management (SCGI) course during my third year. This project focused on the design and development of a secure client-server architecture using the Transport Layer Security (TLS) protocol, with the aim of ensuring the confidentiality, integrity and authenticity of communications.\nContext and Objective The project arose from the need to secure communications in an application that allows the transmission of sensitive data, such as user credentials (login/password), between clients and a central server. This type of application is especially relevant in environments where data privacy is crucial, such as the health sector, banking and any system where users must authenticate their identity to access services.\nConfiguring KeyStores and TrustStores To implement TLS security, it was necessary to configure both the KeyStore and the TrustStore. These elements allow mutual authentication between the client and the server. The KeyStore contains the private keys and associated certificates that identify the entity (client or server), while the TrustStore manages the certificates of the trusted entities, allowing validation that the communication partners are legitimate.\nIn the development of this project, Keytool, a tool included in the Java JDK, was used to create and manage the certificates. The process required the configuration of environment variables such as JAVA_HOME and PATH, to facilitate the use of this tool from the command line.\nImplementation of TLS Sockets The communication between the client and the server was done through SSL (Secure Sockets Layer) sockets, over which the TLS protocol was integrated. At both ends, client and server, secure sockets were configured to allow the transmission of encrypted data. The server was designed to listen for incoming connections on a specific port (in this case, port 3343), authenticating clients by verifying their credentials.\nTLS Server: The server was responsible for receiving client connections and authenticating each user by verifying their credentials (username and password). It also provided responses based on the result of this verification, informing the client whether the authentication was successful or unsuccessful.\nTLS client: The client established a secure connection to the server and sent the user\u0026rsquo;s credentials for verification. The server\u0026rsquo;s response was displayed in a simple user interface, indicating whether the authentication process was successful or if there was an error.\nCommunication and security testing Both secured and unsecured communication tests were conducted in order to highlight the importance of using TLS in applications that handle sensitive information. For the unsecured tests, an unencrypted socket was implemented to observe how an attacker, using tools such as Wireshark, could capture and display in clear text the transmitted user credentials.\nIn contrast, by enabling TLS, it was observed that all transmitted data was encrypted, making it impossible for an attacker to read the captured data on the network. This demonstrated the effectiveness of TLS in protecting the confidentiality and integrity of information.\nPerformance and Concurrency A key aspect of this project was the validation of system performance, specifically the ability to handle multiple concurrent connections. The server was configured to handle up to 300 concurrent connections, which was achieved by using Java threads to handle parallel requests. Although connections were made sequentially in the initial tests (one every second), the system proved to be robust in processing requests efficiently without compromising security or stability.\nProject documentation: View documentation in pdf\n","date":"2021-12-01T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/security-gasca/java3_hu_4fa9a77699e8903d.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/security-gasca/","title":"Client-Server Architecture in Java using TLS"},{"content":"This project was developed as part of the Intelligent Systems (IS) course during my third year of studies. The work consisted of applying different classification and data pre-processing algorithms to solve a hypothetical problem about space travel in the distant future. The main objective was to predict, based on certain attributes of the passengers, whether they would be transported to an alternative dimension after a space accident.\nContext and Objective The problem posed the situation where a spacecraft, with thousands of passengers on board, collides with a space-time anomaly, and half of the passengers are transported to another dimension. The challenge of the project was to develop a predictive model to identify which passengers would have been transported to that alternate dimension, based on attributes such as age, planet of origin, whether they were in cryo-sleep, and other factors.\nData Preprocessing The dataset provided contained information on approximately 8700 passengers, and consisted of 14 attributes, both numerical and nominal. Initially, certain variables considered irrelevant, such as passenger destination and amounts spent on luxury services, were removed. Next, missing value imputations were performed and categorical variables were binarised using One Hot Encoding.\nTo improve the efficiency of distance-based ranking algorithms such as KNN, normalisation of numerical variables, in particular passenger age, was performed to ensure a fair comparison between the different attributes.\nAlgorithms Implemented Several classification algorithms were tested, including:\nZeroR: Used as a baseline, classifying all passengers based on the value of the most frequent class. J48: A decision tree type algorithm that showed good results after a pruning process to avoid overfitting. KNN (k-Nearest Neighbors): This classifier was based on the similarity between individuals to predict whether a passenger would be transported. Different values of K were tested, with 9 being the best performing value. Naive Bayes: An algorithm that, despite its simplicity, provided good results by assuming independence between attributes. Results The best performance was obtained using the KNN algorithm with a value of K=9, achieving an accuracy of 74.3%. In comparison, the other algorithms showed slightly lower performances. The experiments performed with cross-validation helped to more reliably evaluate the models, ensuring that there was no over-fitting.\nProject documentation: View documentation in pdf\n","date":"2021-10-01T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/weka-espacial/weka_hu_c791f303ee6ac83f.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/weka-espacial/","title":"Data project with Weka"},{"content":"This project was developed during the Database (DB) course during my second year of my degree. The main objective was to design, model and develop a SQL database to manage and store the information of the electromedical facilities and equipment of an operating theatre. The database would allow not only to manage the equipment and its maintenance, but also to ensure compliance with health regulations, to control the value of the equipment, to supervise periodic revisions\u0026hellip;\nSome of the tasks performed include:\nDesign and modelling of the database: Based on an exhaustive analysis of the requirements, a relational model was developed representing the different elements of the operating theatre, the electro-medical equipment, the air conditioning and electrical installations, and the profiles of the users managing this information (biomedical engineers, maintenance engineers and general services managers).\nEquipment lifecycle management: The database stores key information about the equipment, such as its acquisition date, useful life, supplier, purchase value and technical parameters that must be kept within certain ranges to ensure its correct functioning. This allows maintenance schedules to be controlled and replacements to be planned in a timely manner.\nAutomation of overhauls: Through triggers and stored procedures, the database is able to generate alerts when overhaul dates are approaching or when a piece of equipment is close to exceeding its useful life. This ensures that the OR is maintained to the required standards without unplanned interruptions.\nAdvanced queries and reporting: Implementation of a series of views and queries to provide easy access to relevant information, such as overhaul status, total equipment cost and engineers\u0026rsquo; maintenance history. This allows users to obtain customised reports that can be used for strategic decision making within the hospital.\nProject documentation: View documentation in pdf\n","date":"2021-04-01T00:00:00Z","image":"https://aleingmar-pi-portfolio.duckdns.org/p/bd-quirofano/bd_hu_f9297ba08bca8bc9.png","permalink":"https://aleingmar-pi-portfolio.duckdns.org/p/bd-quirofano/","title":"Relational database for the management of operating room equipment."}]