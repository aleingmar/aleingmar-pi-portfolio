<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Programación Paralela on Alejandro Inglés Martínez</title><link>https://aleingmar-pi-portfolio.duckdns.org/es/categories/programaci%C3%B3n-paralela/</link><description>Recent content in Programación Paralela on Alejandro Inglés Martínez</description><generator>Hugo -- gohugo.io</generator><language>es</language><lastBuildDate>Wed, 01 Nov 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://aleingmar-pi-portfolio.duckdns.org/es/categories/programaci%C3%B3n-paralela/index.xml" rel="self" type="application/rss+xml"/><item><title>Computación paralela en sistemas distribuidos con MPI.</title><link>https://aleingmar-pi-portfolio.duckdns.org/es/p/mpi-assb/</link><pubDate>Wed, 01 Nov 2023 00:00:00 +0000</pubDate><guid>https://aleingmar-pi-portfolio.duckdns.org/es/p/mpi-assb/</guid><description>&lt;img src="https://aleingmar-pi-portfolio.duckdns.org/p/mpi-assb/mpi-logo.png" alt="Featured image of post Computación paralela en sistemas distribuidos con MPI." /&gt;&lt;p&gt;Este proyecto fue desarrollado para la asignatura Arquitectura de Sistemas y Software de Base (ASSB), durante mi cuarto año de carrera. El objetivo principal era familiarizarse con la programación paralela en computadores de memoria distribuida utilizando la &lt;strong&gt;librería MPI&lt;/strong&gt; en &lt;strong&gt;C&lt;/strong&gt; (Message-Passing Interface). Una técnica muy utilizada para la computación distribuida en múltiples máquinas, comúnmente utilizado en clústeres de alto rendimiento. En el caso de este proyecto todo se ejecutó en mi propio computador, entendiendo mi computador como una especie de clúster y sus diferentes núcleos como máquinas que forman este.&lt;/p&gt;
&lt;p&gt;Las principales tareas de este proyecto son:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Desarrollo del programa de &amp;ldquo;Hola Mundo&amp;rdquo; con MPI&lt;/strong&gt;: Como primer paso, programé un programa básico para que &lt;strong&gt;cada proceso imprimiera un mensaje con su rango y el nombre del procesador en el que estaba ejecutándose&lt;/strong&gt;. Además, experimenté con la posibilidad de lanzar más procesos que el número de núcleos físicos disponibles, observando cómo MPI gestiona este escenario aunque pierda rendimiento.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Producto escalar de vectores en paralelo&lt;/strong&gt;: Implementé un programa con MPI que &lt;strong&gt;calcula el producto escalar de dos vectores&lt;/strong&gt; de gran tamaño, distribuyendo el trabajo entre varios procesos. Cada proceso calculaba una parte del producto escalar, y luego los resultados se reunían en el proceso de rango 0, que mostraba el resultado total. También se añadió una medición del tiempo de ejecución, lo que permitió analizar cómo variaba el rendimiento al cambiar el número de procesos.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Resolución de integrales con el método de los trapecios&lt;/strong&gt;: Posteriormente, &lt;strong&gt;implementé un programa para calcular integrales mediante el método de los trapecios&lt;/strong&gt;, paralelizando el programa para que cada proceso calculara la suma de los trapecios en un subintervalo específico. Al igual que antes, el proceso de rango 0 era responsable de sumar los resultados de todos los procesos y mostrar el valor final de la integral.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Análisis de rendimiento y escalabilidad&lt;/strong&gt;: Para evaluar el rendimiento del programa paralelo, medí los tiempos de ejecución y la aceleración al utilizar diferentes cantidades de procesos, desde 1 hasta más del doble del número de núcleos físicos disponibles. Los resultados se visualizaron mediante gráficos que mostraban cómo la aceleración mejoraba conforme aumentaba el número de procesos, pero también cómo disminuía la eficiencia en ciertos puntos debido a la sobrecarga en la comunicación entre procesos.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Documentación del proyecto: &lt;a class="link" href="../../../post/paralelizacion-mpi-assb/ASSB-mpi.pdf" &gt;&lt;strong&gt;Visualizar documentación en pdf&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Computación paralela en procesadores multinúcleos con OpenMP.</title><link>https://aleingmar-pi-portfolio.duckdns.org/es/p/openmp-assb/</link><pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate><guid>https://aleingmar-pi-portfolio.duckdns.org/es/p/openmp-assb/</guid><description>&lt;img src="https://aleingmar-pi-portfolio.duckdns.org/p/openmp-assb/openmp-logo.png" alt="Featured image of post Computación paralela en procesadores multinúcleos con OpenMP." /&gt;&lt;p&gt;Este proyecto fue desarrollado durante la asignatura de Arquitectura de Sistemas y Software de Base (ASSB) durante mi cuarto año de carrera. El objetivo principal era implementar un algoritmo para el cálculo del número Pi utilizando el método de MonteCarlo programando en C tanto de forma secuencial como utilizando técnicas de programación paralela para aprovechar al máximo los recursos de los procesadores multinúcleo mediante la librería OpenMP, que permite ejecutar código en múltiples hilos de manera eficiente.&lt;/p&gt;
&lt;p&gt;Las principales tareas realizadas fueron las siguientes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Desarrollo de la versión secuencial y paralela del algoritmo&lt;/strong&gt;: Implementé un programa que simula el lanzamiento de &amp;ldquo;dardos&amp;rdquo; aleatorios dentro de un cuadrado inscrito en un círculo. La relación entre los aciertos dentro del círculo y los lanzamientos totales permite calcular el valor de Pi. En la versión paralela, utilicé OpenMP para dividir el trabajo entre varios hilos, aprovechando al máximo los recursos de los procesadores multinúcleo.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Mediciones de tiempo y análisis de rendimiento&lt;/strong&gt;: Tras desarrollar las dos versiones del programa, realicé mediciones de tiempo para evaluar el rendimiento de la versión paralela en comparación con la secuencial. Utilicé diferentes configuraciones de hilos, desde un solo hilo hasta más del doble de los núcleos físicos del procesador, con el objetivo de analizar la aceleración y escalabilidad del algoritmo. La aceleración se calculó como la relación entre el tiempo de ejecución en un único hilo y el tiempo de ejecución con varios hilos.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Optimización y gestión de recursos compartidos&lt;/strong&gt;: Durante el desarrollo, fue necesario resolver problemas comunes de la programación paralela, como las condiciones de carrera. En este caso, utilicé directivas de OpenMP para definir variables privadas en cada hilo, evitando que varios hilos accedieran simultáneamente a las mismas variables globales y afectaran el resultado final.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generación de gráficos de rendimiento&lt;/strong&gt;: Tras recopilar los datos de tiempos de ejecución y aceleración, generé gráficos para visualizar el rendimiento del programa a medida que aumentaba el número de hilos. Estos gráficos demostraron cómo la aplicación escalaba con un mayor número de hilos, destacando las ventajas y limitaciones de la paralelización en un entorno de memoria compartida.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Documentación del proyecto: &lt;a class="link" href="../../../post/paralelizacion-openmp-assb/ASSB-openmp.pdf" &gt;&lt;strong&gt;Visualizar documentación en pdf&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;</description></item></channel></rss>