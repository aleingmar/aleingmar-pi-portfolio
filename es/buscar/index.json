[{"content":"Este proyecto fue desarrollado para la asignatura de Herramientas DevOps, como parte del máster universitario oficial en Desarrollo y Operaciones (DevOps).\nEl objetivo del proyecto fue desplegar de forma automática un sistema MEAN multicapa completamente funcional en la nube de AWS. Este sistema se compone de un balanceador de carga, varias instancias para la aplicación web y una instancia dedicada para la base de datos MongoDB. Utilizo Terraform, Packer y Ansible para la automatización de infraestructura y aprovisionamiento.\nA nivel personal, considero importante destacar que el informe de documentación de este proyecto está especialmente completo, ya que recoge todos los detalles del proceso de desarrollo. Entre ellos, he tenido que enfrentarme a tres problemas principales durante este proceso. Sin entrar en demasiados detalles, estos fueron:\nDespliegue en Azure, incompatibilidad de versiones y elección de AWS. Ejecución de un comando interactivo que bloquea el proceso automático de aprovisionamiento. Inconsistencia de recursos estáticos y cierre de sesión del balanceador. En mi opinión, estos problemas resultan muy interesantes de analizar, ya que son situaciones habituales en este tipo de trabajos. Aunque pueden parecer menores, han sido fundamentales en el desarrollo del proyecto.\nTambién cabe mencionar que para este proyecto he utilizado tecnologías similares a las del proyecto \u0026ldquo;Creación y despliegue automatizado de imagen en entorno multicloud\u0026rdquo;, que también está disponible en mi portfolio. Por esta razón, en esta publicación he decidido resaltar tres aspectos que diferencian ambos trabajos:\nEl uso del stack MEAN. La modularización de Terraform. El proceso de despliegue y arquitectura, aunque este último se presenta de forma resumida, ya que se explica de manera extensa y detallada en el informe. Stack Tecnológico Terraform: con terraform centralizo todo el proceso de despliegue, levanto y gestiono los elementos de infraestructura que forman el sistema. Algunos de estos elementos son por ejemplos las redes que conectan las distintas instancias, las propias instancias, el balanceador de carga\u0026hellip; En definitiva la infra que sustenta el servicio.\nPacker: con packer creo la imagen que me sirve como base para las instancias que levanto con Terraform. En este proyecto, Packer genera una imagen personalizada para la primera capa del sistema, aprovisionándola con los servicios necesarios como Node.js, Nginx, Angular\u0026hellip;\nAnsible: con Ansible realizo el aprovisionamiento de la instancia que levanta y usa Packer para la creación de la imagen. En este proyecto, Ansible aprovisiona de forma automática la instancia con Angular, Express, MongoDB, Nginx, Node\u0026hellip;\nStack MEAN: El sistema que se levanta se trata de un servicio conformado por el stack tecnológico MEAN, ampliamente utilizado en la industria por su versatilidad y rendimiento:\nMongoDB: Base de datos no relacional orientada a documentos, ideal para manejar grandes volúmenes de datos estructurados y no estructurados. Express: Framework backend para Node.js que facilita el desarrollo de aplicaciones web robustas y escalables. Angular: Framework frontend que permite desarrollar interfaces modernas y reactivas, mejorando la experiencia del usuario. Node.js: Entorno de ejecución para Js en el lado del servidor. Modularización de la plantilla Terraform Importancia de la modularización La modularización en Terraform es vital en los proyectos que utilizan Terraform. Básicamente consiste en dividir el main.tf en distintos \u0026ldquo;módulos\u0026rdquo; según ciertas categorías. No solo mejora la legibilidad y mantenimiento del código, sino que también permite dividir responsabilidades y reutilizar configuraciones entre proyectos. Aunque la gestión de variables entre módulos puede ser compleja, esta práctica resulta esencial en infraestructuras grandes y dinámicas.\nMódulos del proyecto Módulo de seguridad: Este módulo gestiona los grupos de seguridad que definen las reglas de tráfico hacia y desde las instancias, posibilitando tráfico de protocolos como SSH, HTTP\u0026hellip; También se configuran las claves SSH necesarias para acceder a las instancias de forma remota.\nMódulo de redes: Define las redes y subredes privadas necesarias para la conectividad del sistema y además, configura tablas de enrutamiento y gateways para garantizar el acceso entre las capas de la infraestructura.\nMódulo de instancias: Despliega las instancias de la primera y segunda capa, asignando direcciones IP públicas y privadas y además,provisiona estas instancias para su correcto funcionamiento en el sistema.\nMódulo de balanceador de carga: Configura y define todo lo relacionado con el balanceador de carga que distribuye el tráfico entre las instancias de la primera capa. Además del propio balanceador de carga, para que este funcione necesita más elementos como son grupos de destino, estrategias de distribución como round-robin, definición de la persistencia de sesiones\u0026hellip;\nMódulo de imagen: Este módulo integra Terraform con Packer para la creación de imágenes base. Terraform ejecuta packer build, recupera la imagen generada y la utiliza para instanciar recursos de la primera capa.\nProceso de despliegue y arquitectura El despliegue comienza con la integración de Terraform y Packer. Terraform invoca a Packer, que se encarga de levantar una instancia temporal en AWS para generar una imagen base. Durante este proceso, esta instancia es aprovisionada con Ansible, que instala y configura servicios como Angular, Express y MongoDB, además de copiar ficheros esenciales desde el entorno local. Una vez completado el aprovisionamiento, Packer crea la imagen base y destruye la instancia temporal, dejando preparada una imagen lista para su reutilización.\nCon la imagen generada, Terraform procede a desplegar la infraestructura completa. En primer lugar, se configuran las redes y subredes, asegurando la conectividad interna entre las capas del sistema. A continuación, se despliegan las instancias de la primera capa utilizando la imagen base. Estas instancias alojan el frontend y backend de la aplicación, con Node.js y Nginx sirviendo como núcleo operativo.\nSimultáneamente, Terraform levanta la instancia de la segunda capa, dedicada a la persistencia de datos con MongoDB. Esta instancia se conecta mediante una red privada a las instancias de la primera capa, garantizando una comunicación segura y estable. Además de esto, terraform también levanta un balanceador de carga, configurado para distribuir el tráfico entre las instancias de la primera capa, lo que asegura alta disponibilidad y escalabilidad.\nEl último paso es el aprovisionamiento final. Terraform utiliza scripting en Bash para terminar de configurar las instancias desplegadas. Por ejemplo, en las instancias de la primera capa, se ajustan las configuraciones de Angular para incluir las direcciones IP del backend, lo que permite que se generen los ficheros estáticos que son servidos por Nginx con las rutas necesarias para conectar el cliente con el backend.\nTodo este proceso asegura un despliegue completamente automatizado, resultando en un sistema funcional y listo para producción, con componentes integrados y configurados para ofrecer un rendimiento óptimo.\nRepositorio de GitHub: https://github.com/aleingmar/Multi-layer_MEAN_Deployment\nVídeo de la experimentación y memoria del proyecto: Documentación del proyecto: Visualizar documentación en pdf\n","date":"2025-01-14T00:00:00Z","image":"https://aleingmar-pi-portfolio.pages.dev/p/stack-mean-terraform/stack_hu_34a297c34a0ad2ca.png","permalink":"https://aleingmar-pi-portfolio.pages.dev/es/p/stack-mean-terraform/","title":"Despliegue automático de servicio MEAN multicapa en AWS"},{"content":"Este proyecto fue desarrollado para la asignatura de Herramientas DevOps, como parte del máster universitario oficial en Desarrollo y Operaciones (DevOps).\nEl objetivo del proyecto es crear y desplegar de forma automática una imagen de un sistema web completo en un entorno multicloud de Azure y AWS. Este sistema web se compone de una pequeña aplicación escrita con Nodejs y un servidor web Nginx. Para conseguirlo, utilizo las tecnologías de Terraform, Ansible y Packer principalmente.\nTecnologías utilizadas: Terraform: Con Terraform centralizo todo la ejecución del proceso y despliego la infraestructura necesaria para levantar una instancia en la nube creada a partir de la imagen del sistema y accesible a través de internet. Packer: Con Packer construyo la imagen del sistema completo. Packer utiliza como proveedor para la creación de la imagen la cloud. Levanta una instancia y toda la infraesturctura necesaria para la creación de la imagen y cuando termina las destruye. Ansible: con ansible se lleva a cabo el aprovisionamiento de la instancia que levanta packer y a partir del cual se crea la imagen. En el caso de Azure este aprovisionamiento lo hago con Ansible, en el caso de AWS hago lo mismo pero directamente con scripting de Bash. Para controlar el despliegue multicloud, se ha implementado un parámetro que se debe pasar al terraform apply \u0026quot;deployment_target=\u0026quot;, indicando si se quiere desplegar en las dos nubes simultaneamente o en una única nube. Si es este caso, hay que indicar en cual se desea desplegar.\nProceso de creación y despliegue: La secuencia de pasos del proceso sería la siguiente:\nInicializar ejecutando manualmente en la shell un terraform init \u0026amp;\u0026amp; terraform apply Tras ello, terraform ejecuta el comando packer build que se encarga de levantar toda la infraesturctura necesaria y la máquina que utilizá para la creación de la imagen. En el caso de Azure, en esta máquina se instala un Ansible y este se autoaprovisiona ejecutando un playbook y una serie de task definidos en él. En el caso de AWS, se ejecutan los mismos pasos pero en vez de con un Ansible directamente de forma manual con un scripting en Bash. El aprovisionamiento se basa entre otras cosas en la instalación y gestión de los servicios: Nodejs, Nginx, pm2 y App.js en la instancia que crea la imagen. Nodejs: Proporciona un entorno con todo lo necesario para que la aplicación ejecute y funcione correctamente. Nginx: servidor web que se encargará de redirijir todo el tráfico a la aplicación y de reenviar sus respuestas. Muy importante su configuración para que cuando se despliegue la iamgen el servidor esté activo y correctamente configurado para que sirva a la app. Pasa el tráfico del puerto 80 al 3000 (donde escucha la app.js) PM2: gestor de procesos de Nodejs que sirve para asegurar que la app.js esté activa cuando se despliegue la imagen sin tener que hacer nada más. (especialmente delicada este paso). App.js: aplicación central y funcional de la imagen, es importante trasferir el código fuente de la app para que sea accesible por la instancia que crea la imagen. Tras esto, Packer crea la imagen y destruye toda la infraestructura que ha necesitado levantar en el proveedor correspondiente de cloud. Terraform tras esperar que la creación de la imagen finalice correctamente, levanta toda la infraestructura necesaria (par de claves, grupo de seguridad, disco\u0026hellip;) para levantar una instancia a partir de esta imagen. Una vez terminado el despliegue, está instancia es accesible a través de internet a raiz de la ip pública. En definitiva, solo ejecutando un: terraform init \u0026amp;\u0026amp; terraform apply despliegas todo un entorno web funcional y accesible desde internet en la nube pública de Azure y AWS. Y además creas una imagen reutilizable para poder desplegar más instancias idénticas a estas en un futuro de una forma mucho más rápida y seguro ante posibles errores humanos.\nRepositorio de GitHub: https://github.com/aleingmar/CreateImages_Nginx-Nodejs_Packer\nContenido del repositorio y ficheros del proyecto: El repositorio de GiHub se compone de dos directorios principales con dos versiones distintas: /version-2 y /version-3.1 El directorio totalmente funcional y que contiene la ultima version del proyecto es el segundo (/version-3.1). Este es el directorio donde hay que ubicarse para desplegar el terraform init \u0026amp;\u0026amp; terraform apply (cd version-3.1/te*).\nExplicando brevemente el contenido del directorio:\n/packer/: directorio donde se encuentra todo el contenido necesario para la ejecución de Packer y para que pueda construir la imagen. /packer/main.pkr.hcl: fichero principal de Packer donde se definen todos los recursos necesarios para contruir la imagen asi como defino todas las variables que van a utilizar. /packer/variables.pkrvars.hcl: fichero donde les asigno valores a todos las variables definidas en el main.pkr.hcl menos a las credenciales de las dos nubes que por seguridad, las defino y asigno valores como variables de entorno de mi sistema operativo del host que utilizo para lanzar el terraform. Estos valores los paso como parámetros en el comando de terraform apply y packer build. /packer/providers/: directorio donde podemos encontrar los ficheros auxiliares que sirven para crear la imagen como pueden ser, el fichero de configuración de apache (nginx_default.conf), el playbook que define el aprovisionamiento con ansible (provision.yml) y el código de la aplicación de nodejs (app.js). /terraform/: directorio donde se encuentra todo el contenido necesario para la ejecución de terraform y para que pueda desplegar toda la infraesturtura necesaria para el proyecto. /terraform/main.tf: fichero principal de terraform, donde se define todo el flujo de proceso que debe seguir el despliegue y toda la infraestrutura a levantar. /terraform/variables.tf: fichero donde se definen todas las variables utilizadas por terraform. /terraform/terraform.tfvars: fichero donde se les da valor a todas las variables menos a las credenciales de las dos nubes que por seguridad, las defino y asigno valores en variables de entorno de mi sistema operativo del host desde donde lanzo el terraform. Estos valores los paso como parámetros en el comando de terraform apply y packer build. Contenido del Packer main: El contenido de este fichero se puede diferenciar en varias partes en las cuales se definen los siguientes componentes necesarios para la creación de la imagen:\nPLUGINS: Define los plugins necesarios para la plantilla.\nDefinición de variables: (no se les asigna valor aquí, solo alomejor el de por defecto)\nBUILDER: Define cómo se construye la AMI en AWS \u0026ndash;\u0026gt; source{}\u0026ndash;\u0026gt; define el sistema base sobre el que quiero crear la imagen (ISO ubuntu) y el proveeedor para el que creamos la imagen (tecnologia con la que desplegará la imagen) \u0026ndash;\u0026gt; AMAZON. AZURE\nPROVISIONERS: Configura el sistema operativo y la aplicación, como se va instalar y configurar el software \u0026ndash;\u0026gt; build{}\nRepositorio de GitHub: https://github.com/aleingmar/CreateImages_Nginx-Nodejs_Packer\nVídeo de la experimentación y memoria del proyecto: Documentación del proyecto: Visualizar documentación en pdf\n","date":"2024-12-14T00:00:00Z","image":"https://aleingmar-pi-portfolio.pages.dev/p/imagen-multicloud-packer/imagen-multicloud-packer2_hu_de124919ef6d011c.png","permalink":"https://aleingmar-pi-portfolio.pages.dev/es/p/imagen-multicloud-packer/","title":"Creación y despliegue automatizado de imagen en entorno multicloud."},{"content":"Este proyecto fue desarrollado para la asignatura de Herramientas de Automatización de Despliegues como parte del máster universitario oficial en Desarrollo y Operaciones (DevOps).\nEl objetivo principal del proyecto fue automatizar el despliegue en local de un entorno WordPress completo utilizando Ansible y Vagrant. Se implementó una arquitectura securizada optimizada mediante el uso de un Nginx como proxy inverso que bloquea el tráfico destinado a ciertas rutas sensibles de administración de Wordpress.\nVagrant crea y levanta la máquina virtual, en la cual se instala Ansible. Luego, Ansible se autoaprovisiona y configura automáticamente todos los servicios necesarios, incluyendo Apache, MySQL, WordPress y Nginx, dejando el sistema completamente listo para su uso.\nEstructura general del proyecto de aprovisionamiento con Anisble A continuación, se detalla la organización de los archivos y roles de Ansible, para facilitar la comprensión del funcionamiento general del proyecto:\nPlaybook principal: provision/playbook.yml Este archivo actúa como el punto de inicio en Ansible. Desde aquí se incluyen los roles necesarios para configurar todos los componentes del entorno. En este caso, el código está dividido en cuatro roles: apache, mysql, wordpress y nginx, que se ejecutan en este orden. La instalación de PHP y sus módulos se ha decidido incluir directamente en este playbook, en lugar de crear un rol separado, ya que son solo unas pocas líneas de código. El orden de aprovisionamiento es el siguiente:\nMódulos de PHP Apache MySQL WordPress Nginx Gestión de variables con Ansible En lugar de usar Hiera como con Puppet, en Ansible se utilizan archivos YAML dentro del directorio group_vars/all.yml, lo que permite separar las variables del código principal. Esto asegura un enfoque más seguro, evitando exponer credenciales sensibles al subir el proyecto a un repositorio. Aunque este proyecto es académico y no incluye variables encriptadas, Ansible Vault permite cifrar variables si fuera necesario.\nLas variables se declaran en: group_vars/all.yml Se utilizan plantillas Jinja2 (.j2) para inyectar valores dinámicos en los archivos de configuración. Roles en Ansible Para organizar mejor los manifiestos y ficheros auxiliares que necesita Ansible para la automatización de la configuración de la infraestructura, dividí el contenido en cuatro roles principales en Ansible, cada uno encargado de una parte del sistema. Esto permite modularidad, reutilización de código y una mejor organización del playbook.\nRol Apache Con este rol, Ansible instala y configura el servidor web Apache, que actúa como backend para servir WordPress. Apache solo es accesible desde la propia máquina virtual, ya que Nginx actuará como proxy inverso.\nLas principales tareas que realiza son:\nInstalar Apache y asegurarse de que el servicio esté activo. Eliminar la página por defecto de Apache. Configurar Apache para escuchar en 127.0.0.1:8080. \u0026ldquo;Configurar el puerto de escucha en 127.0.0.1:8080 significa que Apache solo aceptará conexiones desde procesos locales en la misma máquina donde se ejecuta. La dirección 127.0.0.1 es la dirección de loopback (localhost), lo que impide el acceso desde otras máquinas en la red. Esto es útil cuando Apache está detrás de un proxy inverso, como Nginx, que gestiona las conexiones externas y reenvía las solicitudes a Apache en el puerto 8080.\u0026rdquo; Copiar la configuración personalizada desde una plantilla Jinja2 (wp-apache-config.conf.j2). Habilitar el nuevo sitio y reiniciar Apache automáticamente. Con esta configuración, Apache se mantiene aislado de accesos directos, asegurando que solo pueda ser consultado a través de Nginx.\nRol MySQL En este rol Ansible aprovisiona la máquina virtual de una bd MySQL para garantizar el correcto almacenamiento y acceso a los datos de WordPress.\nLas principales tareas que realiza son:\nInstalar el servidor MySQL. Crear la base de datos necesaria para WordPress. Configurar el usuario y asignarle los permisos adecuados. Ejecutar un script de inicialización (init-wordpress.sql.j2) para preparar la base de datos con la estructura y datos iniciales. Este rol garantiza que la base de datos esté lista y correctamente configurada antes de que WordPress intente conectarse más adelante al ejecutar su rol.\nRol WordPress Este rol automatiza la instalación y configuración de WordPress, asegurando un despliegue funcional y listo para su uso.\nLas tareas clave que realiza incluyen:\nDescargar y extraer WordPress en /var/www/html/wordpress. Crear y configurar el archivo wp-config.php usando una plantilla (wp-config.php.j2). Asegurar los permisos correctos para WordPress (chown -R www-data:www-data). Instalar wp-cli y usarlo para configurar WordPress automáticamente. Inicializar la base de datos con un contenido mínimo utilizando init-wordpress-content.sql.j2. Configurar Apache para servir el contenido de WordPress. Con este rol, WordPress se instala, se configura automáticamente y se deja listo para su uso, sin necesidad de ninguna intervención manual.\nRol Nginx Este rol implementa Nginx como proxy inverso, formando la primera capa de defensa del sistema. Su función principal es gestionar las solicitudes entrantes y bloquear accesos no deseados.\nLas principales acciones realizadas son:\nInstalar Nginx en la máquina virtual. Configurar Nginx como proxy inverso, redirigiendo las solicitudes a Apache en el puerto 8080. Bloquear el acceso a rutas sensibles como /wp-admin y /wp-login.php para aumentar la seguridad. Optimizar la entrega de archivos estáticos (CSS, JS, imágenes) directamente desde Nginx, mejorando el rendimiento. Deshabilitar la página por defecto de Nginx y habilitar una configuración específica para WordPress. Reiniciar Nginx automáticamente después de aplicar la configuración. ¿Por qué es importante Nginx en este proyecto?\nProtege Apache al actuar como único punto de acceso externo, evitando ataques directos. Mejora la seguridad bloqueando accesos a rutas críticas de administración. Con esta configuración, Nginx filtra el tráfico y solo permite solicitudes seguras a WordPress, fortaleciendo la infraestructura del sistema. Arquitectura del sistema Proceso de una solicitud y flujo de datos Cuando un usuario accede a WordPress, la solicitud sigue el siguiente flujo:\nEl usuario accede a WordPress desde un navegador. Nginx recibe la solicitud en el puerto 80 y decide si la bloquea o la reenvía a Apache. Si la solicitud es válida, Nginx la reenvía a Apache en 127.0.0.1:8080. Apache procesa la solicitud, ejecutando los scripts PHP de WordPress. Si la página requiere datos de la base de datos, Apache consulta MySQL. Apache devuelve la respuesta generada a Nginx. Nginx envía la respuesta al usuario. Esto asegura que Apache solo sea accesible desde la propia máquina, mientras que Nginx actúa como la primera línea de defensa.\nComunicación entre Nginx y Apache Para entender mejor cómo se conectan ambos servidores, es importante conocer cómo funcionan sus puertos e IPs:\nNginx escucha en 0.0.0.0:80, lo que significa que acepta conexiones al puerto 80 y hacia cualquier IP que identifique la máquina que lo ejecuta. Apache escucha en 127.0.0.1:8080, lo que significa que con este proceso solo se pueden comunicar otros procesos desde la misma máquina y que manden tráfico a esa ip y al puerto 8080. 127.0.0.1 es la dirección de loopback, usada para comunicación interna dentro de la misma máquina. El tráfico externo nunca llega directamente a Apache, ya que Nginx actúa como intermediario. Beneficio clave: Si alguien intenta acceder a Apache directamente desde otra máquina, la conexión será rechazada porque Apache no está expuesto a la red. Conclusión En definitiva, simplemente ubicándose en la consola en el directorio donde se encuentra el Vagrantfile y ejecutando un simple vagrant up, se despliega automáticamente un entorno WordPress funcional, personalizado y seguro, accesible desde un cliente web en http://192.168.55.10.\nRepositorio de GitHub: https://github.com/aleingmar/wordpress_ansible\nVídeo de la experimentación y memoria del proyecto: Documentación del proyecto: Visualizar documentación en pdf\n","date":"2025-01-24T00:00:00Z","image":"https://aleingmar-pi-portfolio.pages.dev/p/wp-ansible/wordpress-ansible_hu_b6bc727e082dec12.png","permalink":"https://aleingmar-pi-portfolio.pages.dev/es/p/wp-ansible/","title":"Despliegue automatizado de entorno Wordpress con Ansible"},{"content":"Este proyecto fue desarrollado para la asignatura de Automatización de Despliegues, como parte del máster universitario oficial en Desarrollo y Operaciones (DevOps).\nEl objetivo principal de este proyecto es desplegar de forma automática un entorno web de prueba con un servicio WordPress personalizado, utilizando Vagrant como herramienta de Infraestructura como Código (IaC) y Puppet para su aprovisionamiento automatizado. Simplemente ejecutando en la terminal el comando vagrant up en el directorio donde se encuentra el Vagrantfile, se despliega todo el entorno sin necesidad de realizar configuraciones adicionales. Antes de poder desplegar un servicio web de WordPress, es necesario llevar a cabo varias tareas de aprovisionamiento y configuración previas, entre ellas están:\nInstalar, configurar y levantar un servidor web Apache para redirigir y servir todo el contenido. Instalar todos los paquetes y módulos específicos de PHP requeridos por WordPress. Instalar, configurar y levantar una base de datos MySQL que será utilizada por WordPress para la persistencia de sus datos. Para verificar el correcto funcionamiento, basta con acceder a localhost:8080 desde el navegador.\nRepositorio de GitHub: https://github.com/aleingmar/WordPress_deployment-puppet-vagrant\nEl proyecto incluye dos versiones diferentes de entorno, organizadas en directorios separados:\n/puppet-two-nodes En esta versión se despliegan tres nodos Puppet: un Puppet Master y dos Puppet Client. Cada cliente (nodo) aloja un entorno de WordPress, aprovisionado con las directivas enviadas desde el Puppet Master. Cada min de forma automática los puppet clients solicitan la nueva configuracon de puppet si la hubiera mediante una tarea cron.\n/puppet-one-node En esta versión se levanta únicamente una máquina virtual (MV) con un cliente Puppet que se autoaprovisiona, sin necesidad de un Puppet Master. Versión de autoaprovisionamiento: puppet-one-node En esta version del entorno se levanta solo una mv, un puppet agente/cliente que se autoaprovisiona sin necesidad de ningún nodo master. Todo el proceso de despliegue se hace totalmente de forma automática.\npuppet-one-node: Vagrantfile El Vagrantfile define la configuración básica de la máquina virtual (MV) para crear un entorno de infraestructura como código (IaC). Se especifica la caja base de Ubuntu que se utilizará, las opciones de red (incluyendo el redireccionamiento de puertos y la asignación de una IP privada), y se asigna 1024 MB de memoria RAM a la MV. Además, se instala Puppet en modo agente, eliminando la necesidad de un servidor Puppet maestro, y se configura para utilizar el manifiesto principal default.pp, los módulos desde el directorio modules y el archivo de configuración de Hiera hiera.yaml para gestionar datos de forma centralizada.\nVersión de arquitectura cliente-servidor: puppet-two-nodes En esta version del entorno se levantan 3 mvs, un puppet master y dos puppet clients. De forma automática se levantan las mvs y se instalan sus correspondientes versiones de puppet (al nodo master se instala el master y asi). Una vez que se levanta el nodo cliente (que se levanta despues del master), nada mas que arranca envia su certificado al master (que lo conoce porque esta en su fichero puppet.config). De forma MANUAL hay que realizar las distintas tareas de administración:\nLado del SERVER: De forma manual, lo único que tiene que hacer el administrador de sistemas que se encargue de este entorno es hacer un: sudo /opt/puppetlabs/bin/puppetserver ca sign --all para firmar todos los certificados sin firmar que le han llegado (en este caso uno por cada nodo cliente) y mandarselos firmados a los clientes correspondientes. De forma voluntaria pero recomendable a nivel de seguridad sobre todo en entornos más reales de producción, deberia de ejecutar antes de firmarlos un: sudo /opt/puppetlabs/bin/puppetserver ca list --all para listar todos los certificados y verificar que no firma un certificado que no deberia. Lado del CLIENTE: Una vez hecho esto en el server, al cliente correspondiente le debe de llegar su certificado ya firmado, con el que podrá comunicarse ante el nodo master y pedirle la configuraciones/aprovisionamiento de puppet ejecutando este comando: sudo /opt/puppetlabs/bin/puppet agent --test Estructura general del proyecto de aprovisionamiento con puppet A continuación, se detalla la organización de los archivos y módulos, lo que facilita la comprensión del funcionamiento general del proyecto:\nArchivo principal: manifests/default.pp Este archivo actúa como el punto de inicio de Puppet. Desde aquí se importan los módulos necesarios para configurar todos los componentes del entorno. En este caso, el código está dividido en tres módulos: apache, mysql y wordpress, que son ejecutados e importados en este orden. La instalación de PHP he decidido codificarla directamente en este módulo, sin añadir un módulo más simplemente para esto ya que son apenas 3 o 4 líneas de código. La instalación de estos componentes se hace en este orden: apache, php, mysql y wordpress.\nGestión de variables con Hiera: hiera.yaml, data/common.yaml Para gestionar las variables, se utiliza Hiera, lo que permite separar las claves del código fuente. Esto asegura un enfoque más seguro, ya que evita exponer credenciales sensibles al subir el proyecto a un repositorio en la nube. Aunque este proyecto es académico y no incluye variables encriptadas, Hiera también ofrece la posibilidad de encriptar claves.\nLas variables se declaran junto con sus valores en data/common.yaml.\nEl archivo hiera.yaml configura el funcionamiento de Hiera.\nPara integrar Hiera con Vagrant, se añade la línea puppet.hiera_config_path = \u0026quot;hiera.yaml\u0026quot; en el Vagrantfile.\nMódulos utilizados: El proyecto está dividido en tres módulos principales, lo que garantiza modularidad y organización en el código:\nMódulo apache Este módulo aprovisiona y configura el servidor web Apache en la MV, dejándolo preparado y activo para que el módulo wordpress pueda administrar y servir contenido desde él.\nMódulo mysql En este módulo se instala y configura un servidor MySQL en la MV, asegurando el correcto funcionamiento del gestor de bases de datos. Además, se crea la base de datos necesaria para WordPress mediante el archivo init-wordpress.sql.erb\nMódulo wordpress Este módulo instala y configura WordPress, dejándolo completamente funcional. Las principales acciones realizadas son:\nInstalación de los paquetes y dependencias de Wordpress y activación del servicio. Configuración del archivo wp-config.conf.erb, que configura el servicio, entre otras cosas, conecta WordPress con la base de datos y define claves de acceso generadas previamente. Instalación y uso de la herramienta wp-cli para automatizar la configuración del sitio web. Inicialización de la base de datos mediante el archivo init-wordpress-content.sql.erb con un contenido mínimo necesario para lanzar una página web. Configuración de Apache para servir el contenido de la página, utilizando el archivo wp-apache-config.conf.erb. El servicio es accesible desde el host en localhost:8080 gracias a la redirección del puerto 8080 del host al puerto 80 de la máquina virtual, donde Apache escucha las solicitudes HTTP entrantes.\nDespliegue del entorno en la versión puppet-one-node: ","date":"2024-11-25T00:00:00Z","image":"https://aleingmar-pi-portfolio.pages.dev/p/wordpress-puppet/wordpress-puppet_hu_be3819ba70d8c333.png","permalink":"https://aleingmar-pi-portfolio.pages.dev/es/p/wordpress-puppet/","title":"Despliegue automatizado de Wordpress usando Vagrant y Puppet"},{"content":"Este proyecto fue desarrollado para la asignatura de Contenedores como parte del máster universitario oficial en Desarrollo y Operaciones (DevOps). El objetivo principal del presente proyecto es llevar a cabo el despliegue de una aplicación multicapa contruida encima de diversos contenedores orquestados y desplegados usando la tecnología de Docker compose. El despliegue se basa y es una continuación del proyecto \u0026ldquo;Dockerización de aplicación multicapa\u0026rdquo; ya subido a este portfolio. Repositorio de GitHub: https://github.com/aleingmar/multi-layer-app-dockercompose\nMemoria del proyecto: Documentación del proyecto: Visualizar documentación en pdf\n","date":"2025-05-10T00:00:00Z","image":"https://aleingmar-pi-portfolio.pages.dev/p/multi-layer-dockercompose/dockerCompose_hu_61552419702a2d77.png","permalink":"https://aleingmar-pi-portfolio.pages.dev/es/p/multicapa-dockercompose/","title":"Despliegue de servicio multicontenedor"},{"content":"Este proyecto fue desarrollado para la asignatura de Contenedores como parte del máster universitario oficial en Desarrollo y Operaciones (DevOps). El objetivo principal del presente proyecto es llevar a cabo la dockerización de un aplicativo multicapa de carácter sencillo, diseñado con un enfoque pedagógico para ilustrar la arquitectura por capas y su despliegue mediante contenedores. Esta aplicación se ha concebido con tres capas diferenciadas: presentación web, lógica de negocio y persistencia de datos. A pesar de que inicialmente se planteó un stack de tipo MEAN (Mongo - Express – Angular - Node), la capa de presentación basada en Angular y Nginx no ha sido implementada en su totalidad. Por ello, la estructura final del proyecto queda configurada de la siguiente manera:\n•\tPrimera capa, capa de presentación: Nginx + Sitio web\n•\tSegunda capa, capa de lógica de negocio: Aplicación app.js desarrollada utilizando Express sobre Node.js.\n•\tCapa de persistencia: Implementada mediante MongoDB.\nLa funcionalidad del aplicativo es deliberadamente simple, con el objetivo de centrarse en la estructura y el despliegue. Se trata de una especie de \u0026ldquo;Hola Mundo\u0026rdquo;, en la que el cliente se conecta inicialmente a la capa de presentación (Nginx), que le entrega un fichero index.html. Este fichero contiene una llamada que provoca una segunda petición HTTP al mismo servidor Nginx, pero que es procesada de forma distinta en función del PATH de la solicitud.\nEsta petición es redirigida hacia la capa de backend, donde se encuentra desplegada la aplicación Express.\nDesde allí, la aplicación trata de establecer conexión con la base de datos MongoDB.\nEn función del resultado de dicha conexión, el backend responde al cliente con un mensaje en formato JSON, indicando si la conexión ha sido exitosa (\u0026ldquo;Hola mundo, conexión establecida correctamente\u0026rdquo;) o si ha ocurrido algún error.\nRepositorio de GitHub: https://github.com/aleingmar/multi-layer-app-dockerisation\nVídeo de la experimentación y memoria del proyecto: Documentación del proyecto: Visualizar documentación en pdf\n","date":"2025-04-19T00:00:00Z","image":"https://aleingmar-pi-portfolio.pages.dev/p/multilayer-dockerisation/dockerizacion_hu_fa22a3d72aa4d62f.png","permalink":"https://aleingmar-pi-portfolio.pages.dev/es/p/multicapa-dockerizacion/","title":"Dockerización de aplicación multicapa"},{"content":"Este proyecto fue desarrollado para la asignatura de Automatización de Despliegues, como parte del máster universitario oficial en Desarrollo y Operaciones (DevOps).\nEl objetivo del proyecto es desplegar y configurar de manera automatizada un entorno web en una máquina virtual que hospeda un servidor Apache, el cual sirve una página web básica. La máquina virtual se crea mediante IaC (Infrastructure as Code) con Vagrant, y para su aprovisionamiento se utiliza Puppet, que gestiona la instalación de Apache y la carga automática de un archivo HTML simple, creando así un servicio web funcional.\nEn definitiva, simplemente ejecutando un vagrant up comienza todo el proceso de despliegue y aprovisionamiento y de forma automática (sin hacer nada más) se levanta una máquina virtual en la cual se instala puppet, se configura e instala un servidor web Apache para que se active y escuche el puerto 80 (http) de la Mv y para que devuelva una página web simple que se introduce en su interior.\nRepositorio de GitHub: https://github.com/aleingmar/deployment_apache-puppet-vagrant\nEl repositorio de GiHub se compone de dos directorios con dos versiones distintas: /easy_mode y /hard_mode.\nEn la primera carpeta (/easy_mode) se encuentra el proyecto de despliegue con una estructura simplificada. Esta versión no sigue una arquitectura ni una organización de código propias de proyectos de despliegue complejos, y la configuración de Apache es más básica.\nEn la segunda carpeta (/hard_mode) se utiliza un patrón de código más adecuado para Puppet, empleando, por ejemplo, módulos y otros elementos típicos de esta tecnología. Además, la configuración de Apache es más avanzada y detallada.\nAmbas versiones consiguen realizar el despliegue correctamente.\nExplicando por ejemplo la versión sencilla (/easy_mode) un directorio donde se encuentra un fichero \u0026ldquo;Vagrantfile\u0026rdquo; y una carpeta \u0026ldquo;manifests\u0026rdquo; en cuyo interior se encuentra el fichero \u0026ldquo;apache.pp\u0026rdquo;.\nEn el Vagrantfile se define la infraestructura de máquinas virtuales que es necesaria desplegar para sustentar el servicio web. De esto se encarga Vagrant y por debajo, usa como proveedor de virtualización VirtualBox.\nEn el Vagrantfile antes de indicarle a Vagrant que debe hacer el aprovisionamiento de la infraestructura usando Puppet, se ejecuta un script para instalar Puppet dentro de las máquinas virtuales. Puppet en este caso trabaja en modo stand-alone (sin seguir modelo cliiente-servidor).\nEn el fichero \u0026ldquo;apache.pp\u0026rdquo; se define la configuración deseada para esta infraestructur y le sirve a Puppet de guía declarativa para desarrollar su trabajo. Como Puppet usa un lenguaje declarativo no se le indica como se quiere que se hagan las cosas, sino solo lo que se quiere conseguir y Puppet se encarga del resto.\nEl servicio es accesible desde el host en localhost:8080 gracias a la redirección del puerto 8080 del host al puerto 80 de la máquina virtual, donde Apache escucha las solicitudes HTTP entrantes.\n","date":"2024-10-11T00:00:00Z","image":"https://aleingmar-pi-portfolio.pages.dev/p/apache-web-puppet/apache-puppet_hu_27a2163075302ea8.png","permalink":"https://aleingmar-pi-portfolio.pages.dev/es/p/apache-web-puppet/","title":"Despliegue automatizado de Apache usando Vagrant y Puppet"},{"content":"Este proyecto consiste en conseguir alojar y gestionar de forma autónoma determinados servicios en mi propio servidor, una RasberryPi 5 de 8gb de Ram.\nPara la administración y configuración tanto del servidor como de sus servicios alojados, accedo de forma remota a través del protocolo SSH.\nEl servidor está asociado a un dominio principal gestionado por DuckDNS, lo que me permite acceder a los servicios de manera remota a través del navegador. Para evitar que la IP dinámica de mi red doméstica cambie y pierda el acceso al servidor cuento con un servicio que de forma autónoma actualiza automáticamente cada 5 minutos esta ip, asegurando que siempre esté correctamente sincronizada.\nPara organizar los accesos mediante subdominios y asegurar la conexión a mis servicios mediante HTTPS, utilizo Caddy como servidor web, que actúa como intermediario y maneja los certificados TLS/SSL, garantizando un acceso seguro y sin complicaciones.\nAdemás, he implementado un panel de control avanzado llamado Homarr, que me proporciona una interfaz centralizada desde la cual puedo iniciar sesión y acceder fácilmente a los diferentes servicios desplegados en el servidor.\nTodos los servicios alojados en el servidor incluyendo los mencionados anteriormente se alojan mediante contenedores Docker y se organizan en subdominios específicos. En el momento que escribo esto, los servicios no mencionados que aloja el servidor son:\nVaultwarden: Un gestor de contraseñas. Portainer: Un gestor de contenedores con interfaz web. Pi-hole: Un servicio de DNS bloqueador de anuncios. WireGuard: Un servicio VPN. WireGuard está integrado con Pi-hole. Esta configuración me permite no solo redirigir mi tráfico a través de mi servidor para asegurar mi conexión, sino también disfrutar de una navegación libre de anuncios, independientemente de dónde me encuentre.\n","date":"2024-08-01T00:00:00Z","image":"https://aleingmar-pi-portfolio.pages.dev/p/my-server/rasberry_hu_43d07e75b135520b.png","permalink":"https://aleingmar-pi-portfolio.pages.dev/es/p/my-server/","title":"Mi propio servidor RPI5"},{"content":"Durante la realización de mi Trabajo de Fin de Grado y las prácticas en empresa de la carrera, tuve la oportunidad de formar parte del grupo de investigación ES3 (Engineering and Science for Software Systems) de la Universidad de Sevilla. Grupo especializado en la investigación y desarrollo de sistemas software avanzados https://www.linkedin.com/company/grupoes3/.\nMi trabajo se centró en desarrollar parte de la plataforma screenRPA, una plataforma de Automatización Robótica de Procesos (RPA). Mi objetivo principal fue mejorar la capacidad de extracción de valor y conocimiento a partir de los procesos RPA. Para ello, trabajé en tres áreas diferentes:\nMejora de la plataforma web y visualización de resultados: Trabajé en el desarrollo de la plataforma web para visualizar y extraer resultados a lo largo de todo el pipeline del proceso RPA, facilitando así la interacción y el análisis de cada una de las fases por parte de los analistas. Todo ello siguiendo el patrón de diseño software Modelo Vista Controlador. Aplicación de técnicas avanzadas de minería de decisiones: Desarrollé y apliqué una nueva técnica basadas en Machine Learning, específicamente en la fase de descubrimiento de decisiones. Durante esta fase se busca entender porqué un usuario realiza una acción en vez de otra. La minería de decisiones es una especialidad dentro del campo de la minería de procesos. La aplicación de esta técnica permitió descubrir patrones y reglas no deterministas en las decisiones de los usuarios, proporcionando un mayor entendimiento de los procesos automatizados. Generación automática de informes empresariales: Implementé un sistema en la plataforma para la creación automática de reportes empresariales comprensibles por analistas y clientes. Estos informes desglosaban todo el conocimiento extraído por la plataforma tras su ejecución, facilitando la toma de decisiones estratégicas. Este proyecto me permitió aplicar conocimientos avanzados en automatización, Machine Learning y minería de procesos, contribuyendo a la evolución de una plataforma innovadora dentro del ámbito de la automatización empresarial.\nMemoria del trabajo fin de grado relacionado con este proyexto: Visualizar pdf\nParte de la plataforma screenRPA desarrollada en mi trabajo: ","date":"2024-06-06T00:00:00Z","image":"https://aleingmar-pi-portfolio.pages.dev/p/screenrpa-rpa-us/es3_hu_218834e5530632bd.png","permalink":"https://aleingmar-pi-portfolio.pages.dev/es/p/screenrpa-rpa-us/","title":"ScreenRPA"},{"content":"Este proyecto tiene como objetivo crear un portfolio web para runir todos los proyectos que he desarrollado durante mi carrera académica y personal, desplegarlo en mi propio servidor RPI5 y hacerlo accesible de manera segura desde internet.\nDesarrollo Web Para la construcción de mi portfolio elegí Hugo, una plataforma de generación de sitios web estáticos que permite crear páginas modernas de frontend utilizando archivos Markdown. La decisión de usar Hugo se basó en que ya había redactado una parte de mi portfolio en Obsidian, una herramienta (donde también se escribe en Markdown) que utilizo habitualmente para tomar notas y organizar mis apuntes. La capacidad de Hugo para aprovechar archivos en formato Markdown me permitió migrar este contenido fácilmente y enfocarme más en la calidad del contenido que en el desarrollo técnico.\nEl tema que seleccioné para mi portfolio es hugo-theme-stack, debido a su formato limpio y moderno, que se ajusta perfectamente a la estructura y diseño que buscaba. Esta plantilla, con su enfoque en el rendimiento y la simplicidad, me permitió optimizar el desarrollo de mi portfolio sin necesidad de invertir demasiado tiempo en el diseño de la interfaz. Aun así, he programado nuevas funcionalidades que difieren del proyecto base open source para mi uso personal.\nDevOps y Despliegue En cuanto al despliegue y la gestión operativa, mi portfolio está alojado en mi servidor Raspberry Pi 5 con 8 GB de RAM, lo que proporciona una solución eficiente y de bajo consumo energético. Para garantizar un entorno ordenado y aislado, utilizo Docker, donde el portfolio se ejecuta dentro de un contenedor. Esto me permite empaquetar la aplicación de forma independiente del resto del sistema, facilitando la gestión y evitando conflictos con otros servicios que corren en el mismo servidor.\nEl servidor web que gestiona las peticiones es Caddy, una solución ligera que me permite asegurar la conexión con HTTPS de manera automática y redirigir el tráfico a los diferentes servicios que tengo desplegados. Además de mi propio portfolio, en otro contenedor también alojo el portfolio de un compañero de carrera. Para monitorizar el portfolio web utilizo Google Analytics 4 (GA4) lo que me permite ver estadísticas sobre los accesos al sitio web.\nPara el desarrollo, suelo trabajar en local utilizando Visual Studio Code, aunque en ocasiones utilizo GitHub Codespaces cuando prefiero trabajar en un entorno remoto. El proceso de despliegue es sencillo: me conecto al servidor mediante SSH, realizo un pull de los últimos cambios desde GitHub y reinicio el contenedor Docker que ejecuta Hugo. Este flujo de trabajo está totalmente automatizado mediante un script de bash y un archivo Docker Compose, lo que simplifica el proceso de levantar la aplicación web con cada actualización.\nProceso de despliegue de cambios en el portfolio web con script de bash:\n","date":"2024-09-01T00:00:00Z","image":"https://aleingmar-pi-portfolio.pages.dev/p/hugo-portfolio/hugo-portfolio_hu_3980ec3f2ba1d714.png","permalink":"https://aleingmar-pi-portfolio.pages.dev/es/p/hugo-portfolio/","title":"Despliegue y creación de portfolio web con Hugo."},{"content":"Este proyecto fue desarrollado para la asignatura de Administración de Sistemas Cloud como parte del máster universitario oficial en Desarrollo y Operaciones (DevOps).\nEl objetivo principal del proyecto fue desarrollar scripts en Powershell para automatizar tareas comunes en entornos Windows, optimizando procesos que normalmente requerirían intervención manual.\nPiceladas conceptuales de PowerShell PowerShell es el intérprete de comandos y lenguaje de scripting de Microsoft, diseñado específicamente para la administración de sistemas Windows. Se basa en un lenguaje orientado a objetos, lo que significa que la salida de los comandos no es simplemente texto, sino objetos estructurados con propiedades y métodos que pueden ser manipulados fácilmente.\nPor ejemplo, en Bash, la comunicación entre procesos en las tuberías (|) se da en texto plano, lo que obliga a utilizar herramientas adicionales como grep, awk o sed para procesar la salida. En PowerShell, en cambio, las tuberías intercambian objetos, permitiendo trabajar directamente con sus atributos sin necesidad de hacer parsing de texto.\nEjemplo Comparativo de PowerShell vs. Bash PowerShell (trabajando con objetos):\nGet-Process | Where-Object { $_.CPU -gt 10 } | Select-Object Name, CPU\nAquí, Get-Process devuelve una lista de procesos como objetos, y filtramos aquellos cuyo uso de CPU ($_ .CPU) sea mayor a 10. Luego, seleccionamos solo las propiedades Name y CPU.\nBash (trabajando con texto):\nps aux | awk '$3 \u0026gt; 10 {print $11, $3}'\nps aux devuelve la lista de procesos como texto plano, por lo que es necesario usar awk para extraer y comparar la tercera columna (uso de CPU).\nTareas automatizadas mediante los scripts: En concreto las cuatro tareas a automatizar son las siguientes:\nListado de archivos según tamaño Escribir un script que muestre un listado de los ficheros del directorio actual que ocupe más de 1024 bytes.\nRenombrado de archivos JPG con fecha Escribir un script que renombre todos los ficheros con extensión JPG del directorio actual, añadiendo un prefijo con la fecha en formato año, mes, día. Por ejemplo, un fichero con nombre «imagen1.jpg» pasaría a llamarse «20240413-image1.jpg», si el script se ejecuta el 13 de abril de 2024.\nMonitorización del espacio en discos duros Programar un script en PowerShell que muestre los discos duros con un porcentaje de espacio libre inferior a un parámetro dado. El script debe imprimir la letra de la unidad y los valores en GB de espacio libre y tamaño sin decimales. La expresión Get-WmiObject Win32_LogicalDisk recupera la información de los discos del sistema.\nCreación de un menú interactivo Programar un script que muestre un menú con las siguientes opciones, de manera que se ejecute la opción asociada al número que introduzca el usuario:\nListar los servicios arrancados. Mostrar la fecha del sistema. Ejecutar el Bloc de notas. Ejecutar la Calculadora. Salir. Repositorio de GitHub: https://github.com/aleingmar/Powershell-Scripting\nVídeo de la experimentación y memoria del proyecto: Documentación del proyecto: Visualizar documentación en pdf\n","date":"2025-02-17T00:00:00Z","image":"https://aleingmar-pi-portfolio.pages.dev/p/powershell-scripting/powershell-logo_hu_6e5a868b72cd3b95.png","permalink":"https://aleingmar-pi-portfolio.pages.dev/es/p/powershell-scripting/","title":"PowerShell Scripting"},{"content":"Este proyecto fue desarrollado para la asignatura de Codificación y gestión de información sanitaria (CGIS) durante mi tercer año de carrera. El objetivo principal del proyecto es diseñar, modelar y desarrollar una aplicación web para gestión las jornadas laborales del personal sanitario de una entidad hospitalaria.\nDe forma general, por parte de los roles normales de profesionales sanitarios, la aplicación permite que puedan consultar sus accesos al centro hospitalario y crear incidencias si ven que hay alguna anomalía con el registro. Por parte de los roles de adminsitración y responsabilidad, la aplicación permite monitorizar y llevar un control de que los trabajadores cumplen con sus jornadas laborales y posibilita tramitar aquellas incidencias que los profesionales imponen al ver un problema con el registro de sus accesos.\nAdemás de esto, la aplicación permite funcionalidades comunes a todos los roles como son la creación de cuentas de usuarios, la gestión de perfil de usuario\u0026hellip;\nDocumentación Índice Dominio_del_problema Objetivos Usuarios_del_sistema: Requisitos_de_información: Requisitos_funcionales: Reglas_de_negocio: Requisitos_no_funcionales: Modelo_conceptual_UML Dominio del problema: En la actualidad el sistema por el que se rigen las jornadas laborales de los profesionales sanitarios es muy complejo. Eventos como las guardias, los cambios de turnos o las rotaciones entre centros son el pan de cada día en el sector. Esto hace que la organización de este sistema sea muy difícil de gestionar y que en muchas ocasiones no se lleguen a registrar muchos de estos sucesos.\nObjetivos: El objetivo de nuestro sistema será capacitar a los profesionales del sector de una herramienta que les permita tanto a la dirección como a los profesionales sanitarios una forma de gestión que solvente eficaz y eficientemente la problemática anterior . Facilitando en gran medida los procesos y permitiendo que los propios profesionales tengan un papel protagonista en estos. Los principales objetivos de nuestro sistema serán:\nOBJ-1. Gestión de los accesos al centro hospitalario:\nNuestro sistema tendrá como uno de sus objetivos llevar a cabo un control/seguimiento de los accesos al centro hospitalario por parte del personal sanitario. Permitiendo registrar con exactitud el número de horas ejercidas por los profesionales, evitar la saturación del personal, notificar incidencias en los accesos, detectar posibles fraudes…\nOBJ-2.Registro y gestión de incidencias sobre los accesos del personal sanitario:\nNuestro sistema tendrá como uno de sus objetivos llevar a cabo un control de los accesos al centro hospitalario por parte del personal sanitario mediante el uso de incidencias. Permitiendo registrar la fecha de presentación de la incidencia, la fecha de la respuesta, el estado, el motivo de presentación y respuesta\u0026hellip;\nOBJ-3. Registro y gestión de información del personal sanitario:\nNuestro sistema tendrá como uno de sus objetivos llevar a cabo el registro y la gestión de la información del personal sanitario. Permitiendo registrar información personal sobre los profesionales, sobre su especialidad, cargo…\nUsuarios del sistema: Los tipos de usuarios que podrán acceder al sistema y hacer un uso específico de este serán:\nProfesionales Sanitarios (médicos y enfermeros) .\nDirección\nJefes de guardia.\nAdministrador\nRequisitos de información: RI-001. Información sobre los usuarios: El sistema deberá almacenar datos personales sobre todos los usuarios. Dirección de correo electrónico principal y segundario, contraseña y nombre.\nRI-002. Información sobre el personal sanitario: El sistema deberá almacenar datos del personal sanitario. Tipo de profesión (médico y enfermero), especialidad médica y cargo dentro del sistema hospitalario (Dirección, jefe de guardias, sanitario normal).\nRI-003. Información sobre las especialidades médicas: El sistema deberá almacenar datos sobre las especialidades médicas del personal sanitario. Nombre de especialidad (cardiología, radiología y pediatría).\nRI-004. Información sobre los accesos al centro sanitario: El sistema deberá registrar datos sobre los accesos al centro sanitario por parte del personal sanitario .Fecha/hora de entrada, fecha/hora de salida y número de horas trabajadas en cada joornada.\nRI-006. Información sobre incidencias : El sistema deberá registrar datos sobre las incidencias (con respecto a problemas con los accesos) que notifiquen el personal sanitario. Fecha/hora en la que se realiza la incidencia, motivo de la incidencia, acceso a la que hace referencia , estado de la incidencia (aceptada, rechazada, pendienteRespuesta), motivo de la respuesta y sanitario que la realiza.\nRequisitos funcionales: RF-001. Registro de usuarios: (todos los roles)\nQueremos que el sistema nos permita a los profesionales sanitarios registrarnos como usuarios con una contraseña y acceder al sistema.\nACCESOS\nRF-002. Crear acceso: (direccion, administrador)\nQuiero poder crear accesos a los sanitarios que sean compatibles con mi responsabilidad.\nRF-003. Cosultar mis accesos: (todos los roles)\nQuiero poder consultar mi historial de accesos al centro médico.\nRF-004. Consultar accesos: (jefe de guardia, direccion, administrador)\nQuiero poder consultar los acceso de los sanitarios que sean compatibles con mi responsabilidad.\nRF-005. Consultar en detalle un acceso: (todos los roles)\nQuiero poder consultar en detalle un acceso de mis listado de accesos.\nRF-006. Modificar acceso: (direccion, administrador)\nQuiero poder modificar los accesos de los sanitarios que sean compatibles con mi responsabilidad.\nRF-007. Borrar acceso: (direccion, administrador)\nQuiero poder borrar accesos a los sanitarios que sean compatibles con mi responsabilidad.\nRF-008. Filtrar accesos por fechas: (todos los roles)\nQuiero poder filtrar por fechas mi historial de accesos al centro médico.\nINCIDENCIAS\nRF-009. Crear registro de incidencias: (profesional sanitario, jefe de guardias)\nQuiero poder notificar de cualquier incidencia sobre los registros de mis accesos.\nRF-0010. Consultar incidencias: (direccion, administrador)\nQuiero poder consultar un listado de las incidencias de los sanitarios.\nRF-0011. Consultar mis incidencias: (profesional sanitario, jefe de guardias, direccion, administrador)\nQuiero poder consultar un listado de mis incidencias.\nRF-0012. Consultar en detalle incidencias: (todos los roles)\nQuiero poder consultar en detalle las incidencia.\nRF-0013. Modificar incidencias: (profesional sanitario, jefe de guardias)\nQuiero poder modificar mis incidencias.\nRF-0014. Borrar incidencias: (profesional sanitario)\nQuiero poder borrar mis incidencia sobre los registros de mis accesos.\nRF-015. Solucionar incidencias: (direccion, administrador)\nQuiero poder aprobar o denegar a las incidencias haciendoles saber el motivo de la resolución.\nSANITARIOS\nRF-0016. Crear sanitarios : (direccion, administrador )\nQuiero poder crear un nuevo sanitario.\nRF-0017. Cosultar sanitarios: (jefe de guardia, direccion, administrador )\nQuiero poder consultar un listado de los sanitarios que sean compatibles con mi responsabilidad.\nRF-0018. Consultar en detalle sanitarios : (direccion, administrador )\nQuiero poder ver en detalle los datos de un sanitario.\nRF-0019. Cosultar en detalle sanitarios: (jefe de guardia, direccion, administrador )\nQuiero poder consultar en detalle un sanitario del listado de los sanitarios que sean compatibles con mi responsabilidad.\nRF-0020. Modificar sanitarios : (direccion, administrador )\nQuiero poder modificar los datos de un sanitario.\nRF-0021. Borrar sanitarios : (direccion, administrador )\nQuiero poder borrar los datos de un sanitario.\nRF-0022. Filtrar sanitarios por nombre: (jefe de guardia, direccion, administrador )\nQuiero poder filtrar por nombre el listado de los sanitarios que sean compatibles con mi responsabilidad.\nRF-0023. Filtrar sanitarios por nombre: (direccion, administrador )\nQuiero poder filtrar por profesión el listado de los sanitarios .\nReglas de negocio: RN-001. : No especificadas\nRequisitos no funcionales: RNF-001. Seguridad: El sistema debe estar protegido contra el acceso no autorizado.\nRNF-002. Actuación: El sistema debe poder manejar el número requerido de usuarios sin ninguna degradación en el rendimiento.\nRNF-003. Escalabilidad: El sistema debe ser capaz de escalar hacia arriba o hacia abajo según sea necesario.\nRNF-004. Disponibilidad: El sistema debe estar disponible cuando sea necesario.\nRNF-005. Mantenimiento: El sistema debe ser fácil de mantener y actualizar.\nRNF-006. Portabilidad: El sistema debe poder ejecutarse en diferentes plataformas con cambios mínimos.\nRNF-007. Fiabilidad: El sistema debe ser confiable y cumplir con los requisitos del usuario.\nRNF-008. Usabilidad: El sistema debe ser fácil de usar y comprender.\nRNF-009. Compatibilidad: El sistema debe ser compatible con otros sistemas.\nRNF-010. Compliancia: El sistema debe cumplir con todas las leyes y reglamentos aplicables\nEl sistema debe de tener una disponibilidad del 99,96%.\nModelo conceptual UML ","date":"2022-01-01T00:00:00Z","image":"https://aleingmar-pi-portfolio.pages.dev/p/cgis-laravel-jornadas/logo-laravel_hu_ebc70115903d4894.png","permalink":"https://aleingmar-pi-portfolio.pages.dev/es/p/cgis-laravel-jornadas/","title":"Aplicación web para la gestión de jornadas laborales"},{"content":"Este proyecto fue desarrollado durante la asignatura de Gestión de Servicios y Tecnologías de la Información (GSTI) durante mi cuarto año de carrera. El objetivo principal del proyecto era desarrollar una aplicación móvil Android que cubriera la necesidad de teleasistencia de un proceso asistencial integrado (PAI) modelado por el Servicio Andaluz de Salud (SAS). En concreto, esta aplicación se centra en el proceso asistencial de teleasistencia dermatológica de pacientes con cáncer de piel.\nEl propósito de la aplicación es facilitar la interacción entre el paciente y el médico dermatólogo, acelerando la comunicación y la toma de decisiones clínicas, como parte de un sistema de e-Health. Esta aplicación proporciona un entorno que permite al paciente compartir imágenes de su evolución con el dermatólogo, quien puede responder y hacer un seguimiento casi inmediato, reduciendo los tiempos de espera habituales. Estas imágenes son almacenadas en Goggle Cloud lo que asegura la escalabilidad del sistema y la protección ante pérdidas.\nAdemás, la aplicación incluye otras funcionalidades clave como la creación de citas y teleconsultas a través de la app, el acceso a la ubicación de centros médicos cercanos en Google Maps, la gestión de perfiles tanto de médicos como de pacientes\u0026hellip;\nMemoria del proyecto: Visualizar pdf\n","date":"2024-02-15T00:00:00Z","image":"https://aleingmar-pi-portfolio.pages.dev/p/android-gsti/avi-health_hu_3ec35e6a16462f08.png","permalink":"https://aleingmar-pi-portfolio.pages.dev/es/p/android-gsti/","title":"AVI Health"},{"content":"Este proyecto fue desarrollado para un trabajo de la asignatura Arquitecturas de Sistemas y Sistemas Distribuidos (ASSB) durante mi cuarto año de carrera. El objetivo principal era desplegar un servicio web funcional en un entorno en la nube utilizando los servicios de AWS, manteniéndonos dentro de los límites del plan gratuito.\nEl servicio web desplegado se conforma por un balancedor de carga que distribuye las solicitudes entre dos instancias de EC2, cada una con su propio servidor Apache y página web.\nLas principales tareas realizadas incluyeron:\nControl y creación de alertas de costes: Configuración de alertas en AWS para asegurar que todas las operaciones se ajustaran al plan gratuito, evitando cargos adicionales no deseados. Despliegue de instancias EC2: Levantar dos instancias de EC2 y conectarnos a ellas mediante SSH. En cada una de las instancias, instalar el servidor web Apache. Configuración de servidores web Apache: Configuración de los servicios de Apache en ambas instancias para que sirvieran una página web estática creada por mi mismo. Implementación de un balanceador de carga: Despliegue de un balanceador de carga en AWS para distribuir de manera equitativa las peticiones entrantes entre los dos servidores Apache, optimizando la carga y asegurando una mayor disponibilidad. Visualizar memoria en pdf\n","date":"2024-01-01T00:00:00Z","image":"https://aleingmar-pi-portfolio.pages.dev/p/aws-ec2-assb/aws-ec2_hu_b6702369230b4c85.png","permalink":"https://aleingmar-pi-portfolio.pages.dev/es/p/aws-ec2-assb/","title":"Despliegue de servicio web con EC2 AWS"},{"content":"Este proyecto fue desarrollado para la asignatura Arquitectura de Sistemas y Software de Base (ASSB), durante mi cuarto año de carrera. El objetivo principal era familiarizarse con la programación paralela en computadores de memoria distribuida utilizando la librería MPI en C (Message-Passing Interface). Una técnica muy utilizada para la computación distribuida en múltiples máquinas, comúnmente utilizado en clústeres de alto rendimiento. En el caso de este proyecto todo se ejecutó en mi propio computador, entendiendo mi computador como una especie de clúster y sus diferentes núcleos como máquinas que forman este.\nLas principales tareas de este proyecto son:\nDesarrollo del programa de \u0026ldquo;Hola Mundo\u0026rdquo; con MPI: Como primer paso, programé un programa básico para que cada proceso imprimiera un mensaje con su rango y el nombre del procesador en el que estaba ejecutándose. Además, experimenté con la posibilidad de lanzar más procesos que el número de núcleos físicos disponibles, observando cómo MPI gestiona este escenario aunque pierda rendimiento.\nProducto escalar de vectores en paralelo: Implementé un programa con MPI que calcula el producto escalar de dos vectores de gran tamaño, distribuyendo el trabajo entre varios procesos. Cada proceso calculaba una parte del producto escalar, y luego los resultados se reunían en el proceso de rango 0, que mostraba el resultado total. También se añadió una medición del tiempo de ejecución, lo que permitió analizar cómo variaba el rendimiento al cambiar el número de procesos.\nResolución de integrales con el método de los trapecios: Posteriormente, implementé un programa para calcular integrales mediante el método de los trapecios, paralelizando el programa para que cada proceso calculara la suma de los trapecios en un subintervalo específico. Al igual que antes, el proceso de rango 0 era responsable de sumar los resultados de todos los procesos y mostrar el valor final de la integral.\nAnálisis de rendimiento y escalabilidad: Para evaluar el rendimiento del programa paralelo, medí los tiempos de ejecución y la aceleración al utilizar diferentes cantidades de procesos, desde 1 hasta más del doble del número de núcleos físicos disponibles. Los resultados se visualizaron mediante gráficos que mostraban cómo la aceleración mejoraba conforme aumentaba el número de procesos, pero también cómo disminuía la eficiencia en ciertos puntos debido a la sobrecarga en la comunicación entre procesos.\nDocumentación del proyecto: Visualizar documentación en pdf\n","date":"2023-11-01T00:00:00Z","image":"https://aleingmar-pi-portfolio.pages.dev/p/mpi-assb/mpi-logo_hu_e815c776ab5c34cd.png","permalink":"https://aleingmar-pi-portfolio.pages.dev/es/p/mpi-assb/","title":"Computación paralela en sistemas distribuidos con MPI."},{"content":"Este proyecto fue desarrollado durante la asignatura de Arquitectura de Sistemas y Software de Base (ASSB) durante mi cuarto año de carrera. El objetivo principal era implementar un algoritmo para el cálculo del número Pi utilizando el método de MonteCarlo programando en C tanto de forma secuencial como utilizando técnicas de programación paralela para aprovechar al máximo los recursos de los procesadores multinúcleo mediante la librería OpenMP, que permite ejecutar código en múltiples hilos de manera eficiente.\nLas principales tareas realizadas fueron las siguientes:\nDesarrollo de la versión secuencial y paralela del algoritmo: Implementé un programa que simula el lanzamiento de \u0026ldquo;dardos\u0026rdquo; aleatorios dentro de un cuadrado inscrito en un círculo. La relación entre los aciertos dentro del círculo y los lanzamientos totales permite calcular el valor de Pi. En la versión paralela, utilicé OpenMP para dividir el trabajo entre varios hilos, aprovechando al máximo los recursos de los procesadores multinúcleo.\nMediciones de tiempo y análisis de rendimiento: Tras desarrollar las dos versiones del programa, realicé mediciones de tiempo para evaluar el rendimiento de la versión paralela en comparación con la secuencial. Utilicé diferentes configuraciones de hilos, desde un solo hilo hasta más del doble de los núcleos físicos del procesador, con el objetivo de analizar la aceleración y escalabilidad del algoritmo. La aceleración se calculó como la relación entre el tiempo de ejecución en un único hilo y el tiempo de ejecución con varios hilos.\nOptimización y gestión de recursos compartidos: Durante el desarrollo, fue necesario resolver problemas comunes de la programación paralela, como las condiciones de carrera. En este caso, utilicé directivas de OpenMP para definir variables privadas en cada hilo, evitando que varios hilos accedieran simultáneamente a las mismas variables globales y afectaran el resultado final.\nGeneración de gráficos de rendimiento: Tras recopilar los datos de tiempos de ejecución y aceleración, generé gráficos para visualizar el rendimiento del programa a medida que aumentaba el número de hilos. Estos gráficos demostraron cómo la aplicación escalaba con un mayor número de hilos, destacando las ventajas y limitaciones de la paralelización en un entorno de memoria compartida.\nDocumentación del proyecto: Visualizar documentación en pdf\n","date":"2023-10-01T00:00:00Z","image":"https://aleingmar-pi-portfolio.pages.dev/p/openmp-assb/openmp-logo_hu_fd6b904dd9b2afbc.png","permalink":"https://aleingmar-pi-portfolio.pages.dev/es/p/openmp-assb/","title":"Computación paralela en procesadores multinúcleos con OpenMP."},{"content":"Este proyecto fue desarrollado durante mi tercer año de carrera para el Datathon organizado por Dedalus y AWS en Andalucía, en colaboración con dos compañeros de carrera. El principal objetivo de este datathon era trabajar con un conjunto de datos proporcionados sobre pacientes ingresados en Unidades de Cuidados Intensivos (UCI), para extraer valor clínico mediante análisis de cohortes y la creación de modelos predictivos. Se buscaba con este concurso generar información relevante para mejorar la eficiencia hospitalaria y optimizar la toma de decisiones en contextos clínicos.\nAnálisis de Cohortes El análisis de cohortes fue uno de los pilares fundamentales del proyecto, ya que nos permitió identificar patrones clave entre los pacientes hospitalizados.\nRealizamos un análisis específico sobre pacientes con problemas cardiovasculares. Este análisis incluyó varios aspectos importantes:\nEl porcentaje de casos en función del género, que nos ayudó a identificar posibles diferencias en la prevalencia de los problemas cardiovasculares entre hombres y mujeres. La distribución de casos en relación con la edad, lo que permitió obtener un panorama claro sobre las franjas etarias más afectadas. El número de casos según el índice de masa corporal (IMC), que proporcionó una perspectiva sobre cómo el IMC influye en la incidencia de estas patologías. El porcentaje de mortalidad en función del IMC, revelando posibles correlaciones entre el peso corporal y los desenlaces fatales en pacientes con problemas cardiovasculares. Realizamos un análisis de cohortes más general para todos los pacientes ingresados en la UCI. Este estudio incluyó:\nEl número de ingresos y altas en la UCI en distintas franjas horarias, lo que proporcionó información sobre los momentos del día en los que se producían más admisiones. La tasa de mortalidad de los pacientes en función del diagnóstico, arrojando luz sobre cuáles eran las patologías más críticas en términos de supervivencia. El tiempo de estancia en UCI según cada diagnóstico, lo que facilitó la identificación de los tratamientos y patologías que requerían mayor tiempo de hospitalización. Comparativa de Hospitales Otro aspecto destacado del proyecto fue el análisis comparativo entre diferentes hospitales. En este análisis, evaluamos el rendimiento de los hospitales en función de su capacidad para tratar patologías críticas, utilizando la mortalidad como indicador clave. Este análisis nos permitió comparar la eficiencia de los hospitales en el tratamiento de ciertos diagnósticos y patologías, además de explorar el tiempo medio de estancia en la UCI por cada hospital. Con esta información, pudimos identificar posibles áreas de mejora en la gestión hospitalaria y la atención clínica en diferentes centros de salud.\nModelos Predictivos Como parte del proyecto, también desarrollamos modelos predictivos con el fin de anticipar comportamientos y mejorar la planificación en las UCI. Uno de los modelos principales fue el de control de la congestión en las UCI, que tenía la capacidad de predecir cuándo se alcanzarían niveles críticos de ocupación y así anticiparse a posibles cuellos de botella en la atención hospitalaria.\nAdemás, diseñamos modelos que podían predecir la duración de la estancia de los pacientes en la UCI, clasificándolos en estancias cortas, medias o largas. Estos modelos predictivos no solo permitieron optimizar la asignación de recursos hospitalarios, sino que también contribuyeron a una mejor planificación de las altas médicas y a la reducción de tiempos de espera.\nPresentación: Visualizar presentación en pdf\n","date":"2023-02-01T00:00:00Z","image":"https://aleingmar-pi-portfolio.pages.dev/p/datathon/dedalus-logo_hu_8f03971c65980bcc.png","permalink":"https://aleingmar-pi-portfolio.pages.dev/es/p/datathon/","title":"Datathon Andalucía Dedalus"},{"content":"Este proyecto fue desarrollado durante la asignatura de Seguridad, Confidencialidad y Gestión de la Identidad (SCGI) durante mi tercer año de carrera. Este proyecto se centró en el diseño y desarrollo de una arquitectura cliente-servidor segura utilizando el protocolo Transport Layer Security (TLS), con el objetivo de asegurar la confidencialidad, integridad y autenticidad de las comunicaciones.\nContexto y Objetivo El proyecto surgió de la necesidad de asegurar las comunicaciones en una aplicación que permitiera la transmisión de datos sensibles, como credenciales de usuario (login/password), entre clientes y un servidor central. Este tipo de aplicaciones es especialmente relevante en entornos donde la privacidad de los datos es crucial, como el sector de la salud, la banca y cualquier sistema donde los usuarios deban autenticar su identidad para acceder a servicios.\nConfiguración de KeyStores y TrustStores Para implementar la seguridad mediante TLS, fue necesario configurar tanto el almacén de claves (KeyStore) como el almacén de certificados (TrustStore). Estos elementos permiten la autenticación mutua entre el cliente y el servidor. El KeyStore contiene las claves privadas y los certificados asociados que identifican a la entidad (cliente o servidor), mientras que el TrustStore gestiona los certificados de las entidades de confianza, permitiendo validar que las contrapartes de la comunicación son legítimas.\nEn el desarrollo de este proyecto, se utilizó Keytool, una herramienta incluida en el JDK de Java, para crear y gestionar los certificados. El proceso requirió la configuración de las variables de entorno como JAVA_HOME y PATH, para facilitar el uso de esta herramienta desde la línea de comandos.\nImplementación de Sockets TLS La comunicación entre el cliente y el servidor se realizó mediante sockets SSL (Secure Sockets Layer), sobre los cuales se integró el protocolo TLS. En ambos extremos, cliente y servidor, se configuraron sockets seguros que permitían la transmisión de datos encriptados. El servidor estaba diseñado para escuchar conexiones entrantes en un puerto específico (en este caso, el puerto 3343), autenticando a los clientes mediante la verificación de sus credenciales.\nServidor TLS: El servidor se encargaba de recibir las conexiones de los clientes y autenticar a cada usuario mediante la verificación de sus credenciales (nombre de usuario y contraseña). Además, proporcionaba respuestas basadas en el resultado de esta verificación, informando al cliente si la autenticación había sido exitosa o fallida.\nCliente TLS: El cliente establecía una conexión segura con el servidor y enviaba las credenciales del usuario para su verificación. La respuesta del servidor se mostraba en una interfaz de usuario sencilla, indicando si el proceso de autenticación había sido exitoso o si hubo algún error.\nPruebas de Comunicación y Seguridad Se realizaron pruebas de comunicación tanto con seguridad como sin ella, con el fin de destacar la importancia del uso de TLS en aplicaciones que manejan información sensible. Para las pruebas sin seguridad, se implementó un socket sin cifrado que permitía observar cómo un atacante, usando herramientas como Wireshark, podía capturar y visualizar en texto claro las credenciales de usuario transmitidas.\nEn contraste, al habilitar TLS, se observó cómo toda la información transmitida se encontraba encriptada, imposibilitando que un atacante pudiera leer los datos capturados en la red. Esto demostró la efectividad de TLS para proteger la confidencialidad y la integridad de la información.\nRendimiento y Concurrencia Un aspecto clave de este proyecto fue la validación del rendimiento del sistema, específicamente la capacidad de manejar múltiples conexiones concurrentes. El servidor fue configurado para gestionar hasta 300 conexiones simultáneas, lo cual fue logrado utilizando hilos (threads) en Java para manejar las solicitudes en paralelo. A pesar de que las conexiones se realizaban de forma secuencial en las pruebas iniciales (una cada segundo), el sistema demostró ser robusto al procesar las peticiones de forma eficiente sin comprometer la seguridad o la estabilidad.\nDocumentación del proyecto: Visualizar documentación en pdf\n","date":"2021-12-01T00:00:00Z","image":"https://aleingmar-pi-portfolio.pages.dev/p/security-gasca/java3_hu_8c6b53f9c2d2359c.png","permalink":"https://aleingmar-pi-portfolio.pages.dev/es/p/seguridad-gasca/","title":"Implementación de una Arquitectura Segura Cliente-Servidor en Java utilizando TLS"},{"content":"Este proyecto fue desarrollado como parte de la asignatura de Sistemas Inteligentes (SI) durante mi tercer año de carrera. El trabajo consistió en aplicar diferentes algoritmos de clasificación y preprocesamiento de datos para resolver un problema hipotético sobre un viaje espacial en un futuro distante. El objetivo principal era predecir, basado en ciertos atributos de los pasajeros, si estos serían transportados a una dimensión alternativa tras un accidente espacial.\nContexto y Objetivo El problema plantea la situación en la que una nave espacial, con miles de pasajeros a bordo, choca con una anomalía espacio-temporal, y la mitad de los pasajeros son transportados a otra dimensión. El reto del proyecto consistía en desarrollar un modelo predictivo para identificar qué pasajeros habrían sido transportados a esa dimensión alternativa, basado en atributos como la edad, el planeta de origen, si estaban en crio sueño, y otros factores.\nPreprocesamiento de Datos El conjunto de datos proporcionado contenía información sobre aproximadamente 8700 pasajeros, y constaba de 14 atributos, tanto numéricos como nominales. Inicialmente, se eliminaron ciertas variables consideradas irrelevantes, como el destino del pasajero y las cantidades gastadas en servicios de lujo. A continuación, se realizaron imputaciones de valores perdidos y se binarizaron las variables categóricas utilizando One Hot Encoding.\nPara mejorar la eficiencia de los algoritmos de clasificación basados en distancia, como KNN, se realizó la normalización de las variables numéricas, en particular la edad de los pasajeros, con el fin de garantizar una comparación justa entre los diferentes atributos.\nAlgoritmos Implementados Se probaron diversos algoritmos de clasificación, entre ellos:\nZeroR: Utilizado como baseline, clasificando todos los pasajeros en función del valor de la clase más frecuente. J48: Un algoritmo de tipo árbol de decisión que mostró buenos resultados tras un proceso de poda para evitar el sobreajuste. KNN (k-Nearest Neighbors): Este clasificador se basó en la similitud entre individuos para predecir si un pasajero sería transportado. Se probaron diferentes valores de K, siendo 9 el valor que obtuvo mejores resultados. Naive Bayes: Un algoritmo que, pese a su simplicidad, proporcionó buenos resultados al asumir la independencia entre los atributos. Resultados El mejor rendimiento se obtuvo utilizando el algoritmo KNN con un valor de K=9, logrando una precisión del 74.3%. En comparación, los otros algoritmos mostraron rendimientos ligeramente inferiores. Los experimentos realizados con validación cruzada ayudaron a evaluar de manera más confiable los modelos, asegurando que no hubiese sobreajuste.\nDocumentación del proyecto: Visualizar documentación en pdf\n","date":"2021-10-01T00:00:00Z","image":"https://aleingmar-pi-portfolio.pages.dev/p/weka-espacial/weka_hu_49143c2bd7a552bc.png","permalink":"https://aleingmar-pi-portfolio.pages.dev/es/p/weka-espacial/","title":"Proyecto de datos con Weka"},{"content":"Este proyecto fue desarrollado durante la asignatura de Bases de Datos (BD) durante mi segundo año de carrera. El objetivo principal era diseñar, modelar y desarrollar una base de datos SQL para gestionar y almacenar la información de las instalaciones y equipamientos electromédicos de un quirófano. La base de datos permitiría no solo gestionar los equipos y su mantenimiento, sino también asegurar el cumplimiento de regulaciones sanitarias, controlar el valor del equipamiento, supervisar las revisiones periódicas\u0026hellip;\nAlgunas de las tareas realizadas incluyen:\nDiseño y modelado de la base de datos: A partir de un análisis exhaustivo de los requisitos, se elaboró un modelo relacional que representa los diferentes elementos del quirófano, los equipos electromédicos, las instalaciones de climatización y electricidad, y los perfiles de los usuarios que gestionan esta información (ingenieros biomédicos, ingenieros de mantenimiento y directores de servicios generales).\nGestión del ciclo de vida de los equipos: La base de datos almacena información clave sobre el equipamiento, como su fecha de adquisición, vida útil, proveedor, valor de compra y parámetros técnicos que deben mantenerse dentro de ciertos rangos para asegurar su correcto funcionamiento. Esto permite controlar los plazos de mantenimiento y prever sustituciones de manera oportuna.\nAutomatización de revisiones: A través de triggers y procedimientos almacenados, la base de datos es capaz de generar alertas cuando se acerquen las fechas de revisión o cuando un equipo esté próximo a superar su vida útil. Esto asegura que el quirófano se mantenga dentro de los estándares requeridos sin interrupciones imprevistas​.\nConsultas avanzadas y generación de reportes: Implementación de una serie de vistas y consultas para facilitar el acceso a la información relevante, como el estado de las revisiones, el coste total del equipamiento o el historial de mantenimientos realizados por los ingenieros. Esto permite a los usuarios obtener informes personalizados que pueden ser utilizados para la toma de decisiones estratégicas dentro del hospital.\nDocumentación del proyecto: Visualizar documentación en pdf\n","date":"2021-04-01T00:00:00Z","image":"https://aleingmar-pi-portfolio.pages.dev/p/bd-quirofano/bd_hu_ef8b52eca84df40a.png","permalink":"https://aleingmar-pi-portfolio.pages.dev/es/p/bd-quirofano/","title":"Base de datos relacional para la gestión del equipamiento de quirófanos."}]