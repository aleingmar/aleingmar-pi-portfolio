<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>MPI on Alejandro Inglés Martínez</title><link>https://aleingmar-pi-portfolio.duckdns.org/tags/mpi/</link><description>Recent content in MPI on Alejandro Inglés Martínez</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 01 Nov 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://aleingmar-pi-portfolio.duckdns.org/tags/mpi/index.xml" rel="self" type="application/rss+xml"/><item><title>Parallel computing in distributed systems with MPI.</title><link>https://aleingmar-pi-portfolio.duckdns.org/p/mpi-assb/</link><pubDate>Wed, 01 Nov 2023 00:00:00 +0000</pubDate><guid>https://aleingmar-pi-portfolio.duckdns.org/p/mpi-assb/</guid><description>&lt;img src="https://aleingmar-pi-portfolio.duckdns.org/p/mpi-assb/mpi-logo.png" alt="Featured image of post Parallel computing in distributed systems with MPI." /&gt;&lt;p&gt;This project was developed for the course Architecture of Systems and Software Basis (ASSB), during my fourth year of studies. The main objective was to become familiar with parallel programming on distributed memory computers using the &lt;strong&gt;MPI&lt;/strong&gt; library in &lt;strong&gt;C&lt;/strong&gt; (Message-Passing Interface). A widely used technique for distributed computing on multiple machines, commonly used in high performance clusters. In the case of this project everything was executed in my own computer, understanding my computer as a kind of cluster and its different cores as machines that form it.&lt;/p&gt;
&lt;p&gt;The main tasks of this project are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Development of the ‘Hello World’ program with MPI&lt;/strong&gt;: As a first step, I programmed a basic program so that &lt;strong&gt;each process would print a message with its rank and the name of the processor it was running on&lt;/strong&gt;. In addition, I experimented with the possibility of launching more processes than the number of physical cores available, observing how MPI handles this scenario even though it loses performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalar product of vectors in parallel&lt;/strong&gt;: I implemented a program with MPI that &lt;strong&gt;computes the scalar product of two large vectors&lt;/strong&gt;, distributing the work among several processes. Each process calculated a part of the scalar product, and then the results were brought together in the process of rank 0, which displayed the total result. A runtime measurement was also added, allowing analysis of how performance varied as the number of processes changed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Solving integrals using the trapezoid method&lt;/strong&gt;: I then &lt;strong&gt;implemented a program to calculate integrals using the trapezoid method&lt;/strong&gt;, parallelising the program so that each process calculated the sum of the trapezoids in a specific sub-interval. As before, the process at rank 0 was responsible for summing the results of all the processes and displaying the final value of the integral.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Performance and scalability analysis&lt;/strong&gt;: To evaluate the performance of the parallel program, I measured execution times and speedup when using different numbers of processes, from 1 to more than twice the number of available physical cores. The results were visualised in graphs showing how speedup improved as the number of processes increased, but also how efficiency decreased at certain points due to overhead in inter-process communication.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Project documentation: &lt;a class="link" href="../../post/paralelizacion-mpi-assb/ASSB-mpi.pdf" &gt;&lt;strong&gt;View documentation in pdf&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;</description></item></channel></rss>