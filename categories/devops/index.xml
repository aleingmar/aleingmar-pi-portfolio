<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>DevOps on Alejandro Inglés Martínez</title><link>https://aleingmar-pi-portfolio.duckdns.org/categories/devops/</link><description>Recent content in DevOps on Alejandro Inglés Martínez</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 17 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://aleingmar-pi-portfolio.duckdns.org/categories/devops/index.xml" rel="self" type="application/rss+xml"/><item><title>Test Automation with Python</title><link>https://aleingmar-pi-portfolio.duckdns.org/p/automatic-tests-python/</link><pubDate>Sat, 17 May 2025 00:00:00 +0000</pubDate><guid>https://aleingmar-pi-portfolio.duckdns.org/p/automatic-tests-python/</guid><description>&lt;img src="https://aleingmar-pi-portfolio.duckdns.org/p/automatic-tests-python/pytest.png" alt="Featured image of post Test Automation with Python" /&gt;&lt;p&gt;This project was developed for the CI/CD course as part of the official university master&amp;rsquo;s degree in Development and Operations (DevOps).
The main objective of this project is to carry out the development of unit and API test automation of a given Calculator program.
&lt;strong&gt;GitHub repository:&lt;/strong&gt;
&lt;a class="link" href="https://github.com/aleingmar/automatic-tests-python" target="_blank" rel="noopener"
&gt;https://github.com/aleingmar/automatic-tests-python&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="project-report"&gt;Project report:
&lt;/h2&gt;&lt;p&gt;Project documentation: &lt;a class="link" href="../../post/automatic-test-python/Act2_AutomatizacionPruebasPython_AlejandroIngles.pdf" &gt;&lt;strong&gt;View documentation in pdf&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Deployment of multi-container service</title><link>https://aleingmar-pi-portfolio.duckdns.org/p/multi-layer-dockercompose/</link><pubDate>Sat, 10 May 2025 00:00:00 +0000</pubDate><guid>https://aleingmar-pi-portfolio.duckdns.org/p/multi-layer-dockercompose/</guid><description>&lt;img src="https://aleingmar-pi-portfolio.duckdns.org/p/multi-layer-dockercompose/dockerCompose.png" alt="Featured image of post Deployment of multi-container service" /&gt;&lt;p&gt;This project was developed for the Containers subject as part of the official university master&amp;rsquo;s degree in Development and Operations (DevOps).
The main objective of this project is to deploy a multi-tier application built on top of several containers orchestrated and deployed using Docker compose technology.
The deployment is based on and is a continuation of the project &amp;ldquo;Dockerisation of multi-tier application&amp;rdquo; already uploaded to this portfolio.
&lt;strong&gt;GitHub repository:&lt;/strong&gt;
&lt;a class="link" href="https://github.com/aleingmar/multi-layer-app-dockercompose" target="_blank" rel="noopener"
&gt;https://github.com/aleingmar/multi-layer-app-dockercompose&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="project-report"&gt;Project report:
&lt;/h2&gt;&lt;p&gt;Project documentation: &lt;a class="link" href="../../post/multicapa-dockerCompose/Act2_DockerCompose_AlejandroIngles.pdf" &gt;&lt;strong&gt;View documentation in pdf&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Dockerization of a Multi-layer application</title><link>https://aleingmar-pi-portfolio.duckdns.org/p/multilayer-dockerisation/</link><pubDate>Sat, 19 Apr 2025 00:00:00 +0000</pubDate><guid>https://aleingmar-pi-portfolio.duckdns.org/p/multilayer-dockerisation/</guid><description>&lt;img src="https://aleingmar-pi-portfolio.duckdns.org/p/multilayer-dockerisation/dockerizacion.png" alt="Featured image of post Dockerization of a Multi-layer application" /&gt;&lt;p&gt;This project was developed for the Containers subject as part of the official university master&amp;rsquo;s degree in Development and Operations (DevOps).
The main objective of this project is to dockerise a simple multilayer application, designed with a pedagogical perspective to illustrate the layered architecture and its deployment using containers. This application has been designed with three different layers: web presentation, business logic and data persistence.
Although a MEAN (Mongo - Express - Angular - Node) stack was initially considered, the presentation layer based on Angular and Nginx has not been fully implemented. Therefore, the final structure of the project is configured as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;First layer, presentation layer: Nginx + Website&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Second layer, business logic layer: App.js application developed using Express on Node.js.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Persistence layer: Implemented using MongoDB.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The functionality of the application is deliberately simple, with the aim of focusing on structure and deployment. It is a kind of ‘Hello World’, where the client initially connects to the presentation layer (Nginx), which delivers an index.html file.
This file contains a call that triggers a second HTTP request to the same Nginx server, but which is processed differently depending on the PATH of the request.&lt;/p&gt;
&lt;p&gt;&lt;img src="../../p/multilayer-dockerisation/image-1.png"
width="350"
height="111"
srcset="../../p/multilayer-dockerisation/image-1_hu_4ffd855a9baee4c9.png 480w, ../../p/multilayer-dockerisation/image-1_hu_227b83c58f0eea85.png 1024w"
loading="lazy"
alt="Index.html"
class="gallery-image"
data-flex-grow="315"
data-flex-basis="756px"
&gt;
&lt;img src="../../p/multilayer-dockerisation/image-2.png"
width="460"
height="332"
srcset="../../p/multilayer-dockerisation/image-2_hu_9ca88804c53b1ac.png 480w, ../../p/multilayer-dockerisation/image-2_hu_2fdb018dc9a184a2.png 1024w"
loading="lazy"
alt="Nginx.conf"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="332px"
&gt;&lt;/p&gt;
&lt;p&gt;This request is redirected to the backend layer, where the Express application is deployed.&lt;/p&gt;
&lt;p&gt;&lt;img src="../../p/multilayer-dockerisation/image-3.png"
width="515"
height="93"
srcset="../../p/multilayer-dockerisation/image-3_hu_b707ccd8a6c852a9.png 480w, ../../p/multilayer-dockerisation/image-3_hu_fcc7ae7177eba499.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="553"
data-flex-basis="1329px"
&gt;&lt;/p&gt;
&lt;p&gt;From there, the application tries to establish a connection to the MongoDB database.&lt;/p&gt;
&lt;p&gt;&lt;img src="../../p/multilayer-dockerisation/image-4.png"
width="764"
height="77"
srcset="../../p/multilayer-dockerisation/image-4_hu_7c505ba2836f9d85.png 480w, ../../p/multilayer-dockerisation/image-4_hu_8f38b01c35244090.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="992"
data-flex-basis="2381px"
&gt;&lt;/p&gt;
&lt;p&gt;Depending on the result of that connection, the backend responds to the client with a message in JSON format, indicating if the connection was successful (‘Hello world, connection successfully established’) or if an error occurred.&lt;/p&gt;
&lt;p&gt;&lt;img src="../../p/multilayer-dockerisation/image.png"
width="1064"
height="730"
srcset="../../p/multilayer-dockerisation/image_hu_ce7019eb9ec292a9.png 480w, ../../p/multilayer-dockerisation/image_hu_d82b9328e72322bb.png 1024w"
loading="lazy"
alt="Architecture"
class="gallery-image"
data-flex-grow="145"
data-flex-basis="349px"
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GitHub repository:&lt;/strong&gt;
&lt;a class="link" href="https://github.com/aleingmar/multi-layer-app-dockerisation" target="_blank" rel="noopener"
&gt;https://github.com/aleingmar/multi-layer-app-dockerisation&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="experimentation-video-and-project-memory"&gt;Experimentation video and project memory:
&lt;/h2&gt;&lt;p&gt;Documentation of the project: &lt;a class="link" href="../../post/multicapa-dockerizacion/Act1_Dockerizacion_AlejandroIngles.pdf" &gt;&lt;strong&gt;View documentation in pdf&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class="video-wrapper"&gt;
&lt;iframe loading="lazy"
src="https://www.youtube.com/embed/vvb0ahpH7mE"
allowfullscreen
title="YouTube Video"
&gt;
&lt;/iframe&gt;
&lt;/div&gt;</description></item><item><title>Automated deployment of Wordpress environment with Ansible</title><link>https://aleingmar-pi-portfolio.duckdns.org/p/wp-ansible/</link><pubDate>Fri, 24 Jan 2025 00:00:00 +0000</pubDate><guid>https://aleingmar-pi-portfolio.duckdns.org/p/wp-ansible/</guid><description>&lt;img src="https://aleingmar-pi-portfolio.duckdns.org/p/wp-ansible/wordpress-ansible.png" alt="Featured image of post Automated deployment of Wordpress environment with Ansible" /&gt;&lt;p&gt;This project was developed for the Deployment Automation Tools course as part of the official university master&amp;rsquo;s degree in Development and Operations (DevOps).&lt;/p&gt;
&lt;p&gt;The main objective of the project was to &lt;strong&gt;automate the local deployment of a complete WordPress environment&lt;/strong&gt; using &lt;strong&gt;Ansible and Vagrant&lt;/strong&gt;. An optimised secure architecture was implemented using &lt;strong&gt;Nginx as a reverse proxy&lt;/strong&gt; that blocks traffic destined for certain sensitive Wordpress administration paths.&lt;/p&gt;
&lt;p&gt;Vagrant creates and raises the virtual machine, on which Ansible is installed. Ansible then automatically self-provisions and configures all the necessary services, including Apache, MySQL, WordPress and Nginx, leaving the system completely ready for use.&lt;/p&gt;
&lt;h2 id="general-structure-of-the-anisble-provisioning-project"&gt;General structure of the Anisble provisioning project
&lt;/h2&gt;&lt;p&gt;The following is the organisation of the Ansible files and roles, to make it easier to understand the general operation of the project:&lt;/p&gt;
&lt;h3 id="main-playbook-provisionplaybookyml"&gt;Main playbook: provision/playbook.yml.
&lt;/h3&gt;&lt;p&gt;This file acts as the starting point for Ansible. From here, the roles needed to configure all the components of the environment are included.
In this case, the code is divided into four roles: apache, mysql, wordpress and nginx, which are executed in this order.
The installation of PHP and its modules has been decided to be included directly in this playbook, instead of creating a separate role, as it is only a few lines of code.
The order of provisioning is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;PHP modules&lt;/li&gt;
&lt;li&gt;Apache&lt;/li&gt;
&lt;li&gt;MySQL&lt;/li&gt;
&lt;li&gt;WordPress&lt;/li&gt;
&lt;li&gt;Nginx&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="variable-management-with-ansible"&gt;Variable management with Ansible
&lt;/h3&gt;&lt;p&gt;Instead of using Hiera as with &lt;strong&gt;Puppet&lt;/strong&gt;, Ansible uses &lt;strong&gt;YAML files&lt;/strong&gt; inside the &lt;code&gt;group_vars/all.yml&lt;/code&gt; directory, allowing variables to be separated from the main code.
This ensures a more secure approach, avoiding exposing sensitive credentials when uploading the project to a repository. Although this project is academic and does not include encrypted variables, Ansible Vault allows you to encrypt variables if necessary.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Variables are declared in: &lt;code&gt;group_vars/all.yml&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Jinja2 (.j2) templates are used to inject dynamic values into the configuration files.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="roles-in-ansible"&gt;Roles in Ansible
&lt;/h3&gt;&lt;p&gt;To better organise the manifests and auxiliary files that Ansible needs for infrastructure configuration automation, I split the content into &lt;strong&gt;four main roles in Ansible&lt;/strong&gt;, each responsible for a part of the system. This allows for &lt;strong&gt;modularity, code reuse and better organisation&lt;/strong&gt; of the playbook.&lt;/p&gt;
&lt;h4 id="apache-role"&gt;Apache Role
&lt;/h4&gt;&lt;p&gt;With this role, Ansible installs and configures the Apache web server, which acts as a backend to serve WordPress. Apache is only accessible from the virtual machine itself, as Nginx will act as a reverse proxy.&lt;/p&gt;
&lt;p&gt;The main tasks it performs are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install Apache and make sure the service is active.&lt;/li&gt;
&lt;li&gt;Remove the default Apache page.&lt;/li&gt;
&lt;li&gt;Configure Apache to listen on &lt;strong&gt;127.0.0.1:8080&lt;/strong&gt;.
&lt;ul&gt;
&lt;li&gt;Setting the listening port to &lt;strong&gt;127.0.0.1:8080&lt;/strong&gt; means that &lt;strong&gt;Apache will only accept connections from local processes on the same machine&lt;/strong&gt; where it is running. &lt;strong&gt;The address 127.0.0.1 is the loopback (localhost) address&lt;/strong&gt;, which prevents access from other machines on the network. This is useful when Apache is behind a reverse proxy, such as Nginx, which handles external connections and forwards requests to Apache on port 8080.’&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Copy the custom configuration from a Jinja2 template (&lt;code&gt;wp-apache-config.conf.j2&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Enable the new site and restart Apache automatically.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this configuration, Apache is kept isolated from shortcuts, ensuring that it can only be queried through Nginx.&lt;/p&gt;
&lt;h4 id="mysql-role"&gt;MySQL Role
&lt;/h4&gt;&lt;p&gt;In this role Ansible provisions the virtual machine with a MySQL database to ensure proper storage and access to WordPress data.&lt;/p&gt;
&lt;p&gt;The main tasks it performs are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install the MySQL server.&lt;/li&gt;
&lt;li&gt;Create the necessary database for WordPress.&lt;/li&gt;
&lt;li&gt;Configure the user and assign the appropriate permissions.&lt;/li&gt;
&lt;li&gt;Execute an initialisation script (&lt;code&gt;init-wordpress.sql.j2&lt;/code&gt;) to prepare the database with the initial structure and data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This role ensures that the database is ready and properly configured before WordPress attempts to connect later by running its role.&lt;/p&gt;
&lt;h4 id="wordpress-role"&gt;WordPress Role
&lt;/h4&gt;&lt;p&gt;This role automates the installation and configuration of WordPress, ensuring a functional and ready-to-use deployment.&lt;/p&gt;
&lt;p&gt;Key tasks it performs include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Downloading and extracting WordPress into /var/www/html/wordpress.&lt;/li&gt;
&lt;li&gt;Create and configure the wp-config.php file using a template (&lt;code&gt;wp-config.php.j2&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Ensure correct permissions for WordPress (&lt;code&gt;chown -R www-data:www-data&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Install wp-cli and use it to configure WordPress automatically.&lt;/li&gt;
&lt;li&gt;Initialise the database with minimal content using &lt;code&gt;init-wordpress-content.sql.j2&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Configure Apache to serve WordPress content.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this role, WordPress is installed, automatically configured and ready for use, without any manual intervention.&lt;/p&gt;
&lt;h4 id="nginx-role"&gt;Nginx Role
&lt;/h4&gt;&lt;p&gt;This role implements &lt;strong&gt;Nginx as a reverse proxy&lt;/strong&gt;, forming the first layer of defence of the system. Its main function is to handle incoming requests and block unwanted access.&lt;/p&gt;
&lt;p&gt;The main actions performed are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install Nginx in the virtual machine.&lt;/li&gt;
&lt;li&gt;Configure Nginx as a reverse proxy, redirecting requests to Apache on port 8080.&lt;/li&gt;
&lt;li&gt;Block access to sensitive paths such as &lt;code&gt;/wp-admin&lt;/code&gt; and &lt;code&gt;/wp-login.php&lt;/code&gt; to increase security.&lt;/li&gt;
&lt;li&gt;Optimise delivery of static files (CSS, JS, images) directly from Nginx, improving performance.&lt;/li&gt;
&lt;li&gt;Disable the default Nginx page and enable a WordPress-specific configuration.&lt;/li&gt;
&lt;li&gt;Restart Nginx automatically after applying the configuration.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Why is Nginx important in this project?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Protects Apache by acting as a single external access point, preventing direct attacks.&lt;/li&gt;
&lt;li&gt;Improves security by blocking access to critical management paths.
With this configuration, Nginx filters traffic and only allows secure requests to WordPress, strengthening the system infrastructure.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="../../p/wp-ansible/roles.png"
width="382"
height="700"
srcset="../../p/wp-ansible/roles_hu_9c7251995700954d.png 480w, ../../p/wp-ansible/roles_hu_d36af136e21df8f4.png 1024w"
loading="lazy"
alt="Directory structure"
class="gallery-image"
data-flex-grow="54"
data-flex-basis="130px"
&gt;&lt;/p&gt;
&lt;h2 id="system-architecture"&gt;System architecture
&lt;/h2&gt;&lt;h3 id="request-processing-and-data-flow"&gt;&lt;strong&gt;Request processing and data flow&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;When a user accesses WordPress, the request follows the following flow:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;The user accesses WordPress from a browser&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nginx receives the request on port 80&lt;/strong&gt; and decides whether to block the request or forward it to Apache.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;If the request is valid&lt;/strong&gt;, Nginx forwards it to &lt;strong&gt;Apache on &lt;code&gt;127.0.0.1:8080&lt;/code&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Apache processes the request&lt;/strong&gt;, executing the WordPress PHP scripts.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;If the page requires database data,&lt;/strong&gt; Apache queries MySQL.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Apache returns the generated response to Nginx&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nginx sends the response to the user&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This ensures that &lt;strong&gt;Apache is only accessible from the machine itself&lt;/strong&gt;, while Nginx acts as the first line of defence.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Communication between Nginx and Apache&lt;/strong&gt;.
To better understand how the two servers connect, it is important to know how their &lt;strong&gt;ports and IPs&lt;/strong&gt; work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Nginx listens on &lt;code&gt;0.0.0.0.0:80&lt;/code&gt;&lt;/strong&gt;, which means it accepts connections to port 80 and to any IP that identifies the machine running it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Apache listens on &lt;code&gt;127.0.0.1:8080&lt;/code&gt;&lt;/strong&gt;, which means that this process can only communicate with other processes from the same machine that send traffic to that ip and port 8080.
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;127.0.0.1 is the loopback address&lt;/strong&gt;, used for internal communication within the same machine.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;External traffic never reaches Apache directly&lt;/strong&gt;, as Nginx acts as an intermediary.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key benefit:&lt;/strong&gt; If someone tries to access Apache directly from another machine, the connection will be rejected because &lt;strong&gt;Apache is not exposed to the network&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="conclusion"&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;All in all, simply by going to the console in the directory where the &lt;strong&gt;Vagrantfile&lt;/strong&gt; is located and running a simple &lt;code&gt;vagrant up&lt;/code&gt;, a functional, customised and secure WordPress environment is automatically deployed, accessible from a web client at &lt;code&gt;http://192.168.55.10&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GitHub repository:&lt;/strong&gt;
&lt;a class="link" href="https://github.com/aleingmar/wordpress_ansible" target="_blank" rel="noopener"
&gt;https://github.com/aleingmar/wordpress_ansible&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="experimentation-video-and-project-report"&gt;Experimentation video and project report:
&lt;/h2&gt;&lt;p&gt;Project documentation: &lt;a class="link" href="../../post/wordpress-ansible/Act3_Wordpress_Ansible_AlejandroIngles.pdf" &gt;&lt;strong&gt;View documentation in pdf&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div class="video-wrapper"&gt;
&lt;iframe loading="lazy"
src="https://www.youtube.com/embed/lksomzUvzA0"
allowfullscreen
title="YouTube Video"
&gt;
&lt;/iframe&gt;
&lt;/div&gt;</description></item><item><title>Automatic deployment of multilayer MEAN service on AWS</title><link>https://aleingmar-pi-portfolio.duckdns.org/p/stack-mean-terraform/</link><pubDate>Tue, 14 Jan 2025 00:00:00 +0000</pubDate><guid>https://aleingmar-pi-portfolio.duckdns.org/p/stack-mean-terraform/</guid><description>&lt;img src="https://aleingmar-pi-portfolio.duckdns.org/p/stack-mean-terraform/stack.png" alt="Featured image of post Automatic deployment of multilayer MEAN service on AWS" /&gt;&lt;p&gt;This project was developed for the DevOps Tools course, as part of the oficial university&amp;rsquo;s master&amp;rsquo;s degree in Development and Operations (DevOps).&lt;/p&gt;
&lt;p&gt;The aim of the project was to automatically deploy a fully functional multi-tier MEAN system in the AWS cloud. This system consists of a load balancer, several instances for the web application and a dedicated instance for the MongoDB database. I use Terraform, Packer and Ansible for infrastructure automation and provisioning.&lt;/p&gt;
&lt;p&gt;On a personal level, I consider it important to highlight that the documentation report of this project is particularly complete, as it includes all the details of the development process. Among them, I had to deal with three main problems during this process. Without going into too much detail, these were:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Deployment on Azure, version incompatibility and choice of AWS.&lt;/li&gt;
&lt;li&gt;Execution of an interactive command blocking the automatic provisioning process.&lt;/li&gt;
&lt;li&gt;Static resource inconsistency and balancer logoff.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In my opinion, these problems are very interesting to analyse, as they are common situations in this type of work. Although they may seem minor, they have been fundamental in the development of the project.&lt;/p&gt;
&lt;p&gt;It is also worth mentioning that for this project I have used similar technologies to those of the project &lt;strong&gt;‘Creation and automated deployment of image in multicloud environment’&lt;/strong&gt;, which is also available in my portfolio. For this reason, in this publication I have decided to highlight three aspects that differentiate both works:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The use of the MEAN stack.&lt;/li&gt;
&lt;li&gt;The modularisation of Terraform.&lt;/li&gt;
&lt;li&gt;The deployment process and architecture, although the latter is presented in a summarised form, as it is explained extensively and in detail in the report.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="technology-stack"&gt;Technology Stack
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Terraform&lt;/strong&gt;: with terraform I centralise the whole deployment process, raise and manage the infrastructure elements that make up the system. Some of these elements are for example the networks that connect the different instances, the instances themselves, the load balancer&amp;hellip; In short, the infrastructure that supports the service.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Packer&lt;/strong&gt;: with packer I create the image that serves as a base for the instances that I build with Terraform. In this project, Packer generates a custom image for the first layer of the system, provisioning it with the necessary services such as Node.js, Nginx, Angular&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ansible&lt;/strong&gt;: with Ansible I do the provisioning of the instance that is raised and used by Packer for the creation of the image. In this project, Ansible automatically provisions the instance with Angular, Express, MongoDB, Nginx, Node&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MEAN Stack&lt;/strong&gt;: The system being built is a service made up of the MEAN technology stack, widely used in the industry for its versatility and performance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MongoDB&lt;/strong&gt;: Non-relational document-oriented database, ideal for handling large volumes of structured and unstructured data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Express&lt;/strong&gt;: Backend framework for Node.js that facilitates the development of robust and scalable web applications.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Angular&lt;/strong&gt;: Frontend framework that allows the development of modern and reactive interfaces, improving the user experience.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Node.js&lt;/strong&gt;: Execution environment for Js on the server side.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="modularisation-of-the-terraform-template"&gt;Modularisation of the Terraform template
&lt;/h3&gt;&lt;h4 id="importance-of-modularisation"&gt;Importance of modularisation
&lt;/h4&gt;&lt;p&gt;Modularisation in Terraform is vital in projects that use Terraform. It basically consists of dividing the &lt;strong&gt;main.tf&lt;/strong&gt; into different ‘modules’ according to certain categories. Not only does this improve code readability and maintainability, but it also allows responsibilities to be divided and configurations to be reused between projects. Although managing variables between modules can be complex, this practice is essential in large, dynamic infrastructures.&lt;/p&gt;
&lt;h4 id="project-modules"&gt;Project modules
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Security module&lt;/strong&gt;: This module manages the security groups that define the traffic rules to and from the instances, enabling traffic from protocols such as SSH, HTTP&amp;hellip;. It also configures the SSH keys needed to access the instances remotely.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network module&lt;/strong&gt;: Defines the networks and private subnets necessary for system connectivity and also configures routing tables and gateways to guarantee access between infrastructure layers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Instance module&lt;/strong&gt;: Deploys the first and second layer instances, assigning public and private IP addresses and also provisions these instances for proper operation in the system.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Load balancer module&lt;/strong&gt;: Configures and defines everything related to the load balancer that distributes the traffic between the instances of the first layer. In addition to the load balancer itself, for this to work it needs more elements such as destination groups, distribution strategies such as round-robin, definition of session persistence&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Image module&lt;/strong&gt;: This module integrates Terraform with Packer for the creation of base images. Terraform executes &lt;strong&gt;packer build&lt;/strong&gt;, retrieves the generated image and uses it to instantiate resources of the first layer.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="../../p/stack-mean-terraform/modulos.png"
width="475"
height="614"
srcset="../../p/stack-mean-terraform/modulos_hu_8de2a353a31560ed.png 480w, ../../p/stack-mean-terraform/modulos_hu_9c56eb89ab0adb31.png 1024w"
loading="lazy"
alt="Directory structure"
class="gallery-image"
data-flex-grow="77"
data-flex-basis="185px"
&gt;&lt;/p&gt;
&lt;h3 id="deployment-process-and-architecture"&gt;Deployment process and architecture
&lt;/h3&gt;&lt;p&gt;The deployment starts with the integration of Terraform and Packer. Terraform invokes Packer, which is responsible for raising a temporary instance in AWS to generate a base image. During this process, this instance is provisioned with Ansible, which installs and configures services such as Angular, Express and MongoDB, as well as copying essential files from the local environment. Once provisioning is complete, Packer creates the base image and destroys the temporary instance, leaving an image ready for reuse.&lt;/p&gt;
&lt;p&gt;With the image generated, Terraform proceeds to deploy the complete infrastructure. First, the networks and subnets are configured, ensuring internal connectivity between the layers of the system. Next, the first layer instances are deployed using the base image. These instances host the frontend and backend of the application, with Node.js and Nginx serving as the operational core.&lt;/p&gt;
&lt;p&gt;At the same time, Terraform builds the second tier instance, dedicated to data persistence with MongoDB. This instance is connected via a private network to the first layer instances, guaranteeing secure and stable communication. In addition to this, terraform also raises a load balancer, configured to distribute the traffic between the first layer instances, which ensures high availability and scalability.&lt;/p&gt;
&lt;p&gt;The last step is the final provisioning. Terraform uses Bash scripting to finish configuring the deployed instances. For example, in the first layer instances, Angular configurations are adjusted to include the IP addresses of the backend, which allows static files to be generated and served by Nginx with the necessary routes to connect the client to the backend.&lt;/p&gt;
&lt;p&gt;This entire process ensures a fully automated deployment, resulting in a functional, production-ready system, with components integrated and configured for optimal performance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GitHub repository:&lt;/strong&gt;
&lt;a class="link" href="https://github.com/aleingmar/Multi-layer_MEAN_Deployment" target="_blank" rel="noopener"
&gt;https://github.com/aleingmar/Multi-layer_MEAN_Deployment&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="video-of-the-experimentation-and-project-report"&gt;Video of the experimentation and project report:
&lt;/h3&gt;&lt;p&gt;Project documentation: &lt;a class="link" href="../../post/stack-MEAN-Terraform/Act2_StackMEAN_Terraform_AlejandroIngles.pdf" &gt;&lt;strong&gt;View documentation in pdf&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class="video-wrapper"&gt;
&lt;iframe loading="lazy"
src="https://www.youtube.com/embed/zRGhkBebEbA"
allowfullscreen
title="YouTube Video"
&gt;
&lt;/iframe&gt;
&lt;/div&gt;</description></item><item><title>Creation and automated deployment of image in multicloud environment.</title><link>https://aleingmar-pi-portfolio.duckdns.org/p/imagen-multicloud-packer/</link><pubDate>Sat, 14 Dec 2024 00:00:00 +0000</pubDate><guid>https://aleingmar-pi-portfolio.duckdns.org/p/imagen-multicloud-packer/</guid><description>&lt;img src="https://aleingmar-pi-portfolio.duckdns.org/p/imagen-multicloud-packer/imagen-multicloud-packer2.png" alt="Featured image of post Creation and automated deployment of image in multicloud environment." /&gt;&lt;p&gt;This project was developed for the DevOps Tools course, as part of the official university master&amp;rsquo;s degree in Development and Operations (DevOps).&lt;/p&gt;
&lt;p&gt;The aim of the project is to &lt;strong&gt;create and automatically deploy an image of a complete web system in a multicloud environment of Azure and AWS&lt;/strong&gt;. This web system is composed of a small application written with Nodejs and a Nginx web server. To achieve this, I use Terraform, Ansible and Packer technologies mainly.&lt;/p&gt;
&lt;h3 id="technologies-used"&gt;Technologies used:
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Terraform:&lt;/strong&gt; With Terraform I centralize all the execution of the process and deploy the necessary infrastructure to raise an instance in the cloud created from the image of the system and accessible through the internet.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Packer:&lt;/strong&gt; With Packer I build the complete system image. Packer uses the cloud as a provider for the creation of the image. It builds an instance and all the necessary infrastructure for the creation of the image and destroys it when it is finished.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ansible:&lt;/strong&gt; with ansible the provisioning of the instance that packer raises and from which the image is created is carried out. In the case of Azure I do this provisioning with Ansible, in the case of AWS I do the same but directly with Bash scripting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To control multicloud deployment, a parameter has been implemented that must be passed to the &lt;code&gt;terraform apply ‘deployment_target=’&lt;/code&gt;, indicating whether you want to deploy in both clouds simultaneously or in a single cloud. If this is the case, you must indicate in which one you want to deploy.&lt;/p&gt;
&lt;h3 id="creation-and-deployment-process"&gt;Creation and deployment process:
&lt;/h3&gt;&lt;p&gt;The sequence of steps in the process would be as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialise by manually executing a &lt;code&gt;terraform init &amp;amp;&amp;amp; terraform apply&lt;/code&gt; in the shell.&lt;/li&gt;
&lt;li&gt;After that, terraform executes the &lt;code&gt;packer build&lt;/code&gt; command, which takes care of setting up all the necessary infrastructure and the machine used for the creation of the image. In the case of Azure, an Ansible is installed on this machine and it auto-provisions itself by running a playbook and a series of tasks defined in it. In the case of AWS, the same steps are executed, but instead of an Ansible directly by manual scripting in Bash. The provisioning is based among other things on the installation and management of the services: Nodejs, Nginx, pm2 and App.js on the instance that creates the image.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Nodejs:&lt;/strong&gt; Provides an environment with everything necessary for the application to run and function correctly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nginx:&lt;/strong&gt; Web server that will be in charge of redirecting all the traffic to the application and forwarding its responses. It is very important to configure it so that when the image is deployed the server is active and correctly configured to serve the app. Passes traffic from port 80 to port 3000 (where the app.js listens).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PM2:&lt;/strong&gt; Nodejs process manager which is used to ensure that the app.js is active when the image is deployed without having to do anything else (this step is particularly tricky).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;app.js&lt;/strong&gt;: core, functional application of the image, it is important to transfer the source code of the app so that it is accessible by the instance that creates the image.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;After this, Packer creates the image and destroys all the infrastructure it has needed to build on the corresponding cloud provider.&lt;/li&gt;
&lt;li&gt;Terraform, after waiting for the image creation to finish successfully, builds all the necessary infrastructure (key pair, security group, disk&amp;hellip;) to build an instance from this image.&lt;/li&gt;
&lt;li&gt;Once the deployment is finished, this instance is accessible through the internet via the public ip.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In short, just by executing a &lt;code&gt;terraform init &amp;amp;&amp;amp; terraform apply&lt;/code&gt; you deploy a functional web environment accessible from the internet in the public cloud of Azure and AWS. And you also create a reusable image so you can deploy more instances identical to these in the future in a much faster and safer way against possible human errors.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GitHub repository:&lt;/strong&gt; &lt;a class="link" href="https://github.com/aleingmar/CreateImages_Nginx-Nodejs_Packer" target="_blank" rel="noopener"
&gt;https://github.com/aleingmar/CreateImages_Nginx-Nodejs_Packer&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="repository-contents-and-project-files"&gt;Repository contents and project files:
&lt;/h3&gt;&lt;p&gt;The GiHub repository consists of two main directories with two different versions: &lt;code&gt;/version-2&lt;/code&gt; and &lt;code&gt;/version-3.1&lt;/code&gt;.
The fully functional directory containing the latest version of the project is the second one (&lt;code&gt;/version-3.1&lt;/code&gt;). This is the directory where you have to be located to deploy the &lt;code&gt;terraform init &amp;amp;&amp;amp; terraform apply&lt;/code&gt; (&lt;code&gt;cd version-3.1/te*&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Briefly explaining the contents of the directory:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/packer/&lt;/code&gt;: directory where all the content necessary for Packer to run and build the image is located.
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/packer/main.pkr.hcl&lt;/code&gt;: Packer&amp;rsquo;s main file where all the resources needed to build the image are defined as well as all the variables to be used.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/packer/variables.pkrvars.hcl&lt;/code&gt;: file where I assign values to all the variables defined in the &lt;code&gt;main.pkr.hcl&lt;/code&gt; except for the credentials of the two clouds that for security reasons, I define and assign values as environment variables of my host operating system that I use to launch the terraform. I pass these values as parameters in the &lt;code&gt;terraform apply&lt;/code&gt; and &lt;code&gt;packer build&lt;/code&gt; command.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/packer/providers/&lt;/code&gt;: directory where we can find the auxiliary files used to create the image, such as the apache configuration file (&lt;code&gt;nginx_default.conf&lt;/code&gt;), the playbook that defines the provisioning with ansible (&lt;code&gt;provision.yml&lt;/code&gt;) and the nodejs application code (&lt;code&gt;app.js&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/terraform/&lt;/code&gt;: directory where all the content necessary for terraform to run and deploy all the necessary infrastructure for the project is located.
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/terraform/main.tf&lt;/code&gt;: main file of terraform, where all the process flow that the deployment must follow and all the infrastructure to be deployed is defined.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/terraform/variables.tf&lt;/code&gt;: file where all the variables used by terraform are defined.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/terraform/terraform.tfvars&lt;/code&gt;: file where all the variables are given values except for the credentials of the two clouds that, for security reasons, I define and assign values in environment variables of my host operating system from where I launch terraform. I pass these values as parameters in the &lt;code&gt;terraform apply&lt;/code&gt; and &lt;code&gt;packer build&lt;/code&gt; command.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="../../p/imagen-multicloud-packer/ficheros.png"
width="400"
height="469"
srcset="../../p/imagen-multicloud-packer/ficheros_hu_8ecbe9f6c2ea3ce2.png 480w, ../../p/imagen-multicloud-packer/ficheros_hu_d1084500bdd4b64e.png 1024w"
loading="lazy"
alt="Directory content"
class="gallery-image"
data-flex-grow="85"
data-flex-basis="204px"
&gt;&lt;/p&gt;
&lt;h3 id="contents-of-packer-main"&gt;Contents of Packer main:
&lt;/h3&gt;&lt;p&gt;The content of this file can be differentiated in several parts in which the following components necessary for the creation of the image are defined:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PLUGINS&lt;/strong&gt;: Defines the plugins needed for the template.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Definition of variables&lt;/strong&gt;: (no value is assigned here, only maybe the default value).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;BUILDER&lt;/strong&gt;: Define how the AMI is built in AWS &amp;ndash;&amp;gt; &lt;code&gt;source{}&lt;/code&gt;&amp;ndash;&amp;gt; define the base system on which I want to create the image (ubuntu ISO) and the provider for which we create the image (technology with which the image will be deployed) &amp;ndash;&amp;gt; AMAZON. AZURE&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PROVISIONERS&lt;/strong&gt;: Configure the operating system and the application, how the software will be installed and configured &amp;ndash;&amp;gt; &lt;code&gt;build{}&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="experimentation-video-and-report-of-the-project"&gt;Experimentation video and report of the project:
&lt;/h3&gt;&lt;p&gt;Project documentation: &lt;a class="link" href="../../post/imagen-multicloud-packer/Act1_Packer_AlejandroIngles.pdf" &gt;&lt;strong&gt;View the pdf&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class="video-wrapper"&gt;
&lt;iframe loading="lazy"
src="https://www.youtube.com/embed/BhRB0716G5w"
allowfullscreen
title="YouTube Video"
&gt;
&lt;/iframe&gt;
&lt;/div&gt;</description></item><item><title>Automated Wordpress deployment using Vagrant and Puppet</title><link>https://aleingmar-pi-portfolio.duckdns.org/p/wordpress-puppet/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://aleingmar-pi-portfolio.duckdns.org/p/wordpress-puppet/</guid><description>&lt;img src="https://aleingmar-pi-portfolio.duckdns.org/p/wordpress-puppet/wordpress-puppet.png" alt="Featured image of post Automated Wordpress deployment using Vagrant and Puppet" /&gt;&lt;p&gt;This project was developed for the Deployment Automation course, as part of the official university master&amp;rsquo;s degree in Development and Operations (DevOps).&lt;/p&gt;
&lt;p&gt;The main objective of this project is to automatically deploy a test web environment with a custom WordPress service, using Vagrant as the Infrastructure as Code (IaC) tool and Puppet for automated provisioning.
By simply running the &lt;code&gt;vagrant up&lt;/code&gt; command in the terminal in the directory where the Vagrantfile is located, the entire environment is deployed without any additional configuration.
Before you can deploy a WordPress web service, you need to perform several provisioning and configuration tasks, including the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install, configure and set up an Apache web server to redirect and serve all content.&lt;/li&gt;
&lt;li&gt;Install all the specific PHP packages and modules required by WordPress.&lt;/li&gt;
&lt;li&gt;Install, configure and set up a MySQL database that will be used by WordPress for the persistence of its data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To verify correct operation, simply access &lt;code&gt;localhost:8080&lt;/code&gt; from the browser.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GitHub repository:&lt;/strong&gt; &lt;a class="link" href="https://github.com/aleingmar/WordPress_deployment-puppet-vagrant" target="_blank" rel="noopener"
&gt;https://github.com/aleingmar/WordPress_deployment-puppet-vagrant&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The project includes two different versions of the environment, organised in separate directories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/puppet-two-nodes&lt;/code&gt;.
In this version, three Puppet nodes are deployed: one Puppet Master and two Puppet Clients.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each client (node) hosts a WordPress environment, provisioned with the directives sent from the Puppet Master. Every min the puppet clients automatically request the new puppet configuration if any via a cron job.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/puppet-one-node&lt;/code&gt;
In this version, only one virtual machine (VM) with a self-provisioning Puppet client is raised, without the need for a Puppet Master.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="self-provisioning-version-puppet-one-node"&gt;Self-provisioning version: puppet-one-node
&lt;/h2&gt;&lt;p&gt;In this version of the environment, only one mv is raised, a self-provisioning puppet agent/client without the need for a Puppet master node. The whole deployment process is done fully automatically.&lt;/p&gt;
&lt;h3 id="puppet-one-node-vagrantfile"&gt;puppet-one-node: Vagrantfile
&lt;/h3&gt;&lt;p&gt;The Vagrantfile defines the basic virtual machine (VM) configuration for creating an Infrastructure-as-Code (IaC) environment. It specifies the Ubuntu base box to be used, the networking options (including port forwarding and assigning a private IP), and allocates 1024 MB of RAM to the VM. In addition, Puppet is installed in agent mode, eliminating the need for a master Puppet server, and is configured to use the main manifest &lt;code&gt;default.pp&lt;/code&gt;, modules from the &lt;code&gt;modules&lt;/code&gt; directory and the Hiera configuration file &lt;code&gt;hiera.yaml&lt;/code&gt; to centrally manage data.&lt;/p&gt;
&lt;h2 id="client-server-architecture-version-puppet-two-nodes"&gt;Client-server architecture version: puppet-two-nodes
&lt;/h2&gt;&lt;p&gt;In this version of the environment 3 mvs are raised, one puppet master and two puppet clients. The mvs are automatically raised and their corresponding puppet versions are installed (to the master node the master is installed and so on).
Once the client node is up (which is up after the master), as soon as it starts it sends its certificate to the master (which knows it because it is in its puppet.config file).
In a &lt;strong&gt;MANUAL&lt;/strong&gt; way, you have to perform the different administration tasks:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;SERVER side&lt;/strong&gt;:
Manually, the only thing the sysadmin in charge of this environment has to do is to do an:
&lt;code&gt;sudo /opt/puppetlabs/bin/puppetserver ca sign --all&lt;/code&gt; to sign all the unsigned certificates that have arrived (in this case one for each client node) and send them signed to the corresponding clients.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;On a voluntary basis, but recommended for security purposes, especially in real production environments, you should run &lt;strong&gt;before&lt;/strong&gt; signing them a:
&lt;code&gt;sudo /opt/puppetlabs/bin/puppetserver ca list --all&lt;/code&gt; to list all the certificates and verify that you don&amp;rsquo;t sign a certificate that you shouldn&amp;rsquo;t.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;strong&gt;CLIENT side&lt;/strong&gt;:
Once this is done on the server, the corresponding client should receive its signed certificate, with which it will be able to communicate with the master node and
ask for puppet configuration/provisioning by executing this command: &lt;code&gt;sudo /opt/puppetlabs/bin/puppet agent --test&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="general-structure-of-the-puppet-provisioning-project"&gt;General structure of the puppet provisioning project
&lt;/h3&gt;&lt;p&gt;The organisation of the files and modules is detailed below, which makes it easier to understand the general functioning of the project:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Main file&lt;/strong&gt;: &lt;code&gt;manifests/default.pp&lt;/code&gt;.
This file acts as the starting point for Puppet. The modules needed to configure all the components of the environment are imported from here. In this case, the code is split into three modules: &lt;strong&gt;apache, mysql and wordpress&lt;/strong&gt;, which are executed and imported in this order. The PHP installation I have decided to code it directly in this module, without adding another module just for this as it is only 3 or 4 lines of code. The installation of these components is done in this order: &lt;strong&gt;apache, php, mysql and wordpress&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Variables management with Hiera**: &lt;code&gt;hiera.yaml, data/common.yaml&lt;/code&gt;.
To manage variables, Hiera is used, which allows the keys to be separated from the source code. This ensures a more secure approach, as it avoids exposing sensitive credentials when uploading the project to a cloud repository. Although this project is academic and does not include encrypted variables, Hiera also offers the possibility to encrypt keys.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Variables are declared along with their values in &lt;code&gt;data/common.yaml&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;hiera.yaml&lt;/code&gt; file configures how Hiera works.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To integrate Hiera with Vagrant, the line &lt;code&gt;puppet.hiera_config_path = ‘hiera.yaml’&lt;/code&gt; is added to the Vagrantfile.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Modules used&lt;/strong&gt;:
The project is divided into three main modules, which guarantees modularity and organisation in the code:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Apache module&lt;/strong&gt;: This module provisions and configures the Apache web server in the VM.
This module provisions and configures the Apache web server in the VM, leaving it ready and active so that the wordpress module can manage and serve content from it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;mysql module&lt;/strong&gt;
In this module a MySQL server is installed and configured in the VM, ensuring the correct functioning of the database manager. In addition, the necessary database for WordPress is created using the &lt;code&gt;init-wordpress.sql.erb&lt;/code&gt; file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;wordpress module&lt;/strong&gt;
This module installs and configures WordPress, making it fully functional. The main actions performed are:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Installation of the wordpress packages and dependencies and activation of the service.&lt;/li&gt;
&lt;li&gt;Configuration of the &lt;code&gt;wp-config.conf.erb&lt;/code&gt; file, which configures the service, among other things, connects WordPress with the database and defines previously generated access keys.&lt;/li&gt;
&lt;li&gt;Installation and use of the &lt;code&gt;wp-cli&lt;/code&gt; tool to automate the configuration of the website.&lt;/li&gt;
&lt;li&gt;Initialisation of the database using the &lt;code&gt;init-wordpress-content.sql.erb&lt;/code&gt; file with the minimum content necessary to launch a web page.&lt;/li&gt;
&lt;li&gt;Configuration of Apache to serve the page content, using the &lt;code&gt;wp-apache-config.conf.erb&lt;/code&gt; file.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The service is accessible from the host at &lt;code&gt;localhost:8080&lt;/code&gt; thanks to port 8080 redirection from the host to port 80 on the virtual machine, where Apache listens for incoming HTTP requests.&lt;/p&gt;
&lt;p&gt;Deployment of the puppet-one-node version enviroment:
&lt;div class="video-wrapper"&gt;
&lt;iframe loading="lazy"
src="https://www.youtube.com/embed/xQdjuHr-2-U"
allowfullscreen
title="YouTube Video"
&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;</description></item><item><title>Automated Apache deployment using Vagrant and Puppet</title><link>https://aleingmar-pi-portfolio.duckdns.org/p/apache-web-puppet/</link><pubDate>Fri, 11 Oct 2024 00:00:00 +0000</pubDate><guid>https://aleingmar-pi-portfolio.duckdns.org/p/apache-web-puppet/</guid><description>&lt;img src="https://aleingmar-pi-portfolio.duckdns.org/p/apache-web-puppet/apache-puppet.png" alt="Featured image of post Automated Apache deployment using Vagrant and Puppet" /&gt;&lt;p&gt;This project was developed for the Deployment Automation course, as part of the official university master&amp;rsquo;s degree in Development and Operations (DevOps).&lt;/p&gt;
&lt;p&gt;The aim of the project is to deploy and configure in an automated way a web environment on a virtual machine hosting an Apache server, which serves a basic web page. The virtual machine is created using IaC (Infrastructure as Code) with Vagrant, and Puppet is used for its provisioning, which manages the installation of Apache and the automatic loading of a simple HTML file, thus creating a functional web service.&lt;/p&gt;
&lt;p&gt;In short, simply running a &lt;code&gt;vagrant up&lt;/code&gt; starts the whole deployment and provisioning process and automatically (without doing anything else) a virtual machine is raised in which puppet is installed, an Apache web server is configured and installed to activate and listen to port 80 (http) of the VM and to return a simple web page that is inserted inside it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GitHub repository:&lt;/strong&gt; &lt;a class="link" href="https://github.com/aleingmar/deployment_apache-puppet-vagrant" target="_blank" rel="noopener"
&gt;https://github.com/aleingmar/deployment_apache-puppet-vagrant&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The GiHub repository consists of two directories with two different versions: /easy_mode and /hard_mode.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The first folder (/easy_mode) contains the deployment project with a simplified structure. This version does not follow the architecture and code organisation of complex deployment projects, and the Apache configuration is more basic.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The second folder (/hard_mode) uses a more Puppet-friendly code pattern, for example using modules and other Puppet-typical elements. In addition, the Apache configuration is more advanced and detailed.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both versions manage to deploy correctly.&lt;/p&gt;
&lt;p&gt;Explaining for example the simple version (/easy_mode) a directory where a ‘Vagrantfile’ file is located and a ‘manifests’ folder inside which the ‘apache.pp’ file is located.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The Vagrantfile defines the virtual machine infrastructure that needs to be deployed to support the web service. This is taken care of by Vagrant and underneath it, it uses VirtualBox as virtualisation provider.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the Vagrantfile, before telling Vagrant to provision the infrastructure using Puppet, a script is run to install Puppet inside the virtual machines. Puppet in this case works in stand-alone mode (without following the client-server model).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The file ‘apache.pp’ defines the desired configuration for this infrastructure and serves as a declarative guide for Puppet to develop its work. Since Puppet uses a declarative language, you don&amp;rsquo;t tell it how you want things to be done, only what you want to achieve, and Puppet does the rest.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The service is accessible from the host on &lt;code&gt;localhost:8080&lt;/code&gt; by redirecting port 8080 on the host to port 80 on the virtual machine, where Apache listens for incoming HTTP requests.&lt;/p&gt;
&lt;div class="video-wrapper"&gt;
&lt;iframe loading="lazy"
src="https://www.youtube.com/embed/8K7JzFuzGvg"
allowfullscreen
title="YouTube Video"
&gt;
&lt;/iframe&gt;
&lt;/div&gt;</description></item><item><title>Deployment and creation of a web portfolio with Hugo.</title><link>https://aleingmar-pi-portfolio.duckdns.org/p/hugo-portfolio/</link><pubDate>Sun, 01 Sep 2024 00:00:00 +0000</pubDate><guid>https://aleingmar-pi-portfolio.duckdns.org/p/hugo-portfolio/</guid><description>&lt;img src="https://aleingmar-pi-portfolio.duckdns.org/p/hugo-portfolio/hugo-portfolio.png" alt="Featured image of post Deployment and creation of a web portfolio with Hugo." /&gt;&lt;p&gt;This project aims to create a web portfolio to show all the projects I have developed during my academic and personal career, deploy it on my own RPI5 server and make it securely accessible from the internet.&lt;/p&gt;
&lt;h2 id="web-development"&gt;Web Development
&lt;/h2&gt;&lt;p&gt;For the construction of my portfolio I chose &lt;strong&gt;Hugo&lt;/strong&gt;, a static website generation platform that allows you to create modern frontend pages using &lt;strong&gt;Markdown&lt;/strong&gt; files. The decision to use Hugo was based on the fact that I had already written part of my portfolio in Obsidian, a tool (also written in Markdown) that I regularly use to take notes and organise my notes. Hugo&amp;rsquo;s ability to take advantage of Markdown format files allowed me to migrate this content easily and focus more on the quality of the content than on the technical development.&lt;/p&gt;
&lt;p&gt;The theme I selected for my portfolio is &lt;a class="link" href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener"
&gt;hugo-theme-stack&lt;/a&gt;, because of its clean and modern format, which fits perfectly with the structure and design I was looking for. This template, with its focus on performance and simplicity, allowed me to optimise the development of my portfolio without having to invest too much time in interface design. Even so, I have programmed new functionalities that differ from the open source base project for my personal use.&lt;/p&gt;
&lt;h2 id="devops-and-deployment"&gt;DevOps and Deployment
&lt;/h2&gt;&lt;p&gt;In terms of deployment and operational management, my portfolio is hosted on my Raspberry Pi 5 server with 8 GB of RAM, which provides an efficient and energy-efficient solution. To ensure an orderly and isolated environment, I use Docker, where the portfolio runs inside a container. This allows me to package the application independently from the rest of the system, facilitating management and avoiding conflicts with other services running on the same server.&lt;/p&gt;
&lt;p&gt;The web server that handles the requests is &lt;strong&gt;Caddy&lt;/strong&gt;, a lightweight solution that allows me to secure the connection with HTTPS automatically and redirect the traffic to the different services I have deployed. In addition to my own portfolio, I also host the portfolio of a colleague in another container. To monitor the web portfolio I use Google Analytics 4 (GA4) which allows me to see statistics about the accesses to the website.&lt;/p&gt;
&lt;p&gt;For development, I usually work locally using &lt;strong&gt;Visual Studio Code&lt;/strong&gt;, although sometimes I use &lt;strong&gt;GitHub Codespaces&lt;/strong&gt; when I prefer to work in a remote environment. The deployment process is simple: I connect to the server via SSH, pull the latest changes from GitHub and restart the Docker container running Hugo. This workflow is fully automated through a bash script and a Docker Compose file, which simplifies the process of lifting the web application with each update.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Web portfolio change deployment process with bash scripting:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="video-wrapper"&gt;
&lt;iframe loading="lazy"
src="https://www.youtube.com/embed/j6_zmGQ0YFM"
allowfullscreen
title="YouTube Video"
&gt;
&lt;/iframe&gt;
&lt;/div&gt;</description></item><item><title>My own RPI5 server</title><link>https://aleingmar-pi-portfolio.duckdns.org/p/my-server/</link><pubDate>Thu, 01 Aug 2024 00:00:00 +0000</pubDate><guid>https://aleingmar-pi-portfolio.duckdns.org/p/my-server/</guid><description>&lt;img src="https://aleingmar-pi-portfolio.duckdns.org/p/my-server/rasberry.png" alt="Featured image of post My own RPI5 server" /&gt;&lt;p&gt;This project consists of hosting and managing autonomously certain services on my own server, a RasberryPi 5 with 8gb of Ram.&lt;/p&gt;
&lt;p&gt;For the administration and configuration of both the server and its hosted services, I access remotely via SSH protocol.&lt;/p&gt;
&lt;p&gt;The server is associated to a main domain managed by &lt;strong&gt;DuckDNS&lt;/strong&gt;, which allows me to access the services remotely through the browser. To prevent the dynamic IP of my home network from changing and losing access to the server, I have a service that automatically updates this IP every 5 minutes, ensuring that it is always correctly synchronised.&lt;/p&gt;
&lt;p&gt;To organise access via subdomains and ensure connection to my services via &lt;strong&gt;HTTPS&lt;/strong&gt;, I use &lt;strong&gt;Caddy&lt;/strong&gt; as a web server, which acts as an intermediary and handles the TLS/SSL certificates, guaranteeing secure and uncomplicated access.&lt;/p&gt;
&lt;p&gt;In addition, I have implemented an advanced control panel called &lt;strong&gt;Homarr&lt;/strong&gt;, which provides me with a centralised interface from which I can easily log in and access the different services deployed on the server.&lt;/p&gt;
&lt;p&gt;All the services hosted on the server including the ones mentioned above are hosted using &lt;strong&gt;Docker containers&lt;/strong&gt; and are organized in specific subdomains.
At the time of writing, the unmentioned services hosted on the server are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Vaultwarden&lt;/strong&gt;: A password manager.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Portainer&lt;/strong&gt;: A container manager with a web interface.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pi-hole&lt;/strong&gt;: An ad-blocking DNS service.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WireGuard&lt;/strong&gt;: A VPN service.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;WireGuard is integrated with Pi-hole. This setup allows me not only to redirect my traffic through my server to secure my connection, but also to enjoy ad-free browsing, no matter where I am.&lt;/p&gt;
&lt;div class="video-wrapper"&gt;
&lt;iframe loading="lazy"
src="https://www.youtube.com/embed/ZHQXSUq01k0"
allowfullscreen
title="YouTube Video"
&gt;
&lt;/iframe&gt;
&lt;/div&gt;</description></item><item><title>Web service deployment with EC2 AWS</title><link>https://aleingmar-pi-portfolio.duckdns.org/p/aws-ec2-assb/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://aleingmar-pi-portfolio.duckdns.org/p/aws-ec2-assb/</guid><description>&lt;img src="https://aleingmar-pi-portfolio.duckdns.org/p/aws-ec2-assb/aws-ec2.png" alt="Featured image of post Web service deployment with EC2 AWS" /&gt;&lt;p&gt;This project was developed for a Systems Architectures and Distributed Systems (ASSB) assignment during my fourth year of undergraduate studies. The main objective was to deploy a functional web service in a cloud environment using &lt;strong&gt;AWS&lt;/strong&gt; services, staying within the limits of the free plan.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The deployed web service consists of a load balancer that distributes requests between two EC2 instances, each with its own Apache server and web page.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The main tasks performed included:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Controlling and creating cost alerts&lt;/strong&gt;: Setting up alerts in AWS to ensure that all operations conformed to the free plan, avoiding unwanted additional charges.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deployment of EC2 instances&lt;/strong&gt;: Raise two instances of &lt;strong&gt;EC2&lt;/strong&gt; and connect to them via &lt;strong&gt;SSH&lt;/strong&gt;. On each of the instances, install the &lt;strong&gt;Apache&lt;/strong&gt; web server.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Configuring Apache web servers&lt;/strong&gt;: Configuring the Apache services on both instances to serve a static web page created by myself.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deployment of a load balancer&lt;/strong&gt;: Deployment of a load balancer on AWS to evenly distribute incoming requests between the two Apache servers, optimising the load and ensuring higher availability.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a class="link" href="../../post/aws-ec2-assb/assb-aes-ec2.pdf" &gt;&lt;strong&gt;View memory in pdf&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;</description></item></channel></rss>