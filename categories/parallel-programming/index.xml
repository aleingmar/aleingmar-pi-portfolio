<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Parallel Programming on Alejandro Inglés Martínez</title><link>https://aleingmar-pi-portfolio.duckdns.org/categories/parallel-programming/</link><description>Recent content in Parallel Programming on Alejandro Inglés Martínez</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 01 Nov 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://aleingmar-pi-portfolio.duckdns.org/categories/parallel-programming/index.xml" rel="self" type="application/rss+xml"/><item><title>Parallel computing in distributed systems with MPI.</title><link>https://aleingmar-pi-portfolio.duckdns.org/p/mpi-assb/</link><pubDate>Wed, 01 Nov 2023 00:00:00 +0000</pubDate><guid>https://aleingmar-pi-portfolio.duckdns.org/p/mpi-assb/</guid><description>&lt;img src="https://aleingmar-pi-portfolio.duckdns.org/p/mpi-assb/mpi-logo.png" alt="Featured image of post Parallel computing in distributed systems with MPI." /&gt;&lt;p&gt;This project was developed for the course Architecture of Systems and Software Basis (ASSB), during my fourth year of studies. The main objective was to become familiar with parallel programming on distributed memory computers using the &lt;strong&gt;MPI&lt;/strong&gt; library in &lt;strong&gt;C&lt;/strong&gt; (Message-Passing Interface). A widely used technique for distributed computing on multiple machines, commonly used in high performance clusters. In the case of this project everything was executed in my own computer, understanding my computer as a kind of cluster and its different cores as machines that form it.&lt;/p&gt;
&lt;p&gt;The main tasks of this project are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Development of the ‘Hello World’ program with MPI&lt;/strong&gt;: As a first step, I programmed a basic program so that &lt;strong&gt;each process would print a message with its rank and the name of the processor it was running on&lt;/strong&gt;. In addition, I experimented with the possibility of launching more processes than the number of physical cores available, observing how MPI handles this scenario even though it loses performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalar product of vectors in parallel&lt;/strong&gt;: I implemented a program with MPI that &lt;strong&gt;computes the scalar product of two large vectors&lt;/strong&gt;, distributing the work among several processes. Each process calculated a part of the scalar product, and then the results were brought together in the process of rank 0, which displayed the total result. A runtime measurement was also added, allowing analysis of how performance varied as the number of processes changed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Solving integrals using the trapezoid method&lt;/strong&gt;: I then &lt;strong&gt;implemented a program to calculate integrals using the trapezoid method&lt;/strong&gt;, parallelising the program so that each process calculated the sum of the trapezoids in a specific sub-interval. As before, the process at rank 0 was responsible for summing the results of all the processes and displaying the final value of the integral.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Performance and scalability analysis&lt;/strong&gt;: To evaluate the performance of the parallel program, I measured execution times and speedup when using different numbers of processes, from 1 to more than twice the number of available physical cores. The results were visualised in graphs showing how speedup improved as the number of processes increased, but also how efficiency decreased at certain points due to overhead in inter-process communication.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Project documentation: &lt;a class="link" href="../../post/paralelizacion-mpi-assb/ASSB-mpi.pdf" &gt;&lt;strong&gt;View documentation in pdf&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Parallel computing on multicore processors with OpenMP.</title><link>https://aleingmar-pi-portfolio.duckdns.org/p/openmp-assb/</link><pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate><guid>https://aleingmar-pi-portfolio.duckdns.org/p/openmp-assb/</guid><description>&lt;img src="https://aleingmar-pi-portfolio.duckdns.org/p/openmp-assb/openmp-logo.png" alt="Featured image of post Parallel computing on multicore processors with OpenMP." /&gt;&lt;p&gt;This project was developed during the Systems and Software Architecture and Basis (ASSB) course during my fourth year of studies. The main objective was to implement an algorithm for calculating the Pi number using the MonteCarlo method by programming in C both sequentially and using parallel programming techniques to take full advantage of the resources of multicore processors using the OpenMP library, which allows code to be executed in multiple threads efficiently.&lt;/p&gt;
&lt;p&gt;The main tasks performed were the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Development of the sequential and parallel version of the algorithm&lt;/strong&gt;: I implemented a program that simulates the throwing of random ‘darts’ inside a square inscribed in a circle. The ratio between the hits inside the circle and the total throws is used to calculate the value of Pi. In the parallel version, I used OpenMP to divide the work among several threads, taking full advantage of the resources of multi-core processors.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Time measurements and performance analysis&lt;/strong&gt;: After developing the two versions of the program, I performed time measurements to evaluate the performance of the parallel version compared to the sequential version. I used different thread configurations, from a single thread to more than twice the physical cores of the processor, in order to analyse the speedup and scalability of the algorithm. The speedup was calculated as the ratio between the single-threaded execution time and the multi-threaded execution time.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Optimisation and management of shared resources&lt;/strong&gt;: During development, it was necessary to solve common problems in parallel programming, such as race conditions. In this case, I used OpenMP directives to define private variables on a per-thread basis, preventing multiple threads from simultaneously accessing the same global variables and affecting the final result.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generating performance graphs&lt;/strong&gt;: After collecting runtime and speedup data, I generated graphs to visualise the performance of the program as the number of threads increased. These graphs demonstrated how the application scaled with increased thread count, highlighting the advantages and limitations of parallelisation in a shared-memory environment.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Project documentation: &lt;a class="link" href="../../post/paralelizacion-openmp-assb/ASSB-openmp.pdf" &gt;&lt;strong&gt;View documentation in pdf&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;</description></item></channel></rss>